{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "all_spans = {}\n",
    "\n",
    "with open(\"../../mfc_v4.0/spans_no_context.json\", \"r\") as f:\n",
    "    all_spans = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/.local/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2243: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for span 100 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 200 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 300 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 400 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 500 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 600 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 700 of 695878 computed\n",
      "torch.Size([60, 106, 768])\n",
      "Embedding for span 800 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 900 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 1000 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 1100 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 1200 of 695878 computed\n",
      "torch.Size([60, 8, 768])\n",
      "Embedding for span 1300 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 1400 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 1500 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 1600 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 1700 of 695878 computed\n",
      "torch.Size([60, 61, 768])\n",
      "Embedding for span 1800 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 1900 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 2000 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 2100 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 2200 of 695878 computed\n",
      "torch.Size([60, 9, 768])\n",
      "Embedding for span 2300 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 2400 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 2500 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 2600 of 695878 computed\n",
      "torch.Size([60, 35, 768])\n",
      "Embedding for span 2700 of 695878 computed\n",
      "torch.Size([60, 21, 768])\n",
      "Embedding for span 2800 of 695878 computed\n",
      "torch.Size([60, 20, 768])\n",
      "Embedding for span 2900 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 3000 of 695878 computed\n",
      "torch.Size([60, 9, 768])\n",
      "Embedding for span 3100 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 3200 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 3300 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 3400 of 695878 computed\n",
      "torch.Size([60, 51, 768])\n",
      "Embedding for span 3500 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 3600 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 3700 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 3800 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 3900 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 4000 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 4100 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 4200 of 695878 computed\n",
      "torch.Size([60, 7, 768])\n",
      "Embedding for span 4300 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 4400 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 4500 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 4600 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 4700 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 4800 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 4900 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 5000 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 5100 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 5200 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 5300 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 5400 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 5500 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 5600 of 695878 computed\n",
      "torch.Size([60, 5, 768])\n",
      "Embedding for span 5700 of 695878 computed\n",
      "torch.Size([60, 6, 768])\n",
      "Embedding for span 5800 of 695878 computed\n",
      "torch.Size([60, 11, 768])\n",
      "Embedding for span 5900 of 695878 computed\n",
      "torch.Size([60, 15, 768])\n",
      "Embedding for span 6000 of 695878 computed\n",
      "torch.Size([60, 20, 768])\n",
      "Embedding for span 6100 of 695878 computed\n",
      "torch.Size([60, 14, 768])\n",
      "Embedding for span 6200 of 695878 computed\n",
      "torch.Size([60, 15, 768])\n",
      "Embedding for span 6300 of 695878 computed\n",
      "torch.Size([60, 26, 768])\n",
      "Embedding for span 6400 of 695878 computed\n",
      "torch.Size([60, 13, 768])\n",
      "Embedding for span 6500 of 695878 computed\n",
      "torch.Size([60, 12, 768])\n",
      "Embedding for span 6600 of 695878 computed\n",
      "torch.Size([60, 19, 768])\n",
      "Embedding for span 6700 of 695878 computed\n",
      "torch.Size([60, 25, 768])\n",
      "Embedding for span 6800 of 695878 computed\n",
      "torch.Size([60, 24, 768])\n",
      "Embedding for span 6900 of 695878 computed\n",
      "torch.Size([60, 21, 768])\n",
      "Embedding for span 7000 of 695878 computed\n",
      "torch.Size([60, 17, 768])\n",
      "Embedding for span 7100 of 695878 computed\n",
      "torch.Size([60, 15, 768])\n",
      "Embedding for span 7200 of 695878 computed\n",
      "torch.Size([60, 17, 768])\n",
      "Embedding for span 7300 of 695878 computed\n",
      "torch.Size([60, 16, 768])\n",
      "Embedding for span 7400 of 695878 computed\n",
      "torch.Size([60, 45, 768])\n",
      "Embedding for span 7500 of 695878 computed\n",
      "torch.Size([60, 17, 768])\n",
      "Embedding for span 7600 of 695878 computed\n",
      "torch.Size([60, 34, 768])\n",
      "Embedding for span 7700 of 695878 computed\n",
      "torch.Size([60, 36, 768])\n",
      "Embedding for span 7800 of 695878 computed\n",
      "torch.Size([60, 50, 768])\n",
      "Embedding for span 7900 of 695878 computed\n",
      "torch.Size([60, 30, 768])\n",
      "Embedding for span 8000 of 695878 computed\n",
      "torch.Size([60, 35, 768])\n",
      "Embedding for span 8100 of 695878 computed\n",
      "torch.Size([60, 25, 768])\n",
      "Embedding for span 8200 of 695878 computed\n",
      "torch.Size([60, 46, 768])\n",
      "Embedding for span 8300 of 695878 computed\n",
      "torch.Size([60, 42, 768])\n",
      "Embedding for span 8400 of 695878 computed\n",
      "torch.Size([60, 26, 768])\n",
      "Embedding for span 8500 of 695878 computed\n",
      "torch.Size([60, 39, 768])\n",
      "Embedding for span 8600 of 695878 computed\n",
      "torch.Size([60, 26, 768])\n",
      "Embedding for span 8700 of 695878 computed\n",
      "torch.Size([60, 48, 768])\n",
      "Embedding for span 8800 of 695878 computed\n",
      "torch.Size([60, 25, 768])\n",
      "Embedding for span 8900 of 695878 computed\n",
      "torch.Size([60, 26, 768])\n",
      "Embedding for span 9000 of 695878 computed\n",
      "torch.Size([60, 36, 768])\n",
      "Embedding for span 9100 of 695878 computed\n",
      "torch.Size([60, 23, 768])\n",
      "Embedding for span 9200 of 695878 computed\n",
      "torch.Size([60, 16, 768])\n",
      "Embedding for span 9300 of 695878 computed\n",
      "torch.Size([60, 28, 768])\n",
      "Embedding for span 9400 of 695878 computed\n",
      "torch.Size([60, 38, 768])\n",
      "Embedding for span 9500 of 695878 computed\n",
      "torch.Size([60, 31, 768])\n",
      "Embedding for span 9600 of 695878 computed\n",
      "torch.Size([60, 43, 768])\n",
      "Embedding for span 9700 of 695878 computed\n",
      "torch.Size([60, 20, 768])\n",
      "Embedding for span 9800 of 695878 computed\n",
      "torch.Size([60, 18, 768])\n",
      "Embedding for span 9900 of 695878 computed\n",
      "torch.Size([60, 24, 768])\n",
      "Embedding for span 10000 of 695878 computed\n",
      "torch.Size([60, 50, 768])\n",
      "Embedding for span 10100 of 695878 computed\n",
      "torch.Size([60, 22, 768])\n",
      "Embedding for span 10200 of 695878 computed\n",
      "torch.Size([60, 45, 768])\n",
      "Embedding for span 10300 of 695878 computed\n",
      "torch.Size([60, 95, 768])\n",
      "Embedding for span 10400 of 695878 computed\n",
      "torch.Size([60, 308, 768])\n",
      "Embedding for span 10500 of 695878 computed\n",
      "torch.Size([60, 101, 768])\n",
      "Embedding for span 10600 of 695878 computed\n",
      "torch.Size([60, 147, 768])\n",
      "Embedding for span 10700 of 695878 computed\n",
      "torch.Size([60, 130, 768])\n",
      "Embedding for span 10800 of 695878 computed\n",
      "torch.Size([60, 79, 768])\n",
      "Embedding for span 10900 of 695878 computed\n",
      "torch.Size([60, 321, 768])\n",
      "Embedding for span 11000 of 695878 computed\n",
      "torch.Size([60, 327, 768])\n",
      "Embedding for span 11100 of 695878 computed\n",
      "torch.Size([60, 253, 768])\n",
      "Embedding for span 11200 of 695878 computed\n",
      "torch.Size([60, 113, 768])\n",
      "Embedding for span 11300 of 695878 computed\n",
      "torch.Size([60, 305, 768])\n",
      "Embedding for span 11400 of 695878 computed\n",
      "torch.Size([60, 276, 768])\n",
      "Embedding for span 11500 of 695878 computed\n",
      "torch.Size([60, 303, 768])\n",
      "Embedding for span 11600 of 695878 computed\n",
      "torch.Size([60, 459, 768])\n",
      "Embedding for span 11700 of 695878 computed\n",
      "torch.Size([60, 110, 768])\n",
      "Embedding for span 11800 of 695878 computed\n",
      "torch.Size([60, 204, 768])\n",
      "Embedding for span 11900 of 695878 computed\n",
      "torch.Size([60, 312, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-daed8d3a0383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "label_map = {key:i for i,key in enumerate(all_spans.keys())}\n",
    "\n",
    "for label in all_spans:\n",
    "    for text in all_spans[label]:\n",
    "        x.append(text)\n",
    "        y.append(label_map[label])\n",
    "\n",
    "x_tokenized = tokenizer(x,return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    \n",
    "train_dataset = Dataset(x_tokenized, y)\n",
    "\n",
    "# ----- 2. Fine-tune pretrained model -----#\n",
    "# Define Trainer parameters\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model.to(\"cuda\"),\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# n_spans = sum(len(all_spans[key]) for key in all_spans)\n",
    "\n",
    "# batch_x = []\n",
    "# batch_y = []\n",
    "\n",
    "# model = model.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for label in all_spans:\n",
    "#         for span in all_spans[label]:\n",
    "#             count += 1\n",
    "#             batch_x.append(span)\n",
    "#             batch_y.append(label)\n",
    "#             if len(batch_x) == 60:\n",
    "#                 tokenized = tokenizer(batch_x, return_tensors='pt', padding=True, max_length=512).to(device)\n",
    "#                 output = model(**tokenized)\n",
    "#                 x.append(output.last_hidden_state.to(\"cpu\")[:,0,:].type(torch.float16))\n",
    "#                 y.append(batch_y)\n",
    "#                 del output\n",
    "#                 del tokenized\n",
    "#                 batch_x = []\n",
    "#                 batch_y = []\n",
    "#             if count % 100 == 0:\n",
    "#                 print(\"Embedding for span %d of %d computed\" % (count, n_spans))\n",
    "#                 print(x[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'empty_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-63c7cd0cdb8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'empty_cache'"
     ]
    }
   ],
   "source": [
    "torch.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-da6abd6b0c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mrobertaClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NN'"
     ]
    }
   ],
   "source": [
    "from torch import NN\n",
    "\n",
    "class robertaClassifier(nn.Module):\n",
    "    def __init__():\n",
    "        pretrained = RobertaModel.from_pretrained('roberta-base')\n",
    "        padder = F.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
