{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3016 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 321 articles and 8147 spans to fit bert input size\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "all_spans = {}\n",
    "\n",
    "with open(\"../../mfc_v4.0/spans_with_context.json\", \"r\") as f:\n",
    "    all_spans = json.load(f)\n",
    "    \n",
    "articles = all_spans[\"articles\"]\n",
    "spans = all_spans[\"spans\"]\n",
    "\n",
    "a_deleted = 0\n",
    "for article in list(articles.keys()):\n",
    "    if len(tokenizer(articles[article])['input_ids']) >= 512:\n",
    "        del articles[article]\n",
    "        a_deleted += 1\n",
    "        \n",
    "s_deleted = 0\n",
    "filtered_spans = []\n",
    "for span in spans:\n",
    "    if span[-1] not in articles:\n",
    "        s_deleted += 1\n",
    "        continue\n",
    "    filtered_spans.append(span)\n",
    "    \n",
    "spans = filtered_spans\n",
    "labels = set(e[0] for e in spans)\n",
    "\n",
    "print(\"Deleted {} articles and {} spans to fit bert input size\".format(a_deleted, s_deleted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as NN\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "\n",
    "class FullContextSpanClassifier(NN.Module):\n",
    "    def __init__(self, labels, reporting=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.transformer = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        for params in self.transformer.parameters():\n",
    "            params.requires_grad = False\n",
    "        self.transformer.eval()\n",
    "        self.fc = NN.Linear(768, len(labels))\n",
    "        self.logits = NN.Softmax()\n",
    "        self.labels = labels\n",
    "        self.reporting=reporting\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tokens = x[0]\n",
    "        indices = x[1]\n",
    "        dims = list(indices.shape)\n",
    "        indices = torch.flatten(indices)\n",
    "        \n",
    "        self.report(\"Data unpacked. running bigbird...\")\n",
    "        \n",
    "        x = self.transformer(**tokens).last_hidden_state\n",
    "        \n",
    "        self.report(\"bigbird run. applying mask and summing...\")\n",
    "        \n",
    "        x = torch.reshape(x, (dims[0]*dims[1], 768))\n",
    "        self.report(\"mask shape:\", indices.shape, \"data shape:\", x.shape)\n",
    "        \n",
    "        x = (x.t()*indices).t()\n",
    "        self.report(\"after masking, data is of shape\", x.shape)\n",
    "        x = torch.reshape(x, (dims[0], dims[1], 768))\n",
    "        \n",
    "        x = torch.sum(x, dim=1)\n",
    "        self.report(\"after summing, data is of shape\", x.shape)\n",
    "        \n",
    "        x = normalize(x, dim=1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.logits(x)\n",
    "        \n",
    "        self.report(\"classifier run.\")\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def report(self,*args):\n",
    "        if self.reporting:\n",
    "            print(\"(FullContextSpanClassifier): \", \" \".join([str(x) for x in args]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annotation_mask(offset_mapping, batch_bounds):\n",
    "    token_spans = []\n",
    "    for i, inp in enumerate(offset_mapping):\n",
    "\n",
    "        start_idx = -1\n",
    "        end_idx = -1\n",
    "\n",
    "        for j, span in enumerate(inp):\n",
    "            tok_start = span[0]\n",
    "            tok_end = span[1]\n",
    "            annotation_start = batch_bounds[i][0]\n",
    "            annotation_end = batch_bounds[i][1]\n",
    "            if tok_end > annotation_start and start_idx == -1:\n",
    "                start_idx = j\n",
    "            if tok_end > annotation_end:\n",
    "                end_idx = j\n",
    "                break\n",
    "        token_spans.append([1 if i >= start_idx and i < end_idx else 0 for i in range(len(inp))])\n",
    "    return token_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "constructing batches...\n",
      "batches constructed. starting training steps...\n",
      "Training Loss at step 0 : 14.959721565246582\n",
      "Test Loss 14.867717742919922 \n",
      "\n",
      "done with step 0 of 311\n",
      "current time: 0.011228454113006592 est epoch finish:  3.4920492291450502\n",
      "Training Loss at step 1 : 14.894336700439453\n",
      "Training Loss at step 2 : 14.858057975769043\n",
      "Training Loss at step 3 : 14.871723175048828\n",
      "Training Loss at step 4 : 14.857890129089355\n",
      "Training Loss at step 5 : 14.851085662841797\n",
      "Training Loss at step 6 : 14.791114807128906\n",
      "Training Loss at step 7 : 14.797819137573242\n",
      "Training Loss at step 8 : 14.750685691833496\n",
      "Training Loss at step 9 : 14.730365753173828\n",
      "Training Loss at step 10 : 14.79826545715332\n",
      "Test Loss 14.77617073059082 \n",
      "\n",
      "done with step 10 of 311\n",
      "current time: 0.07496475378672282 est epoch finish:  2.1194580388791633\n",
      "Training Loss at step 11 : 14.66836166381836\n",
      "Training Loss at step 12 : 14.718295097351074\n",
      "Training Loss at step 13 : 14.719673156738281\n",
      "Training Loss at step 14 : 14.782073020935059\n",
      "Training Loss at step 15 : 14.735532760620117\n",
      "Training Loss at step 16 : 14.666997909545898\n",
      "Training Loss at step 17 : 14.70473575592041\n",
      "Training Loss at step 18 : 14.721081733703613\n",
      "Training Loss at step 19 : 14.819938659667969\n",
      "Training Loss at step 20 : 14.702545166015625\n",
      "Test Loss 14.726005554199219 \n",
      "\n",
      "done with step 20 of 311\n",
      "current time: 0.13811822334925333 est epoch finish:  2.0454651172198948\n",
      "Training Loss at step 21 : 14.640329360961914\n",
      "Training Loss at step 22 : 14.526067733764648\n",
      "Training Loss at step 23 : 14.676734924316406\n",
      "Training Loss at step 24 : 14.657090187072754\n",
      "Training Loss at step 25 : 14.575481414794922\n",
      "Training Loss at step 26 : 14.6260347366333\n",
      "Training Loss at step 27 : 14.681526184082031\n",
      "Training Loss at step 28 : 14.638670921325684\n",
      "Training Loss at step 29 : 14.589092254638672\n",
      "Training Loss at step 30 : 14.504438400268555\n",
      "Test Loss 14.692578315734863 \n",
      "\n",
      "done with step 30 of 311\n",
      "current time: 0.2021547834078471 est epoch finish:  2.028068956123885\n",
      "Training Loss at step 31 : 14.519195556640625\n",
      "Training Loss at step 32 : 14.586626052856445\n",
      "Training Loss at step 33 : 14.54588508605957\n",
      "Training Loss at step 34 : 14.670275688171387\n",
      "Training Loss at step 35 : 14.469854354858398\n",
      "Training Loss at step 36 : 14.590648651123047\n",
      "Training Loss at step 37 : 14.62772274017334\n",
      "Training Loss at step 38 : 14.526590347290039\n",
      "Training Loss at step 39 : 14.486689567565918\n",
      "Training Loss at step 40 : 14.53089714050293\n",
      "Test Loss 14.650032997131348 \n",
      "\n",
      "done with step 40 of 311\n",
      "current time: 0.2645954926808675 est epoch finish:  2.0070536152134095\n",
      "Training Loss at step 41 : 14.564129829406738\n",
      "Training Loss at step 42 : 14.342215538024902\n",
      "Training Loss at step 43 : 14.426498413085938\n",
      "Training Loss at step 44 : 14.58516788482666\n",
      "Training Loss at step 45 : 14.434224128723145\n",
      "Training Loss at step 46 : 14.53390121459961\n",
      "Training Loss at step 47 : 14.579174041748047\n",
      "Training Loss at step 48 : 14.458316802978516\n",
      "Training Loss at step 49 : 14.445606231689453\n",
      "Training Loss at step 50 : 14.515145301818848\n",
      "Test Loss 14.632133483886719 \n",
      "\n",
      "done with step 50 of 311\n",
      "current time: 0.3284486492474874 est epoch finish:  2.0028927434503645\n",
      "Training Loss at step 51 : 14.437420845031738\n",
      "Training Loss at step 52 : 14.502147674560547\n",
      "Training Loss at step 53 : 14.419245719909668\n",
      "Training Loss at step 54 : 14.409858703613281\n",
      "Training Loss at step 55 : 14.527795791625977\n",
      "Training Loss at step 56 : 14.452646255493164\n",
      "Training Loss at step 57 : 14.4701566696167\n",
      "Training Loss at step 58 : 14.676634788513184\n",
      "Training Loss at step 59 : 14.600128173828125\n",
      "Training Loss at step 60 : 14.575093269348145\n",
      "Test Loss 14.605663299560547 \n",
      "\n",
      "done with step 60 of 311\n",
      "current time: 0.39400264819463093 est epoch finish:  2.008767599811971\n",
      "Training Loss at step 61 : 14.466986656188965\n",
      "Training Loss at step 62 : 14.292312622070312\n",
      "Training Loss at step 63 : 14.329612731933594\n",
      "Training Loss at step 64 : 14.353239059448242\n",
      "Training Loss at step 65 : 14.40070629119873\n",
      "Training Loss at step 66 : 14.355830192565918\n",
      "Training Loss at step 67 : 14.41333293914795\n",
      "Training Loss at step 68 : 14.301450729370117\n",
      "Training Loss at step 69 : 14.08911418914795\n",
      "Training Loss at step 70 : 14.144020080566406\n",
      "Test Loss 14.58308219909668 \n",
      "\n",
      "done with step 70 of 311\n",
      "current time: 0.46028249263763427 est epoch finish:  2.0161669747930175\n",
      "Training Loss at step 71 : 14.797281265258789\n",
      "Training Loss at step 72 : 14.266841888427734\n",
      "Training Loss at step 73 : 14.207818984985352\n",
      "Training Loss at step 74 : 14.344566345214844\n",
      "Training Loss at step 75 : 14.428274154663086\n",
      "Training Loss at step 76 : 14.56521224975586\n",
      "Training Loss at step 77 : 14.219880104064941\n",
      "Training Loss at step 78 : 14.095708847045898\n",
      "Training Loss at step 79 : 14.319390296936035\n",
      "Training Loss at step 80 : 14.369339942932129\n",
      "Test Loss 14.560327529907227 \n",
      "\n",
      "done with step 80 of 311\n",
      "current time: 0.5243018706639607 est epoch finish:  2.0130602688455776\n",
      "Training Loss at step 81 : 14.419920921325684\n",
      "Training Loss at step 82 : 14.441702842712402\n",
      "Training Loss at step 83 : 14.288752555847168\n",
      "Training Loss at step 84 : 14.002073287963867\n",
      "Training Loss at step 85 : 14.172502517700195\n",
      "Training Loss at step 86 : 14.375255584716797\n",
      "Training Loss at step 87 : 14.369346618652344\n",
      "Training Loss at step 88 : 14.35531234741211\n",
      "Training Loss at step 89 : 14.14932632446289\n",
      "Training Loss at step 90 : 14.25786018371582\n",
      "Test Loss 14.555051803588867 \n",
      "\n",
      "done with step 90 of 311\n",
      "current time: 0.5898043831189473 est epoch finish:  2.0157050895603588\n",
      "Training Loss at step 91 : 14.176795959472656\n",
      "Training Loss at step 92 : 14.255077362060547\n",
      "Training Loss at step 93 : 14.112340927124023\n",
      "Training Loss at step 94 : 14.18418025970459\n",
      "Training Loss at step 95 : 13.98786735534668\n",
      "Training Loss at step 96 : 13.957626342773438\n",
      "Training Loss at step 97 : 14.039300918579102\n",
      "Training Loss at step 98 : 14.094903945922852\n",
      "Training Loss at step 99 : 14.12855339050293\n",
      "Training Loss at step 100 : 13.987893104553223\n",
      "Test Loss 14.538894653320312 \n",
      "\n",
      "done with step 100 of 311\n",
      "current time: 0.654053270816803 est epoch finish:  2.0139660121190666\n",
      "Training Loss at step 101 : 14.356401443481445\n",
      "Training Loss at step 102 : 14.509035110473633\n",
      "Training Loss at step 103 : 14.021677017211914\n",
      "Training Loss at step 104 : 14.099874496459961\n",
      "Training Loss at step 105 : 14.185898780822754\n",
      "Training Loss at step 106 : 14.336325645446777\n",
      "Training Loss at step 107 : 14.026288032531738\n",
      "Training Loss at step 108 : 14.129608154296875\n",
      "Training Loss at step 109 : 14.302682876586914\n",
      "Training Loss at step 110 : 14.168964385986328\n",
      "Test Loss 14.520773887634277 \n",
      "\n",
      "done with step 110 of 311\n",
      "current time: 0.719774619738261 est epoch finish:  2.0166658264738664\n",
      "Training Loss at step 111 : 13.900726318359375\n",
      "Training Loss at step 112 : 14.109943389892578\n",
      "Training Loss at step 113 : 14.423736572265625\n",
      "Training Loss at step 114 : 14.053512573242188\n",
      "Training Loss at step 115 : 13.99001407623291\n",
      "Training Loss at step 116 : 14.048162460327148\n",
      "Training Loss at step 117 : 14.380462646484375\n",
      "Training Loss at step 118 : 14.019929885864258\n",
      "Training Loss at step 119 : 14.171670913696289\n",
      "Training Loss at step 120 : 14.088581085205078\n",
      "Test Loss 14.500014305114746 \n",
      "\n",
      "done with step 120 of 311\n",
      "current time: 0.7854752143224081 est epoch finish:  2.018866046729495\n",
      "Training Loss at step 121 : 14.188457489013672\n",
      "Training Loss at step 122 : 14.323100090026855\n",
      "Training Loss at step 123 : 13.963747024536133\n",
      "Training Loss at step 124 : 14.208879470825195\n",
      "Training Loss at step 125 : 14.346389770507812\n",
      "Training Loss at step 126 : 14.347846031188965\n",
      "Training Loss at step 127 : 14.400190353393555\n",
      "Training Loss at step 128 : 14.050210952758789\n",
      "Training Loss at step 129 : 14.02358627319336\n",
      "Training Loss at step 130 : 13.87131404876709\n",
      "Test Loss 14.49460220336914 \n",
      "\n",
      "done with step 130 of 311\n",
      "current time: 0.8501970767974854 est epoch finish:  2.0184068006413582\n",
      "Training Loss at step 131 : 14.401603698730469\n",
      "Training Loss at step 132 : 14.004568099975586\n",
      "Training Loss at step 133 : 14.40561294555664\n",
      "Training Loss at step 134 : 14.289970397949219\n",
      "Training Loss at step 135 : 14.355634689331055\n",
      "Training Loss at step 136 : 14.111517906188965\n",
      "Training Loss at step 137 : 14.13900375366211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 138 : 14.141304969787598\n",
      "Training Loss at step 139 : 13.964757919311523\n",
      "Training Loss at step 140 : 13.989887237548828\n",
      "Test Loss 14.484991073608398 \n",
      "\n",
      "done with step 140 of 311\n",
      "current time: 0.9137767990430196 est epoch finish:  2.015493507109072\n",
      "Training Loss at step 141 : 14.272119522094727\n",
      "Training Loss at step 142 : 14.026771545410156\n",
      "Training Loss at step 143 : 14.262874603271484\n",
      "Training Loss at step 144 : 13.904277801513672\n",
      "Training Loss at step 145 : 13.968494415283203\n",
      "Training Loss at step 146 : 14.079180717468262\n",
      "Training Loss at step 147 : 14.188714981079102\n",
      "Training Loss at step 148 : 14.207623481750488\n",
      "Training Loss at step 149 : 14.084989547729492\n",
      "Training Loss at step 150 : 13.898887634277344\n",
      "Test Loss 14.469326972961426 \n",
      "\n",
      "done with step 150 of 311\n",
      "current time: 0.9755862792332967 est epoch finish:  2.0093200850434125\n",
      "Training Loss at step 151 : 14.138829231262207\n",
      "Training Loss at step 152 : 13.954305648803711\n",
      "Training Loss at step 153 : 14.029413223266602\n",
      "Training Loss at step 154 : 13.99413776397705\n",
      "Training Loss at step 155 : 13.687232971191406\n",
      "Training Loss at step 156 : 13.96645450592041\n",
      "Training Loss at step 157 : 14.260787010192871\n",
      "Training Loss at step 158 : 13.951891899108887\n",
      "Training Loss at step 159 : 14.088058471679688\n",
      "Training Loss at step 160 : 13.942368507385254\n",
      "Test Loss 14.44771957397461 \n",
      "\n",
      "done with step 160 of 311\n",
      "current time: 1.0404163082440694 est epoch finish:  2.0097482724466187\n",
      "Training Loss at step 161 : 13.704963684082031\n",
      "Training Loss at step 162 : 14.042366027832031\n",
      "Training Loss at step 163 : 13.825660705566406\n",
      "Training Loss at step 164 : 13.8961181640625\n",
      "Training Loss at step 165 : 14.39146614074707\n",
      "Training Loss at step 166 : 13.70366096496582\n",
      "Training Loss at step 167 : 13.737771034240723\n",
      "Training Loss at step 168 : 13.83504867553711\n",
      "Training Loss at step 169 : 14.117337226867676\n",
      "Training Loss at step 170 : 13.894553184509277\n",
      "Test Loss 14.44963264465332 \n",
      "\n",
      "done with step 170 of 311\n",
      "current time: 1.1038365244865418 est epoch finish:  2.007562334007687\n",
      "Training Loss at step 171 : 13.72494125366211\n",
      "Training Loss at step 172 : 13.917877197265625\n",
      "Training Loss at step 173 : 13.695648193359375\n",
      "Training Loss at step 174 : 13.89393424987793\n",
      "Training Loss at step 175 : 14.035669326782227\n",
      "Training Loss at step 176 : 13.993341445922852\n",
      "Training Loss at step 177 : 13.753375053405762\n",
      "Training Loss at step 178 : 14.076522827148438\n",
      "Training Loss at step 179 : 14.419418334960938\n",
      "Training Loss at step 180 : 13.951254844665527\n",
      "Test Loss 14.439092636108398 \n",
      "\n",
      "done with step 180 of 311\n",
      "current time: 1.1678898294766744 est epoch finish:  2.0067057291008052\n",
      "Training Loss at step 181 : 13.752630233764648\n",
      "Training Loss at step 182 : 14.310226440429688\n",
      "Training Loss at step 183 : 14.00960636138916\n",
      "Training Loss at step 184 : 14.041191101074219\n",
      "Training Loss at step 185 : 14.190780639648438\n",
      "Training Loss at step 186 : 13.46662712097168\n",
      "Training Loss at step 187 : 14.377053260803223\n",
      "Training Loss at step 188 : 14.127107620239258\n",
      "Training Loss at step 189 : 14.188405990600586\n",
      "Training Loss at step 190 : 13.940441131591797\n",
      "Test Loss 14.438897132873535 \n",
      "\n",
      "done with step 190 of 311\n",
      "current time: 1.2310486952463786 est epoch finish:  2.0044824304797055\n",
      "Training Loss at step 191 : 14.011390686035156\n",
      "Training Loss at step 192 : 14.0049409866333\n",
      "Training Loss at step 193 : 13.645040512084961\n",
      "Training Loss at step 194 : 13.91108226776123\n",
      "Training Loss at step 195 : 13.745424270629883\n",
      "Training Loss at step 196 : 14.176627159118652\n",
      "Training Loss at step 197 : 13.479598045349121\n",
      "Training Loss at step 198 : 13.9813232421875\n",
      "Training Loss at step 199 : 14.16145133972168\n",
      "Training Loss at step 200 : 14.117486953735352\n",
      "Test Loss 14.411205291748047 \n",
      "\n",
      "done with step 200 of 311\n",
      "current time: 1.2961629112561543 est epoch finish:  2.0055057980132536\n",
      "Training Loss at step 201 : 13.834880828857422\n",
      "Training Loss at step 202 : 13.779468536376953\n",
      "Training Loss at step 203 : 13.438849449157715\n",
      "Training Loss at step 204 : 14.202686309814453\n",
      "Training Loss at step 205 : 13.790281295776367\n",
      "Training Loss at step 206 : 13.633124351501465\n",
      "Training Loss at step 207 : 14.019214630126953\n",
      "Training Loss at step 208 : 14.20882797241211\n",
      "Training Loss at step 209 : 14.062604904174805\n",
      "Training Loss at step 210 : 14.009439468383789\n",
      "Test Loss 14.426589965820312 \n",
      "\n",
      "done with step 210 of 311\n",
      "current time: 1.3597421129544576 est epoch finish:  2.0041696546390346\n",
      "Training Loss at step 211 : 14.047981262207031\n",
      "Training Loss at step 212 : 13.642879486083984\n",
      "Training Loss at step 213 : 14.285975456237793\n",
      "Training Loss at step 214 : 13.269744873046875\n",
      "Training Loss at step 215 : 13.582893371582031\n",
      "Training Loss at step 216 : 13.726692199707031\n",
      "Training Loss at step 217 : 14.289350509643555\n",
      "Training Loss at step 218 : 13.24688720703125\n",
      "Training Loss at step 219 : 13.994918823242188\n",
      "Training Loss at step 220 : 13.913463592529297\n",
      "Test Loss 14.411447525024414 \n",
      "\n",
      "done with step 220 of 311\n",
      "current time: 1.4239156166712443 est epoch finish:  2.003790754682158\n",
      "Training Loss at step 221 : 14.105620384216309\n",
      "Training Loss at step 222 : 13.536079406738281\n",
      "Training Loss at step 223 : 13.782431602478027\n",
      "Training Loss at step 224 : 13.737552642822266\n",
      "Training Loss at step 225 : 13.541467666625977\n",
      "Training Loss at step 226 : 13.871767044067383\n",
      "Training Loss at step 227 : 13.522050857543945\n",
      "Training Loss at step 228 : 13.655017852783203\n",
      "Training Loss at step 229 : 13.464973449707031\n",
      "Training Loss at step 230 : 14.324613571166992\n",
      "Test Loss 14.431180000305176 \n",
      "\n",
      "done with step 230 of 311\n",
      "current time: 1.4882903138796488 est epoch finish:  2.0037155308076655\n",
      "Training Loss at step 231 : 13.74195671081543\n",
      "Training Loss at step 232 : 14.25015640258789\n",
      "Training Loss at step 233 : 14.121622085571289\n",
      "Training Loss at step 234 : 13.710030555725098\n",
      "Training Loss at step 235 : 13.483259201049805\n",
      "Training Loss at step 236 : 13.943746566772461\n",
      "Training Loss at step 237 : 13.8501558303833\n",
      "Training Loss at step 238 : 13.561103820800781\n",
      "Training Loss at step 239 : 13.576066017150879\n",
      "Training Loss at step 240 : 14.086276054382324\n",
      "Test Loss 14.42379093170166 \n",
      "\n",
      "done with step 240 of 311\n",
      "current time: 1.5534059882164002 est epoch finish:  2.004602748279255\n",
      "Training Loss at step 241 : 13.988229751586914\n",
      "Training Loss at step 242 : 13.953486442565918\n",
      "Training Loss at step 243 : 13.780866622924805\n",
      "Training Loss at step 244 : 14.172706604003906\n",
      "Training Loss at step 245 : 13.583564758300781\n",
      "Training Loss at step 246 : 14.135910987854004\n",
      "Training Loss at step 247 : 13.821758270263672\n",
      "Training Loss at step 248 : 13.652664184570312\n",
      "Training Loss at step 249 : 14.362305641174316\n",
      "Training Loss at step 250 : 13.884180068969727\n",
      "Test Loss 14.421159744262695 \n",
      "\n",
      "done with step 250 of 311\n",
      "current time: 1.618864321708679 est epoch finish:  2.005843840842228\n",
      "Training Loss at step 251 : 13.758759498596191\n",
      "Training Loss at step 252 : 13.85367202758789\n",
      "Training Loss at step 253 : 13.594797134399414\n",
      "Training Loss at step 254 : 14.029624938964844\n",
      "Training Loss at step 255 : 13.31026840209961\n",
      "Training Loss at step 256 : 13.820423126220703\n",
      "Training Loss at step 257 : 13.301692962646484\n",
      "Training Loss at step 258 : 13.744586944580078\n",
      "Training Loss at step 259 : 13.656843185424805\n",
      "Training Loss at step 260 : 13.378129959106445\n",
      "Test Loss 14.434831619262695 \n",
      "\n",
      "done with step 260 of 311\n",
      "current time: 1.684266996383667 est epoch finish:  2.0069235091008446\n",
      "Training Loss at step 261 : 13.605558395385742\n",
      "Training Loss at step 262 : 13.551290512084961\n",
      "Training Loss at step 263 : 13.434206008911133\n",
      "Training Loss at step 264 : 13.875997543334961\n",
      "Training Loss at step 265 : 14.063862800598145\n",
      "Training Loss at step 266 : 13.528127670288086\n",
      "Training Loss at step 267 : 14.005199432373047\n",
      "Training Loss at step 268 : 13.379056930541992\n",
      "Training Loss at step 269 : 14.052396774291992\n",
      "Training Loss at step 270 : 13.569891929626465\n",
      "Test Loss 14.412954330444336 \n",
      "\n",
      "done with step 270 of 311\n",
      "current time: 1.7517306447029113 est epoch finish:  2.0102886734413485\n",
      "Training Loss at step 271 : 14.156376838684082\n",
      "Training Loss at step 272 : 14.540592193603516\n",
      "Training Loss at step 273 : 14.026281356811523\n",
      "Training Loss at step 274 : 13.466514587402344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 275 : 13.759088516235352\n",
      "Training Loss at step 276 : 13.976300239562988\n",
      "Training Loss at step 277 : 14.11009693145752\n",
      "Training Loss at step 278 : 13.846817016601562\n",
      "Training Loss at step 279 : 13.529170989990234\n",
      "Training Loss at step 280 : 14.044897079467773\n",
      "Test Loss 14.435081481933594 \n",
      "\n",
      "done with step 280 of 311\n",
      "current time: 1.819424311319987 est epoch finish:  2.0136688997171386\n",
      "Training Loss at step 281 : 13.88992977142334\n",
      "Training Loss at step 282 : 13.429948806762695\n",
      "Training Loss at step 283 : 13.854198455810547\n",
      "Training Loss at step 284 : 12.915481567382812\n",
      "Training Loss at step 285 : 13.7476806640625\n",
      "Training Loss at step 286 : 14.205766677856445\n",
      "Training Loss at step 287 : 13.832663536071777\n",
      "Training Loss at step 288 : 13.262690544128418\n",
      "Training Loss at step 289 : 13.662324905395508\n",
      "Training Loss at step 290 : 13.929732322692871\n",
      "Test Loss 14.420700073242188 \n",
      "\n",
      "done with step 290 of 311\n",
      "current time: 1.8861392339070637 est epoch finish:  2.015770796374903\n",
      "Training Loss at step 291 : 14.071735382080078\n",
      "Training Loss at step 292 : 13.565109252929688\n",
      "Training Loss at step 293 : 13.928653717041016\n",
      "Training Loss at step 294 : 14.013606071472168\n",
      "Training Loss at step 295 : 13.705116271972656\n",
      "Training Loss at step 296 : 13.562875747680664\n",
      "Training Loss at step 297 : 14.019002914428711\n",
      "Training Loss at step 298 : 13.59841537475586\n",
      "Training Loss at step 299 : 14.019274711608887\n",
      "Training Loss at step 300 : 13.533583641052246\n",
      "Test Loss 14.414566040039062 \n",
      "\n",
      "done with step 300 of 311\n",
      "current time: 1.9532416820526124 est epoch finish:  2.0181334322869184\n",
      "Training Loss at step 301 : 13.496321678161621\n",
      "Training Loss at step 302 : 13.13154411315918\n",
      "Training Loss at step 303 : 13.589690208435059\n",
      "Training Loss at step 304 : 13.635906219482422\n",
      "Training Loss at step 305 : 13.077180862426758\n",
      "Training Loss at step 306 : 14.44054126739502\n",
      "Training Loss at step 307 : 13.67647933959961\n",
      "Training Loss at step 308 : 14.1489839553833\n",
      "Training Loss at step 309 : 13.087672233581543\n",
      "Training Loss at step 310 : 14.090631484985352\n",
      "Test Loss 14.41457748413086 \n",
      "\n",
      "done with step 310 of 311\n",
      "current time: 2.020219608147939 est epoch finish:  2.0202196081479387\n",
      "Starting training epoch 1\n",
      "constructing batches...\n",
      "batches constructed. starting training steps...\n",
      "Training Loss at step 0 : 14.015278816223145\n",
      "Test Loss 14.410292625427246 \n",
      "\n",
      "done with step 0 of 311\n",
      "current time: 0.0108177383740743 est epoch finish:  3.3643166343371074\n",
      "Training Loss at step 1 : 13.563302993774414\n",
      "Training Loss at step 2 : 13.544885635375977\n",
      "Training Loss at step 3 : 13.140053749084473\n",
      "Training Loss at step 4 : 14.58255386352539\n",
      "Training Loss at step 5 : 13.182435035705566\n",
      "Training Loss at step 6 : 13.329141616821289\n",
      "Training Loss at step 7 : 13.078009605407715\n",
      "Training Loss at step 8 : 13.995499610900879\n",
      "Training Loss at step 9 : 13.674520492553711\n",
      "Training Loss at step 10 : 13.91430377960205\n",
      "Test Loss 14.410250663757324 \n",
      "\n",
      "done with step 10 of 311\n",
      "current time: 0.07688806454340617 est epoch finish:  2.173835279363574\n",
      "Training Loss at step 11 : 13.771764755249023\n",
      "Training Loss at step 12 : 13.892004013061523\n",
      "Training Loss at step 13 : 13.893630027770996\n",
      "Training Loss at step 14 : 12.951181411743164\n",
      "Training Loss at step 15 : 13.390128135681152\n",
      "Training Loss at step 16 : 13.263792037963867\n",
      "Training Loss at step 17 : 14.007205963134766\n",
      "Training Loss at step 18 : 13.265199661254883\n",
      "Training Loss at step 19 : 14.349359512329102\n",
      "Training Loss at step 20 : 13.394264221191406\n",
      "Test Loss 14.414249420166016 \n",
      "\n",
      "done with step 20 of 311\n",
      "current time: 0.14255385398864745 est epoch finish:  2.1111546947842554\n",
      "Training Loss at step 21 : 13.646552085876465\n",
      "Training Loss at step 22 : 13.591594696044922\n",
      "Training Loss at step 23 : 13.795137405395508\n",
      "Training Loss at step 24 : 14.427184104919434\n",
      "Training Loss at step 25 : 13.779062271118164\n",
      "Training Loss at step 26 : 14.141189575195312\n",
      "Training Loss at step 27 : 13.505277633666992\n",
      "Training Loss at step 28 : 13.500935554504395\n",
      "Training Loss at step 29 : 13.569515228271484\n",
      "Training Loss at step 30 : 14.299003601074219\n",
      "Test Loss 14.427848815917969 \n",
      "\n",
      "done with step 30 of 311\n",
      "current time: 0.2074251929918925 est epoch finish:  2.080943065176728\n",
      "Training Loss at step 31 : 13.06010627746582\n",
      "Training Loss at step 32 : 13.534400939941406\n",
      "Training Loss at step 33 : 14.796689987182617\n",
      "Training Loss at step 34 : 13.790998458862305\n",
      "Training Loss at step 35 : 13.055917739868164\n",
      "Training Loss at step 36 : 13.492692947387695\n",
      "Training Loss at step 37 : 13.419870376586914\n",
      "Training Loss at step 38 : 12.34328842163086\n",
      "Training Loss at step 39 : 13.448892593383789\n",
      "Training Loss at step 40 : 13.38985824584961\n",
      "Test Loss 14.416008949279785 \n",
      "\n",
      "done with step 40 of 311\n",
      "current time: 0.2736406445503235 est epoch finish:  2.075664401345137\n",
      "Training Loss at step 41 : 13.350841522216797\n",
      "Training Loss at step 42 : 13.354324340820312\n",
      "Training Loss at step 43 : 13.065197944641113\n",
      "Training Loss at step 44 : 13.865005493164062\n",
      "Training Loss at step 45 : 13.869674682617188\n",
      "Training Loss at step 46 : 13.462362289428711\n",
      "Training Loss at step 47 : 13.885744094848633\n",
      "Training Loss at step 48 : 14.005620956420898\n",
      "Training Loss at step 49 : 14.331982612609863\n",
      "Training Loss at step 50 : 13.22917366027832\n",
      "Test Loss 14.420920372009277 \n",
      "\n",
      "done with step 50 of 311\n",
      "current time: 0.34065791765848796 est epoch finish:  2.0773453410154854\n",
      "Training Loss at step 51 : 14.252519607543945\n",
      "Training Loss at step 52 : 13.695355415344238\n",
      "Training Loss at step 53 : 13.285906791687012\n",
      "Training Loss at step 54 : 13.46639633178711\n",
      "Training Loss at step 55 : 13.947786331176758\n",
      "Training Loss at step 56 : 13.520644187927246\n",
      "Training Loss at step 57 : 12.929727554321289\n",
      "Training Loss at step 58 : 13.600994110107422\n",
      "Training Loss at step 59 : 13.547690391540527\n",
      "Training Loss at step 60 : 14.186896324157715\n",
      "Test Loss 14.412734985351562 \n",
      "\n",
      "done with step 60 of 311\n",
      "current time: 0.4087437589963277 est epoch finish:  2.0839230991452116\n",
      "Training Loss at step 61 : 13.522603988647461\n",
      "Training Loss at step 62 : 13.273869514465332\n",
      "Training Loss at step 63 : 13.99136734008789\n",
      "Training Loss at step 64 : 13.221549987792969\n",
      "Training Loss at step 65 : 13.680288314819336\n",
      "Training Loss at step 66 : 13.923616409301758\n",
      "Training Loss at step 67 : 13.19082260131836\n",
      "Training Loss at step 68 : 13.362069129943848\n",
      "Training Loss at step 69 : 13.24866008758545\n",
      "Training Loss at step 70 : 13.26435661315918\n",
      "Test Loss 14.409931182861328 \n",
      "\n",
      "done with step 70 of 311\n",
      "current time: 0.4740032951037089 est epoch finish:  2.0762679546092038\n",
      "Training Loss at step 71 : 14.057943344116211\n",
      "Training Loss at step 72 : 14.42625904083252\n",
      "Training Loss at step 73 : 13.31218147277832\n",
      "Training Loss at step 74 : 13.37185287475586\n",
      "Training Loss at step 75 : 13.197671890258789\n",
      "Training Loss at step 76 : 13.130340576171875\n",
      "Training Loss at step 77 : 13.629610061645508\n",
      "Training Loss at step 78 : 13.648228645324707\n",
      "Training Loss at step 79 : 13.921255111694336\n",
      "Training Loss at step 80 : 12.87657356262207\n",
      "Test Loss 14.408286094665527 \n",
      "\n",
      "done with step 80 of 311\n",
      "current time: 0.5402804811795553 est epoch finish:  2.074410242553601\n",
      "Training Loss at step 81 : 13.2622709274292\n",
      "Training Loss at step 82 : 13.842811584472656\n",
      "Training Loss at step 83 : 14.517790794372559\n",
      "Training Loss at step 84 : 13.564595222473145\n",
      "Training Loss at step 85 : 13.500984191894531\n",
      "Training Loss at step 86 : 13.376440048217773\n",
      "Training Loss at step 87 : 13.223320007324219\n",
      "Training Loss at step 88 : 13.566299438476562\n",
      "Training Loss at step 89 : 12.872406005859375\n",
      "Training Loss at step 90 : 14.129839897155762\n",
      "Test Loss 14.416915893554688 \n",
      "\n",
      "done with step 90 of 311\n",
      "current time: 0.606015125910441 est epoch finish:  2.0711066391005186\n",
      "Training Loss at step 91 : 13.221502304077148\n",
      "Training Loss at step 92 : 14.241849899291992\n",
      "Training Loss at step 93 : 13.355506896972656\n",
      "Training Loss at step 94 : 13.531856536865234\n",
      "Training Loss at step 95 : 13.960098266601562\n",
      "Training Loss at step 96 : 14.212772369384766\n",
      "Training Loss at step 97 : 13.086347579956055\n",
      "Training Loss at step 98 : 13.01279067993164\n",
      "Training Loss at step 99 : 13.820242881774902\n",
      "Training Loss at step 100 : 13.95442008972168\n",
      "Test Loss 14.432524681091309 \n",
      "\n",
      "done with step 100 of 311\n",
      "current time: 0.6734841267267863 est epoch finish:  2.0737976575448567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 101 : 13.081175804138184\n",
      "Training Loss at step 102 : 13.608415603637695\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "subset_size = 5000\n",
    "\n",
    "data = spans[:subset_size]\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "keys = [float(i+1) for i in range(15)]\n",
    "\n",
    "data = [ [[1 if i == d[0] else 0 for i in keys],d[1],d[2],d[3]] for d in data]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 5e-4\n",
    "\n",
    "model = FullContextSpanClassifier(list(labels)).to(device)\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "test_x = [d[1:] for d in data[:batch_size]]\n",
    "test_x_tokens = tokenizer([articles[x[-1]] for x in test_x], padding=True, return_offsets_mapping=True, return_tensors='pt')\n",
    "test_annotation_mask = calc_annotation_mask(test_x_tokens[\"offset_mapping\"], test_x)\n",
    "test_slice_tensor = torch.tensor(test_annotation_mask, dtype=torch.float)\n",
    "del test_x_tokens[\"offset_mapping\"]\n",
    "test_model_input = [test_x_tokens.to(device), test_slice_tensor.to(device)]\n",
    "\n",
    "test_y = torch.tensor([d[0] for d in data[:batch_size]], dtype=torch.float).to(device)\n",
    "\n",
    "train_data = data[batch_size:]\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    print(\"Starting training epoch\", epoch)\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    train_x = [d[1:] for d in train_data]\n",
    "    train_y = [d[0] for d in train_data]\n",
    "\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    print(\"constructing batches...\")\n",
    "    \n",
    "    for i in range(len(train_data)//batch_size):\n",
    "        batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y = torch.tensor(train_y[i*batch_size:(i+1)*batch_size], dtype=torch.float).to(device)\n",
    "        x_batches.append(batch_x)\n",
    "        y_batches.append(batch_y)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"batches constructed. starting training steps...\")\n",
    "\n",
    "    for step in range(len(x_batches)):\n",
    "        \n",
    "        #print(\"tokenizing batch {} of {}...\".format(step, len(x_batches)))\n",
    "        \n",
    "        batch_tokens = tokenizer([articles[x[-1]] for x in x_batches[step]], padding=True, truncation=True, return_offsets_mapping=True, return_tensors='pt')\n",
    "        \n",
    "        #print(\"tokenized. calculating annotation mask...\")\n",
    "        \n",
    "        annotation_mask = calc_annotation_mask(batch_tokens[\"offset_mapping\"], x_batches[step])\n",
    "        slice_tensor = torch.tensor(annotation_mask, dtype=torch.float)\n",
    "        del batch_tokens[\"offset_mapping\"]\n",
    "        model_input = [batch_tokens.to(device), slice_tensor.to(device)]\n",
    "        \n",
    "        #print(\"mask calculated. running model...\")\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             embeddings = model(model_input).detach().cpu().numpy().astype('float32')\n",
    "#         all_embeddings.append(embeddings)\n",
    "        y_train_pred = model(model_input)\n",
    "        train_loss = loss_fn(y_train_pred, y_batches[step])\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        print(\"Training Loss at step\",step,\":\",train_loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            y_test_pred = model(test_model_input)\n",
    "            test_loss = loss_fn(y_test_pred, test_y).item()\n",
    "            test_losses.append(test_loss)\n",
    "            print(\"Test Loss\",test_loss,\"\\n\")\n",
    "            \n",
    "            diff = time.time() - t1\n",
    "            proj_end = (diff/(step+1)) * len(x_batches)\n",
    "            print(\"done with step {} of {}\".format(step, len(x_batches)))\n",
    "            print(\"current time:\", diff/60, \"est epoch finish: \", proj_end/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_domain = list(range(len(train_losses)))\n",
    "test_domain = ([i*10 for i in range(len(test_losses))])\n",
    "\n",
    "plt.plot(train_domain, train_losses, label=\"train\")\n",
    "plt.plot(test_domain, test_losses, label=\"test\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./distilberta-mfc-with-context.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0 of 32014 articles\n",
      "Done with 1000 of 32014 articles\n",
      "Done with 2000 of 32014 articles\n",
      "Done with 3000 of 32014 articles\n",
      "Done with 4000 of 32014 articles\n",
      "Done with 5000 of 32014 articles\n",
      "Done with 6000 of 32014 articles\n",
      "Done with 7000 of 32014 articles\n",
      "Done with 8000 of 32014 articles\n",
      "Done with 9000 of 32014 articles\n",
      "Done with 10000 of 32014 articles\n",
      "Done with 11000 of 32014 articles\n",
      "Done with 12000 of 32014 articles\n",
      "Done with 13000 of 32014 articles\n",
      "Done with 14000 of 32014 articles\n",
      "Done with 15000 of 32014 articles\n",
      "Done with 16000 of 32014 articles\n",
      "Done with 17000 of 32014 articles\n",
      "Done with 18000 of 32014 articles\n",
      "Done with 19000 of 32014 articles\n",
      "Done with 20000 of 32014 articles\n",
      "Done with 21000 of 32014 articles\n",
      "Done with 22000 of 32014 articles\n",
      "Done with 23000 of 32014 articles\n",
      "Done with 24000 of 32014 articles\n",
      "Done with 25000 of 32014 articles\n",
      "Done with 26000 of 32014 articles\n",
      "Done with 27000 of 32014 articles\n",
      "Done with 28000 of 32014 articles\n",
      "Done with 29000 of 32014 articles\n",
      "Done with 30000 of 32014 articles\n",
      "Done with 31000 of 32014 articles\n",
      "Done with 32000 of 32014 articles\n"
     ]
    }
   ],
   "source": [
    "lengths = dict({})\n",
    "for i in range(14):\n",
    "    lengths[2**i] = 0\n",
    "\n",
    "for n, title in enumerate(articles.keys()):\n",
    "    if n % 1000 == 0:\n",
    "        print(\"Done with {} of {} articles\".format(n, len(articles)))\n",
    "    length = len(tokenizer(articles[title])['input_ids'])\n",
    "    for i in range(1,15):\n",
    "        if length < 2**i:\n",
    "            lengths[2**(i-1)] += 1\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 2: 0,\n",
       " 4: 0,\n",
       " 8: 0,\n",
       " 16: 0,\n",
       " 32: 0,\n",
       " 64: 20,\n",
       " 128: 2092,\n",
       " 256: 29581,\n",
       " 512: 240,\n",
       " 1024: 75,\n",
       " 2048: 6,\n",
       " 4096: 0,\n",
       " 8192: 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
