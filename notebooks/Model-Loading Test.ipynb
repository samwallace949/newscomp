{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as NN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class SentenceClassifier(NN.Module):\n",
    "    def __init__(self, labels):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "        for params in self.transformer.parameters():\n",
    "            params.requires_grad = False\n",
    "        \n",
    "        self.fc = NN.Linear(768, len(labels))\n",
    "        self.logits = NN.Softmax()\n",
    "        self.labels = labels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.logits(self.fc(torch.tensor(self.transformer.encode(x))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def find_attention_span(sentences, idx,tokenizer):\n",
    "\n",
    "    inp_len = 513\n",
    "\n",
    "    sents = [sent for sent in sentences]\n",
    "\n",
    "    removed_from_back = 0\n",
    "    removed_from_front = 0\n",
    "    \n",
    "    while True:\n",
    "        tokenizer_inp = \" \".join(sents)\n",
    "        tokenized = tokenizer(tokenizer_inp, padding=True, return_tensors='pt')\n",
    "        #print(\"Token shape:\", tokenized[\"input_ids\"].shape)\n",
    "        if tokenized[\"input_ids\"].shape[1] > 512:\n",
    "            if idx < len(sents)-1:\n",
    "                sents = sents[:-1]\n",
    "                removed_from_back += 1\n",
    "            else:\n",
    "                sents = sents[1:]\n",
    "                idx -= 1\n",
    "                removed_from_front += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    print(\"Removed\", removed_from_back, \"sentences from back\")\n",
    "    print(\"Removed\", removed_from_front, \"sentences from front\")\n",
    "    print(\"Attention Span:\", len(sents), \"sentences\")\n",
    "    return sents, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 299 sentences from back\n",
      "Removed 74 sentences from front\n",
      "Attention Span: 127 sentences\n",
      "operation took 1.056912899017334\n",
      "Final index: 126\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "sentences = [\"i like apples.\"]*500\n",
    "\n",
    "t0 = time.time()\n",
    "_,idx = find_attention_span(sentences, 200, tokenizer)\n",
    "print(\"operation took\", time.time()-t0)\n",
    "print(\"Final index:\", idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "all_spans = {}\n",
    "\n",
    "with open(\"../../mfc_v4.0/spans_no_context.json\", \"r\") as f:\n",
    "    all_spans = json.load(f)\n",
    "\n",
    "keys = list(all_spans.keys())\n",
    "\n",
    "for key in keys:\n",
    "    if key[-2:] != '.0':\n",
    "        del all_spans[key]\n",
    "        \n",
    "keys = list(all_spans.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reloaded = SentenceClassifier(keys)\n",
    "model_reloaded.load_state_dict(torch.load(\"./distilberta-mfc-no-context.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5195, 0.0260, 0.0730, 0.0143, 0.0745, 0.0224, 0.0543, 0.0840, 0.0103,\n",
       "        0.0207, 0.0104, 0.0228, 0.0060, 0.0600, 0.0018],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reloaded(\"the president's ratings plummeted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
