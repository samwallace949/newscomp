{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6251ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03375624 -0.06316338 -0.0316612  ...  0.03684864 -0.02036646\n",
      "  -0.01574   ]\n",
      " [-0.01409588  0.00091114 -0.00096315 ... -0.02571585 -0.00289072\n",
      "  -0.00579975]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "model.eval()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399937f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871a00fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f05da23b308>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2280ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13.0',\n",
       " '6.0',\n",
       " '1.0',\n",
       " '14.0',\n",
       " '12.0',\n",
       " '2.0',\n",
       " '11.0',\n",
       " '9.0',\n",
       " '3.0',\n",
       " '10.0',\n",
       " '5.0',\n",
       " '8.0',\n",
       " '4.0',\n",
       " '7.0',\n",
       " '15.0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "all_spans = {}\n",
    "\n",
    "with open(\"../../mfc_v4.0/spans_no_context.json\", \"r\") as f:\n",
    "    all_spans = json.load(f)\n",
    "\n",
    "keys = list(all_spans.keys())\n",
    "\n",
    "for key in keys:\n",
    "    if key[-2:] != '.0':\n",
    "        del all_spans[key]\n",
    "        \n",
    "keys = list(all_spans.keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe44cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as NN\n",
    "\n",
    "\n",
    "class SentenceClassifier(NN.Module):\n",
    "    def __init__(self, labels):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
    "        for params in self.transformer.parameters():\n",
    "            params.requires_grad = False\n",
    "        \n",
    "        self.fc = NN.Linear(768, len(labels))\n",
    "        self.logits = NN.Softmax()\n",
    "        self.labels = labels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.logits(self.fc(torch.tensor(self.transformer.encode(x))))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x = []\n",
    "# y = []\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# n_spans = sum(len(all_spans[key]) for key in all_spans)\n",
    "\n",
    "# batch_x = []\n",
    "# batch_y = []\n",
    "\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for label in all_spans:\n",
    "#         if count > 100:\n",
    "#             break\n",
    "#         for span in all_spans[label]:\n",
    "#             count += 1\n",
    "#             batch_x.append(span)\n",
    "#             batch_y.append(label)\n",
    "#             if len(batch_x) == 60:\n",
    "#                 tokenized = tokenizer(batch_x, return_tensors='pt', padding=True)\n",
    "#                 output = model(**tokenized)\n",
    "#                 x.append(output.last_hidden_state)\n",
    "#                 y.append(batch_y)\n",
    "#                 batch_x = []\n",
    "#                 batch_y = []\n",
    "#             if count % 100 == 0:\n",
    "#                 print(\"Embedding for span %d of %d computed\" % (count, n_spans))\n",
    "#                 print(x[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38d24fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1241d05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565299"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d3c9e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad9b5203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 0 : 1868.222412109375\n",
      "Test Loss 1858.1199951171875 \n",
      "\n",
      "Training Loss at step 1 : 1857.4749755859375\n",
      "Training Loss at step 2 : 1849.923095703125\n",
      "Training Loss at step 3 : 1843.0841064453125\n",
      "Training Loss at step 4 : 1836.74462890625\n",
      "Training Loss at step 5 : 1832.6336669921875\n",
      "Training Loss at step 6 : 1828.0020751953125\n",
      "Training Loss at step 7 : 1824.6148681640625\n",
      "Training Loss at step 8 : 1821.1578369140625\n",
      "Training Loss at step 9 : 1814.9979248046875\n",
      "Training Loss at step 10 : 1812.6513671875\n",
      "Test Loss 1811.1109619140625 \n",
      "\n",
      "Training Loss at step 11 : 1809.430419921875\n",
      "Training Loss at step 12 : 1803.488525390625\n",
      "Training Loss at step 13 : 1799.546875\n",
      "Training Loss at step 14 : 1798.8201904296875\n",
      "Training Loss at step 15 : 1796.674072265625\n",
      "Training Loss at step 16 : 1791.3427734375\n",
      "Training Loss at step 17 : 1790.881103515625\n",
      "Training Loss at step 18 : 1785.1810302734375\n",
      "Training Loss at step 19 : 1783.4910888671875\n",
      "Training Loss at step 20 : 1778.3236083984375\n",
      "Test Loss 1781.4981689453125 \n",
      "\n",
      "Training Loss at step 21 : 1773.876953125\n",
      "Training Loss at step 22 : 1776.9344482421875\n",
      "Training Loss at step 23 : 1770.4012451171875\n",
      "Training Loss at step 24 : 1772.951171875\n",
      "Training Loss at step 25 : 1768.4197998046875\n",
      "Training Loss at step 26 : 1763.8662109375\n",
      "Training Loss at step 27 : 1760.358154296875\n",
      "Training Loss at step 28 : 1757.6268310546875\n",
      "Training Loss at step 29 : 1759.2037353515625\n",
      "Training Loss at step 30 : 1756.152587890625\n",
      "Test Loss 1757.0341796875 \n",
      "\n",
      "Training Loss at step 31 : 1754.1708984375\n",
      "Training Loss at step 32 : 1752.997314453125\n",
      "Training Loss at step 33 : 1747.3448486328125\n",
      "Training Loss at step 34 : 1741.07763671875\n",
      "Training Loss at step 35 : 1741.819091796875\n",
      "Training Loss at step 36 : 1738.6407470703125\n",
      "Training Loss at step 37 : 1740.572021484375\n",
      "Training Loss at step 38 : 1736.281982421875\n",
      "Training Loss at step 39 : 1738.426025390625\n",
      "Training Loss at step 40 : 1733.9207763671875\n",
      "Test Loss 1735.386962890625 \n",
      "\n",
      "Training Loss at step 41 : 1728.3038330078125\n",
      "Training Loss at step 42 : 1731.752685546875\n",
      "Training Loss at step 43 : 1721.1424560546875\n",
      "Training Loss at step 44 : 1722.861083984375\n",
      "Training Loss at step 45 : 1722.240234375\n",
      "Training Loss at step 46 : 1714.3604736328125\n",
      "Training Loss at step 47 : 1716.0059814453125\n",
      "Training Loss at step 48 : 1718.236572265625\n",
      "Training Loss at step 49 : 1709.04345703125\n",
      "Training Loss at step 50 : 1709.1318359375\n",
      "Test Loss 1715.34033203125 \n",
      "\n",
      "Training Loss at step 51 : 1705.386962890625\n",
      "Training Loss at step 52 : 1696.2149658203125\n",
      "Training Loss at step 53 : 1706.797607421875\n",
      "Training Loss at step 54 : 1704.867919921875\n",
      "Training Loss at step 55 : 1699.0836181640625\n",
      "Training Loss at step 56 : 1702.11962890625\n",
      "Training Loss at step 57 : 1695.1033935546875\n",
      "Training Loss at step 58 : 1697.052734375\n",
      "Training Loss at step 59 : 1698.5303955078125\n",
      "Training Loss at step 60 : 1691.66455078125\n",
      "Test Loss 1696.78369140625 \n",
      "\n",
      "Training Loss at step 61 : 1687.1678466796875\n",
      "Training Loss at step 62 : 1687.53173828125\n",
      "Training Loss at step 63 : 1681.764404296875\n",
      "Training Loss at step 64 : 1676.2969970703125\n",
      "Training Loss at step 65 : 1679.133056640625\n",
      "Training Loss at step 66 : 1669.016357421875\n",
      "Training Loss at step 67 : 1673.1658935546875\n",
      "Training Loss at step 68 : 1671.510009765625\n",
      "Training Loss at step 69 : 1666.760009765625\n",
      "Training Loss at step 70 : 1667.806884765625\n",
      "Test Loss 1678.9150390625 \n",
      "\n",
      "Training Loss at step 71 : 1681.67236328125\n",
      "Training Loss at step 72 : 1661.9429931640625\n",
      "Training Loss at step 73 : 1673.7021484375\n",
      "Training Loss at step 74 : 1659.5167236328125\n",
      "Training Loss at step 75 : 1668.8878173828125\n",
      "Training Loss at step 76 : 1661.9052734375\n",
      "Training Loss at step 77 : 1654.65283203125\n",
      "Training Loss at step 78 : 1650.6094970703125\n",
      "Training Loss at step 79 : 1661.71435546875\n",
      "Training Loss at step 80 : 1648.7628173828125\n",
      "Test Loss 1662.5205078125 \n",
      "\n",
      "Training Loss at step 81 : 1654.8800048828125\n",
      "Training Loss at step 82 : 1663.802978515625\n",
      "Training Loss at step 83 : 1641.9375\n",
      "Training Loss at step 84 : 1650.8966064453125\n",
      "Training Loss at step 85 : 1644.975830078125\n",
      "Training Loss at step 86 : 1638.1741943359375\n",
      "Training Loss at step 87 : 1639.3367919921875\n",
      "Training Loss at step 88 : 1639.750732421875\n",
      "Training Loss at step 89 : 1633.3828125\n",
      "Training Loss at step 90 : 1651.2176513671875\n",
      "Test Loss 1646.977783203125 \n",
      "\n",
      "Training Loss at step 91 : 1638.070068359375\n",
      "Training Loss at step 92 : 1629.880126953125\n",
      "Training Loss at step 93 : 1631.185546875\n",
      "Training Loss at step 94 : 1638.0673828125\n",
      "Training Loss at step 95 : 1639.03515625\n",
      "Training Loss at step 96 : 1645.44091796875\n",
      "Training Loss at step 97 : 1627.0343017578125\n",
      "Training Loss at step 98 : 1633.3953857421875\n",
      "Training Loss at step 99 : 1627.1337890625\n",
      "Training Loss at step 100 : 1629.4124755859375\n",
      "Test Loss 1632.3267822265625 \n",
      "\n",
      "Training Loss at step 101 : 1619.380859375\n",
      "Training Loss at step 102 : 1627.072021484375\n",
      "Training Loss at step 103 : 1606.466552734375\n",
      "Training Loss at step 104 : 1618.1070556640625\n",
      "Training Loss at step 105 : 1620.203125\n",
      "Training Loss at step 106 : 1617.380615234375\n",
      "Training Loss at step 107 : 1614.158935546875\n",
      "Training Loss at step 108 : 1614.734619140625\n",
      "Training Loss at step 109 : 1608.7998046875\n",
      "Training Loss at step 110 : 1612.2425537109375\n",
      "Test Loss 1618.233154296875 \n",
      "\n",
      "Training Loss at step 111 : 1602.008056640625\n",
      "Training Loss at step 112 : 1607.918701171875\n",
      "Training Loss at step 113 : 1608.7557373046875\n",
      "Training Loss at step 114 : 1589.938720703125\n",
      "Training Loss at step 115 : 1616.515380859375\n",
      "Training Loss at step 116 : 1599.1197509765625\n",
      "Training Loss at step 117 : 1596.23388671875\n",
      "Training Loss at step 118 : 1574.099365234375\n",
      "Training Loss at step 119 : 1596.9483642578125\n",
      "Training Loss at step 120 : 1596.55810546875\n",
      "Test Loss 1604.6434326171875 \n",
      "\n",
      "Training Loss at step 121 : 1597.386474609375\n",
      "Training Loss at step 122 : 1585.026611328125\n",
      "Training Loss at step 123 : 1575.758544921875\n",
      "Training Loss at step 124 : 1601.54248046875\n",
      "Training Loss at step 125 : 1586.95751953125\n",
      "Training Loss at step 126 : 1581.88037109375\n",
      "Training Loss at step 127 : 1590.11181640625\n",
      "Training Loss at step 128 : 1588.7486572265625\n",
      "Training Loss at step 129 : 1580.239501953125\n",
      "Training Loss at step 130 : 1581.240966796875\n",
      "Test Loss 1591.8101806640625 \n",
      "\n",
      "Training Loss at step 131 : 1562.971923828125\n",
      "Training Loss at step 132 : 1584.082763671875\n",
      "Training Loss at step 133 : 1578.3154296875\n",
      "Training Loss at step 134 : 1583.789306640625\n",
      "Training Loss at step 135 : 1569.2021484375\n",
      "Training Loss at step 136 : 1557.0565185546875\n",
      "Training Loss at step 137 : 1569.1646728515625\n",
      "Training Loss at step 138 : 1583.080810546875\n",
      "Training Loss at step 139 : 1564.639404296875\n",
      "Training Loss at step 140 : 1565.3236083984375\n",
      "Test Loss 1579.4075927734375 \n",
      "\n",
      "Training Loss at step 141 : 1567.538818359375\n",
      "Training Loss at step 142 : 1561.064208984375\n",
      "Training Loss at step 143 : 1578.52685546875\n",
      "Training Loss at step 144 : 1563.0775146484375\n",
      "Training Loss at step 145 : 1554.2611083984375\n",
      "Training Loss at step 146 : 1550.44384765625\n",
      "Training Loss at step 147 : 1551.63818359375\n",
      "Training Loss at step 148 : 1547.90478515625\n",
      "Training Loss at step 149 : 1556.082763671875\n",
      "Training Loss at step 150 : 1550.79736328125\n",
      "Test Loss 1567.599365234375 \n",
      "\n",
      "Training Loss at step 151 : 1541.381591796875\n",
      "Training Loss at step 152 : 1552.7440185546875\n",
      "Training Loss at step 153 : 1559.815673828125\n",
      "Training Loss at step 154 : 1550.5458984375\n",
      "Training Loss at step 155 : 1539.4146728515625\n",
      "Training Loss at step 156 : 1554.4329833984375\n",
      "Training Loss at step 157 : 1551.9483642578125\n",
      "Training Loss at step 158 : 1537.943603515625\n",
      "Training Loss at step 159 : 1536.792236328125\n",
      "Training Loss at step 160 : 1535.175048828125\n",
      "Test Loss 1556.134521484375 \n",
      "\n",
      "Training Loss at step 161 : 1530.5308837890625\n",
      "Training Loss at step 162 : 1544.2484130859375\n",
      "Training Loss at step 163 : 1544.9490966796875\n",
      "Training Loss at step 164 : 1539.555419921875\n",
      "Training Loss at step 165 : 1539.208984375\n",
      "Training Loss at step 166 : 1535.299560546875\n",
      "Training Loss at step 167 : 1541.242431640625\n",
      "Training Loss at step 168 : 1531.301513671875\n",
      "Training Loss at step 169 : 1537.0675048828125\n",
      "Training Loss at step 170 : 1529.9619140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 1545.2646484375 \n",
      "\n",
      "Training Loss at step 171 : 1525.574462890625\n",
      "Training Loss at step 172 : 1524.8558349609375\n",
      "Training Loss at step 173 : 1520.491455078125\n",
      "Training Loss at step 174 : 1523.1361083984375\n",
      "Training Loss at step 175 : 1517.47412109375\n",
      "Training Loss at step 176 : 1520.424072265625\n",
      "Training Loss at step 177 : 1523.218017578125\n",
      "Training Loss at step 178 : 1516.8896484375\n",
      "Training Loss at step 179 : 1526.5234375\n",
      "Training Loss at step 180 : 1524.77978515625\n",
      "Test Loss 1534.85546875 \n",
      "\n",
      "Training Loss at step 181 : 1528.162109375\n",
      "Training Loss at step 182 : 1513.0426025390625\n",
      "Training Loss at step 183 : 1517.9013671875\n",
      "Training Loss at step 184 : 1519.04052734375\n",
      "Training Loss at step 185 : 1506.4822998046875\n",
      "Training Loss at step 186 : 1513.703369140625\n",
      "Training Loss at step 187 : 1534.5218505859375\n",
      "Training Loss at step 188 : 1503.529296875\n",
      "Training Loss at step 189 : 1520.7135009765625\n",
      "Training Loss at step 190 : 1514.23046875\n",
      "Test Loss 1524.8695068359375 \n",
      "\n",
      "Training Loss at step 191 : 1506.4217529296875\n",
      "Training Loss at step 192 : 1499.17431640625\n",
      "Training Loss at step 193 : 1511.0545654296875\n",
      "Training Loss at step 194 : 1493.984130859375\n",
      "Training Loss at step 195 : 1506.12109375\n",
      "Training Loss at step 196 : 1499.760009765625\n",
      "Training Loss at step 197 : 1495.733642578125\n",
      "Training Loss at step 198 : 1498.864013671875\n",
      "Training Loss at step 199 : 1521.0421142578125\n",
      "Training Loss at step 200 : 1489.6043701171875\n",
      "Test Loss 1515.0032958984375 \n",
      "\n",
      "Training Loss at step 201 : 1506.193115234375\n",
      "Training Loss at step 202 : 1503.5379638671875\n",
      "Training Loss at step 203 : 1504.8001708984375\n",
      "Training Loss at step 204 : 1498.3687744140625\n",
      "Training Loss at step 205 : 1491.287353515625\n",
      "Training Loss at step 206 : 1497.500244140625\n",
      "Training Loss at step 207 : 1498.790771484375\n",
      "Training Loss at step 208 : 1504.1070556640625\n",
      "Training Loss at step 209 : 1497.509521484375\n",
      "Training Loss at step 210 : 1492.496826171875\n",
      "Test Loss 1505.6893310546875 \n",
      "\n",
      "Training Loss at step 211 : 1472.1263427734375\n",
      "Training Loss at step 212 : 1498.928466796875\n",
      "Training Loss at step 213 : 1492.2818603515625\n",
      "Training Loss at step 214 : 1498.7666015625\n",
      "Training Loss at step 215 : 1491.423828125\n",
      "Training Loss at step 216 : 1461.1083984375\n",
      "Training Loss at step 217 : 1480.632080078125\n",
      "Training Loss at step 218 : 1475.2589111328125\n",
      "Training Loss at step 219 : 1477.109130859375\n",
      "Training Loss at step 220 : 1465.657470703125\n",
      "Test Loss 1496.435302734375 \n",
      "\n",
      "Training Loss at step 221 : 1486.9346923828125\n",
      "Training Loss at step 222 : 1485.724609375\n",
      "Training Loss at step 223 : 1464.2349853515625\n",
      "Training Loss at step 224 : 1477.8555908203125\n",
      "Training Loss at step 225 : 1492.273681640625\n",
      "Training Loss at step 226 : 1468.4395751953125\n",
      "Training Loss at step 227 : 1466.7119140625\n",
      "Training Loss at step 228 : 1484.2169189453125\n",
      "Training Loss at step 229 : 1470.5172119140625\n",
      "Training Loss at step 230 : 1462.5537109375\n",
      "Test Loss 1487.71533203125 \n",
      "\n",
      "Training Loss at step 231 : 1480.6737060546875\n",
      "Training Loss at step 232 : 1470.246826171875\n",
      "Training Loss at step 233 : 1482.5999755859375\n",
      "Training Loss at step 234 : 1468.8236083984375\n",
      "Training Loss at step 235 : 1479.1353759765625\n",
      "Training Loss at step 236 : 1469.463134765625\n",
      "Training Loss at step 237 : 1464.275146484375\n",
      "Training Loss at step 238 : 1443.181640625\n",
      "Training Loss at step 239 : 1461.7939453125\n",
      "Training Loss at step 240 : 1455.314453125\n",
      "Test Loss 1478.8958740234375 \n",
      "\n",
      "Training Loss at step 241 : 1459.0948486328125\n",
      "Training Loss at step 242 : 1450.767822265625\n",
      "Training Loss at step 243 : 1451.364501953125\n",
      "Training Loss at step 244 : 1461.673095703125\n",
      "Training Loss at step 245 : 1471.8280029296875\n",
      "Training Loss at step 246 : 1464.947265625\n",
      "Training Loss at step 247 : 1440.9918212890625\n",
      "Training Loss at step 248 : 1462.001708984375\n",
      "Training Loss at step 249 : 1447.5765380859375\n",
      "Training Loss at step 250 : 1452.326904296875\n",
      "Test Loss 1470.6043701171875 \n",
      "\n",
      "Training Loss at step 251 : 1445.13671875\n",
      "Training Loss at step 252 : 1468.9210205078125\n",
      "Training Loss at step 253 : 1464.3095703125\n",
      "Training Loss at step 254 : 1450.96533203125\n",
      "Training Loss at step 255 : 1441.7265625\n",
      "Training Loss at step 256 : 1440.8878173828125\n",
      "Training Loss at step 257 : 1444.931640625\n",
      "Training Loss at step 258 : 1451.7021484375\n",
      "Training Loss at step 259 : 1456.0841064453125\n",
      "Training Loss at step 260 : 1446.7161865234375\n",
      "Test Loss 1462.4459228515625 \n",
      "\n",
      "Training Loss at step 261 : 1445.1416015625\n",
      "Training Loss at step 262 : 1446.46240234375\n",
      "Training Loss at step 263 : 1441.7391357421875\n",
      "Training Loss at step 264 : 1443.6605224609375\n",
      "Training Loss at step 265 : 1413.5164794921875\n",
      "Training Loss at step 266 : 1433.14404296875\n",
      "Training Loss at step 267 : 1448.2628173828125\n",
      "Training Loss at step 268 : 1448.708740234375\n",
      "Training Loss at step 269 : 1437.7344970703125\n",
      "Training Loss at step 270 : 1431.256103515625\n",
      "Test Loss 1454.3736572265625 \n",
      "\n",
      "Training Loss at step 271 : 1416.5377197265625\n",
      "Training Loss at step 272 : 1430.4036865234375\n",
      "Training Loss at step 273 : 1434.8109130859375\n",
      "Training Loss at step 274 : 1436.95654296875\n",
      "Training Loss at step 275 : 1436.06494140625\n",
      "Training Loss at step 276 : 1405.26025390625\n",
      "Training Loss at step 277 : 1448.1810302734375\n",
      "Training Loss at step 278 : 1408.804931640625\n",
      "Training Loss at step 279 : 1432.61669921875\n",
      "Training Loss at step 280 : 1428.368408203125\n",
      "Test Loss 1446.627685546875 \n",
      "\n",
      "Starting training epoch 1\n",
      "Training Loss at step 0 : 1411.2613525390625\n",
      "Test Loss 1445.876220703125 \n",
      "\n",
      "Training Loss at step 1 : 1416.2083740234375\n",
      "Training Loss at step 2 : 1425.4931640625\n",
      "Training Loss at step 3 : 1420.5599365234375\n",
      "Training Loss at step 4 : 1427.1910400390625\n",
      "Training Loss at step 5 : 1398.886474609375\n",
      "Training Loss at step 6 : 1425.523193359375\n",
      "Training Loss at step 7 : 1419.989013671875\n",
      "Training Loss at step 8 : 1435.4241943359375\n",
      "Training Loss at step 9 : 1414.5921630859375\n",
      "Training Loss at step 10 : 1410.925537109375\n",
      "Test Loss 1438.3885498046875 \n",
      "\n",
      "Training Loss at step 11 : 1418.7177734375\n",
      "Training Loss at step 12 : 1428.752197265625\n",
      "Training Loss at step 13 : 1411.284423828125\n",
      "Training Loss at step 14 : 1432.99609375\n",
      "Training Loss at step 15 : 1421.384765625\n",
      "Training Loss at step 16 : 1396.0460205078125\n",
      "Training Loss at step 17 : 1413.326904296875\n",
      "Training Loss at step 18 : 1406.4453125\n",
      "Training Loss at step 19 : 1426.181396484375\n",
      "Training Loss at step 20 : 1405.416748046875\n",
      "Test Loss 1430.9716796875 \n",
      "\n",
      "Training Loss at step 21 : 1415.7867431640625\n",
      "Training Loss at step 22 : 1404.88037109375\n",
      "Training Loss at step 23 : 1423.59033203125\n",
      "Training Loss at step 24 : 1417.479736328125\n",
      "Training Loss at step 25 : 1402.8956298828125\n",
      "Training Loss at step 26 : 1410.728759765625\n",
      "Training Loss at step 27 : 1393.875244140625\n",
      "Training Loss at step 28 : 1388.9290771484375\n",
      "Training Loss at step 29 : 1410.4097900390625\n",
      "Training Loss at step 30 : 1422.278076171875\n",
      "Test Loss 1423.7825927734375 \n",
      "\n",
      "Training Loss at step 31 : 1398.532958984375\n",
      "Training Loss at step 32 : 1400.56787109375\n",
      "Training Loss at step 33 : 1412.5294189453125\n",
      "Training Loss at step 34 : 1435.2696533203125\n",
      "Training Loss at step 35 : 1400.90283203125\n",
      "Training Loss at step 36 : 1419.335205078125\n",
      "Training Loss at step 37 : 1423.445556640625\n",
      "Training Loss at step 38 : 1396.917236328125\n",
      "Training Loss at step 39 : 1390.43310546875\n",
      "Training Loss at step 40 : 1393.9124755859375\n",
      "Test Loss 1416.94970703125 \n",
      "\n",
      "Training Loss at step 41 : 1402.4290771484375\n",
      "Training Loss at step 42 : 1402.71630859375\n",
      "Training Loss at step 43 : 1388.71044921875\n",
      "Training Loss at step 44 : 1396.959716796875\n",
      "Training Loss at step 45 : 1403.3446044921875\n",
      "Training Loss at step 46 : 1400.07568359375\n",
      "Training Loss at step 47 : 1409.299560546875\n",
      "Training Loss at step 48 : 1372.5306396484375\n",
      "Training Loss at step 49 : 1395.74853515625\n",
      "Training Loss at step 50 : 1388.435791015625\n",
      "Test Loss 1410.0157470703125 \n",
      "\n",
      "Training Loss at step 51 : 1384.1529541015625\n",
      "Training Loss at step 52 : 1389.68994140625\n",
      "Training Loss at step 53 : 1399.8082275390625\n",
      "Training Loss at step 54 : 1399.0858154296875\n",
      "Training Loss at step 55 : 1374.9866943359375\n",
      "Training Loss at step 56 : 1377.1241455078125\n",
      "Training Loss at step 57 : 1379.2149658203125\n",
      "Training Loss at step 58 : 1375.1697998046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 59 : 1382.5343017578125\n",
      "Training Loss at step 60 : 1382.1385498046875\n",
      "Test Loss 1403.2335205078125 \n",
      "\n",
      "Training Loss at step 61 : 1357.2255859375\n",
      "Training Loss at step 62 : 1416.2587890625\n",
      "Training Loss at step 63 : 1353.314208984375\n",
      "Training Loss at step 64 : 1398.02001953125\n",
      "Training Loss at step 65 : 1379.669189453125\n",
      "Training Loss at step 66 : 1376.7603759765625\n",
      "Training Loss at step 67 : 1362.9620361328125\n",
      "Training Loss at step 68 : 1404.33154296875\n",
      "Training Loss at step 69 : 1406.15234375\n",
      "Training Loss at step 70 : 1408.9962158203125\n",
      "Test Loss 1396.64453125 \n",
      "\n",
      "Training Loss at step 71 : 1359.426513671875\n",
      "Training Loss at step 72 : 1385.41015625\n",
      "Training Loss at step 73 : 1367.0426025390625\n",
      "Training Loss at step 74 : 1399.64892578125\n",
      "Training Loss at step 75 : 1380.9686279296875\n",
      "Training Loss at step 76 : 1362.0601806640625\n",
      "Training Loss at step 77 : 1373.19873046875\n",
      "Training Loss at step 78 : 1364.6058349609375\n",
      "Training Loss at step 79 : 1383.7236328125\n",
      "Training Loss at step 80 : 1354.6319580078125\n",
      "Test Loss 1390.1368408203125 \n",
      "\n",
      "Training Loss at step 81 : 1377.935302734375\n",
      "Training Loss at step 82 : 1380.9801025390625\n",
      "Training Loss at step 83 : 1347.3983154296875\n",
      "Training Loss at step 84 : 1374.04052734375\n",
      "Training Loss at step 85 : 1371.34521484375\n",
      "Training Loss at step 86 : 1365.5460205078125\n",
      "Training Loss at step 87 : 1392.784912109375\n",
      "Training Loss at step 88 : 1372.44189453125\n",
      "Training Loss at step 89 : 1346.6488037109375\n",
      "Training Loss at step 90 : 1366.228759765625\n",
      "Test Loss 1383.8160400390625 \n",
      "\n",
      "Training Loss at step 91 : 1352.541259765625\n",
      "Training Loss at step 92 : 1353.4149169921875\n",
      "Training Loss at step 93 : 1364.3787841796875\n",
      "Training Loss at step 94 : 1355.541015625\n",
      "Training Loss at step 95 : 1367.2032470703125\n",
      "Training Loss at step 96 : 1346.13427734375\n",
      "Training Loss at step 97 : 1380.5762939453125\n",
      "Training Loss at step 98 : 1380.594482421875\n",
      "Training Loss at step 99 : 1364.9627685546875\n",
      "Training Loss at step 100 : 1365.8321533203125\n",
      "Test Loss 1377.7928466796875 \n",
      "\n",
      "Training Loss at step 101 : 1354.5545654296875\n",
      "Training Loss at step 102 : 1356.5211181640625\n",
      "Training Loss at step 103 : 1353.061279296875\n",
      "Training Loss at step 104 : 1349.731689453125\n",
      "Training Loss at step 105 : 1348.4097900390625\n",
      "Training Loss at step 106 : 1383.4232177734375\n",
      "Training Loss at step 107 : 1341.1512451171875\n",
      "Training Loss at step 108 : 1324.249267578125\n",
      "Training Loss at step 109 : 1374.656494140625\n",
      "Training Loss at step 110 : 1379.015625\n",
      "Test Loss 1371.8697509765625 \n",
      "\n",
      "Training Loss at step 111 : 1374.1234130859375\n",
      "Training Loss at step 112 : 1378.945068359375\n",
      "Training Loss at step 113 : 1376.9705810546875\n",
      "Training Loss at step 114 : 1363.69775390625\n",
      "Training Loss at step 115 : 1333.41162109375\n",
      "Training Loss at step 116 : 1328.0064697265625\n",
      "Training Loss at step 117 : 1347.724609375\n",
      "Training Loss at step 118 : 1312.693115234375\n",
      "Training Loss at step 119 : 1332.5692138671875\n",
      "Training Loss at step 120 : 1367.42041015625\n",
      "Test Loss 1366.0865478515625 \n",
      "\n",
      "Training Loss at step 121 : 1366.61669921875\n",
      "Training Loss at step 122 : 1338.615966796875\n",
      "Training Loss at step 123 : 1339.62646484375\n",
      "Training Loss at step 124 : 1322.618896484375\n",
      "Training Loss at step 125 : 1360.385986328125\n",
      "Training Loss at step 126 : 1328.635986328125\n",
      "Training Loss at step 127 : 1363.289794921875\n",
      "Training Loss at step 128 : 1342.369873046875\n",
      "Training Loss at step 129 : 1355.7723388671875\n",
      "Training Loss at step 130 : 1332.64111328125\n",
      "Test Loss 1360.09619140625 \n",
      "\n",
      "Training Loss at step 131 : 1333.0697021484375\n",
      "Training Loss at step 132 : 1346.2998046875\n",
      "Training Loss at step 133 : 1333.481201171875\n",
      "Training Loss at step 134 : 1312.45166015625\n",
      "Training Loss at step 135 : 1325.000244140625\n",
      "Training Loss at step 136 : 1345.45263671875\n",
      "Training Loss at step 137 : 1329.9197998046875\n",
      "Training Loss at step 138 : 1295.5101318359375\n",
      "Training Loss at step 139 : 1329.6396484375\n",
      "Training Loss at step 140 : 1338.2581787109375\n",
      "Test Loss 1354.295166015625 \n",
      "\n",
      "Training Loss at step 141 : 1307.69873046875\n",
      "Training Loss at step 142 : 1341.743408203125\n",
      "Training Loss at step 143 : 1321.585205078125\n",
      "Training Loss at step 144 : 1317.1337890625\n",
      "Training Loss at step 145 : 1314.0565185546875\n",
      "Training Loss at step 146 : 1338.9107666015625\n",
      "Training Loss at step 147 : 1314.8648681640625\n",
      "Training Loss at step 148 : 1358.9339599609375\n",
      "Training Loss at step 149 : 1331.3189697265625\n",
      "Training Loss at step 150 : 1318.694580078125\n",
      "Test Loss 1348.6153564453125 \n",
      "\n",
      "Training Loss at step 151 : 1314.1971435546875\n",
      "Training Loss at step 152 : 1348.880615234375\n",
      "Training Loss at step 153 : 1338.4197998046875\n",
      "Training Loss at step 154 : 1324.9095458984375\n",
      "Training Loss at step 155 : 1301.453125\n",
      "Training Loss at step 156 : 1323.37548828125\n",
      "Training Loss at step 157 : 1309.05859375\n",
      "Training Loss at step 158 : 1306.826904296875\n",
      "Training Loss at step 159 : 1290.627685546875\n",
      "Training Loss at step 160 : 1304.609375\n",
      "Test Loss 1343.008056640625 \n",
      "\n",
      "Training Loss at step 161 : 1330.0977783203125\n",
      "Training Loss at step 162 : 1322.2872314453125\n",
      "Training Loss at step 163 : 1329.192138671875\n",
      "Training Loss at step 164 : 1343.058837890625\n",
      "Training Loss at step 165 : 1316.4942626953125\n",
      "Training Loss at step 166 : 1308.00732421875\n",
      "Training Loss at step 167 : 1310.957275390625\n",
      "Training Loss at step 168 : 1300.1033935546875\n",
      "Training Loss at step 169 : 1318.5498046875\n",
      "Training Loss at step 170 : 1308.4864501953125\n",
      "Test Loss 1337.6724853515625 \n",
      "\n",
      "Training Loss at step 171 : 1306.982666015625\n",
      "Training Loss at step 172 : 1290.908203125\n",
      "Training Loss at step 173 : 1330.9146728515625\n",
      "Training Loss at step 174 : 1314.3602294921875\n",
      "Training Loss at step 175 : 1306.6180419921875\n",
      "Training Loss at step 176 : 1315.2918701171875\n",
      "Training Loss at step 177 : 1322.167724609375\n",
      "Training Loss at step 178 : 1309.883544921875\n",
      "Training Loss at step 179 : 1318.7613525390625\n",
      "Training Loss at step 180 : 1316.3470458984375\n",
      "Test Loss 1332.4256591796875 \n",
      "\n",
      "Training Loss at step 181 : 1323.486328125\n",
      "Training Loss at step 182 : 1310.900634765625\n",
      "Training Loss at step 183 : 1282.126708984375\n",
      "Training Loss at step 184 : 1323.2947998046875\n",
      "Training Loss at step 185 : 1311.5111083984375\n",
      "Training Loss at step 186 : 1319.002197265625\n",
      "Training Loss at step 187 : 1308.6453857421875\n",
      "Training Loss at step 188 : 1310.9359130859375\n",
      "Training Loss at step 189 : 1312.7734375\n",
      "Training Loss at step 190 : 1305.5106201171875\n",
      "Test Loss 1327.227783203125 \n",
      "\n",
      "Training Loss at step 191 : 1310.605712890625\n",
      "Training Loss at step 192 : 1306.223876953125\n",
      "Training Loss at step 193 : 1323.92919921875\n",
      "Training Loss at step 194 : 1324.860107421875\n",
      "Training Loss at step 195 : 1294.036376953125\n",
      "Training Loss at step 196 : 1323.5496826171875\n",
      "Training Loss at step 197 : 1277.1376953125\n",
      "Training Loss at step 198 : 1294.333740234375\n",
      "Training Loss at step 199 : 1305.595458984375\n",
      "Training Loss at step 200 : 1309.9053955078125\n",
      "Test Loss 1322.0347900390625 \n",
      "\n",
      "Training Loss at step 201 : 1313.513916015625\n",
      "Training Loss at step 202 : 1279.162109375\n",
      "Training Loss at step 203 : 1296.939208984375\n",
      "Training Loss at step 204 : 1321.011474609375\n",
      "Training Loss at step 205 : 1277.585693359375\n",
      "Training Loss at step 206 : 1301.325927734375\n",
      "Training Loss at step 207 : 1296.340576171875\n",
      "Training Loss at step 208 : 1306.204833984375\n",
      "Training Loss at step 209 : 1295.012939453125\n",
      "Training Loss at step 210 : 1285.627685546875\n",
      "Test Loss 1317.077880859375 \n",
      "\n",
      "Training Loss at step 211 : 1308.1982421875\n",
      "Training Loss at step 212 : 1300.197509765625\n",
      "Training Loss at step 213 : 1294.505859375\n",
      "Training Loss at step 214 : 1278.5240478515625\n",
      "Training Loss at step 215 : 1313.6529541015625\n",
      "Training Loss at step 216 : 1282.1839599609375\n",
      "Training Loss at step 217 : 1294.1380615234375\n",
      "Training Loss at step 218 : 1289.3843994140625\n",
      "Training Loss at step 219 : 1304.57763671875\n",
      "Training Loss at step 220 : 1291.2738037109375\n",
      "Test Loss 1312.1715087890625 \n",
      "\n",
      "Training Loss at step 221 : 1313.431396484375\n",
      "Training Loss at step 222 : 1273.46435546875\n",
      "Training Loss at step 223 : 1301.02978515625\n",
      "Training Loss at step 224 : 1288.3118896484375\n",
      "Training Loss at step 225 : 1282.7535400390625\n",
      "Training Loss at step 226 : 1278.7906494140625\n",
      "Training Loss at step 227 : 1292.1248779296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 228 : 1260.852783203125\n",
      "Training Loss at step 229 : 1280.31103515625\n",
      "Training Loss at step 230 : 1289.2547607421875\n",
      "Test Loss 1307.501220703125 \n",
      "\n",
      "Training Loss at step 231 : 1271.624267578125\n",
      "Training Loss at step 232 : 1279.506591796875\n",
      "Training Loss at step 233 : 1291.279296875\n",
      "Training Loss at step 234 : 1283.66015625\n",
      "Training Loss at step 235 : 1294.265869140625\n",
      "Training Loss at step 236 : 1279.3817138671875\n",
      "Training Loss at step 237 : 1271.9676513671875\n",
      "Training Loss at step 238 : 1294.8349609375\n",
      "Training Loss at step 239 : 1257.4254150390625\n",
      "Training Loss at step 240 : 1301.2109375\n",
      "Test Loss 1302.6484375 \n",
      "\n",
      "Training Loss at step 241 : 1293.7587890625\n",
      "Training Loss at step 242 : 1278.7684326171875\n",
      "Training Loss at step 243 : 1300.623779296875\n",
      "Training Loss at step 244 : 1272.8924560546875\n",
      "Training Loss at step 245 : 1284.9464111328125\n",
      "Training Loss at step 246 : 1282.62109375\n",
      "Training Loss at step 247 : 1278.336181640625\n",
      "Training Loss at step 248 : 1280.644287109375\n",
      "Training Loss at step 249 : 1305.479248046875\n",
      "Training Loss at step 250 : 1264.249267578125\n",
      "Test Loss 1298.0125732421875 \n",
      "\n",
      "Training Loss at step 251 : 1293.30419921875\n",
      "Training Loss at step 252 : 1282.588134765625\n",
      "Training Loss at step 253 : 1285.38427734375\n",
      "Training Loss at step 254 : 1275.978271484375\n",
      "Training Loss at step 255 : 1305.09228515625\n",
      "Training Loss at step 256 : 1279.2344970703125\n",
      "Training Loss at step 257 : 1287.2431640625\n",
      "Training Loss at step 258 : 1276.670654296875\n",
      "Training Loss at step 259 : 1279.6888427734375\n",
      "Training Loss at step 260 : 1289.51904296875\n",
      "Test Loss 1293.4981689453125 \n",
      "\n",
      "Training Loss at step 261 : 1257.6927490234375\n",
      "Training Loss at step 262 : 1268.2427978515625\n",
      "Training Loss at step 263 : 1280.35546875\n",
      "Training Loss at step 264 : 1276.076416015625\n",
      "Training Loss at step 265 : 1289.6456298828125\n",
      "Training Loss at step 266 : 1267.9825439453125\n",
      "Training Loss at step 267 : 1274.6820068359375\n",
      "Training Loss at step 268 : 1258.03857421875\n",
      "Training Loss at step 269 : 1261.3408203125\n",
      "Training Loss at step 270 : 1260.312744140625\n",
      "Test Loss 1289.0814208984375 \n",
      "\n",
      "Training Loss at step 271 : 1251.695068359375\n",
      "Training Loss at step 272 : 1256.495361328125\n",
      "Training Loss at step 273 : 1270.1153564453125\n",
      "Training Loss at step 274 : 1267.9456787109375\n",
      "Training Loss at step 275 : 1275.171630859375\n",
      "Training Loss at step 276 : 1269.88720703125\n",
      "Training Loss at step 277 : 1282.828369140625\n",
      "Training Loss at step 278 : 1278.42431640625\n",
      "Training Loss at step 279 : 1254.6199951171875\n",
      "Training Loss at step 280 : 1283.2259521484375\n",
      "Test Loss 1284.74462890625 \n",
      "\n",
      "Starting training epoch 2\n",
      "Training Loss at step 0 : 1271.248046875\n",
      "Test Loss 1284.279541015625 \n",
      "\n",
      "Training Loss at step 1 : 1296.6771240234375\n",
      "Training Loss at step 2 : 1278.9666748046875\n",
      "Training Loss at step 3 : 1264.8681640625\n",
      "Training Loss at step 4 : 1272.51513671875\n",
      "Training Loss at step 5 : 1261.751953125\n",
      "Training Loss at step 6 : 1262.051513671875\n",
      "Training Loss at step 7 : 1293.224609375\n",
      "Training Loss at step 8 : 1269.41796875\n",
      "Training Loss at step 9 : 1278.4195556640625\n",
      "Training Loss at step 10 : 1267.8780517578125\n",
      "Test Loss 1279.9874267578125 \n",
      "\n",
      "Training Loss at step 11 : 1263.0120849609375\n",
      "Training Loss at step 12 : 1286.23486328125\n",
      "Training Loss at step 13 : 1257.703125\n",
      "Training Loss at step 14 : 1252.2286376953125\n",
      "Training Loss at step 15 : 1255.99658203125\n",
      "Training Loss at step 16 : 1270.8826904296875\n",
      "Training Loss at step 17 : 1270.268310546875\n",
      "Training Loss at step 18 : 1243.8465576171875\n",
      "Training Loss at step 19 : 1265.25341796875\n",
      "Training Loss at step 20 : 1260.1146240234375\n",
      "Test Loss 1275.6982421875 \n",
      "\n",
      "Training Loss at step 21 : 1281.696533203125\n",
      "Training Loss at step 22 : 1253.01806640625\n",
      "Training Loss at step 23 : 1265.390380859375\n",
      "Training Loss at step 24 : 1285.8638916015625\n",
      "Training Loss at step 25 : 1268.5201416015625\n",
      "Training Loss at step 26 : 1259.4483642578125\n",
      "Training Loss at step 27 : 1267.5966796875\n",
      "Training Loss at step 28 : 1274.4654541015625\n",
      "Training Loss at step 29 : 1234.5498046875\n",
      "Training Loss at step 30 : 1238.50390625\n",
      "Test Loss 1271.41845703125 \n",
      "\n",
      "Training Loss at step 31 : 1253.3739013671875\n",
      "Training Loss at step 32 : 1255.5318603515625\n",
      "Training Loss at step 33 : 1267.457763671875\n",
      "Training Loss at step 34 : 1242.9698486328125\n",
      "Training Loss at step 35 : 1220.608154296875\n",
      "Training Loss at step 36 : 1236.91845703125\n",
      "Training Loss at step 37 : 1246.65380859375\n",
      "Training Loss at step 38 : 1245.031005859375\n",
      "Training Loss at step 39 : 1251.9639892578125\n",
      "Training Loss at step 40 : 1257.033447265625\n",
      "Test Loss 1267.375 \n",
      "\n",
      "Training Loss at step 41 : 1233.70166015625\n",
      "Training Loss at step 42 : 1237.2489013671875\n",
      "Training Loss at step 43 : 1257.2794189453125\n",
      "Training Loss at step 44 : 1252.201904296875\n",
      "Training Loss at step 45 : 1256.341064453125\n",
      "Training Loss at step 46 : 1267.7244873046875\n",
      "Training Loss at step 47 : 1217.65087890625\n",
      "Training Loss at step 48 : 1235.648681640625\n",
      "Training Loss at step 49 : 1214.2989501953125\n",
      "Training Loss at step 50 : 1257.8818359375\n",
      "Test Loss 1263.473388671875 \n",
      "\n",
      "Training Loss at step 51 : 1268.048828125\n",
      "Training Loss at step 52 : 1260.290283203125\n",
      "Training Loss at step 53 : 1227.725341796875\n",
      "Training Loss at step 54 : 1242.98828125\n",
      "Training Loss at step 55 : 1266.076904296875\n",
      "Training Loss at step 56 : 1232.6142578125\n",
      "Training Loss at step 57 : 1253.378662109375\n",
      "Training Loss at step 58 : 1282.478759765625\n",
      "Training Loss at step 59 : 1226.2421875\n",
      "Training Loss at step 60 : 1242.4637451171875\n",
      "Test Loss 1259.6590576171875 \n",
      "\n",
      "Training Loss at step 61 : 1228.2618408203125\n",
      "Training Loss at step 62 : 1238.3768310546875\n",
      "Training Loss at step 63 : 1201.88037109375\n",
      "Training Loss at step 64 : 1211.54638671875\n",
      "Training Loss at step 65 : 1208.806396484375\n",
      "Training Loss at step 66 : 1226.063720703125\n",
      "Training Loss at step 67 : 1247.7939453125\n",
      "Training Loss at step 68 : 1228.37353515625\n",
      "Training Loss at step 69 : 1230.94140625\n",
      "Training Loss at step 70 : 1246.359619140625\n",
      "Test Loss 1255.85693359375 \n",
      "\n",
      "Training Loss at step 71 : 1244.9620361328125\n",
      "Training Loss at step 72 : 1229.16015625\n",
      "Training Loss at step 73 : 1248.70556640625\n",
      "Training Loss at step 74 : 1254.8721923828125\n",
      "Training Loss at step 75 : 1215.963134765625\n",
      "Training Loss at step 76 : 1222.560791015625\n",
      "Training Loss at step 77 : 1235.8145751953125\n",
      "Training Loss at step 78 : 1238.623779296875\n",
      "Training Loss at step 79 : 1224.0556640625\n",
      "Training Loss at step 80 : 1234.9486083984375\n",
      "Test Loss 1252.2012939453125 \n",
      "\n",
      "Training Loss at step 81 : 1236.8909912109375\n",
      "Training Loss at step 82 : 1203.259765625\n",
      "Training Loss at step 83 : 1233.7479248046875\n",
      "Training Loss at step 84 : 1235.6697998046875\n",
      "Training Loss at step 85 : 1215.514892578125\n",
      "Training Loss at step 86 : 1236.4361572265625\n",
      "Training Loss at step 87 : 1235.05224609375\n",
      "Training Loss at step 88 : 1244.9505615234375\n",
      "Training Loss at step 89 : 1212.0357666015625\n",
      "Training Loss at step 90 : 1205.847412109375\n",
      "Test Loss 1248.44189453125 \n",
      "\n",
      "Training Loss at step 91 : 1224.849609375\n",
      "Training Loss at step 92 : 1243.2037353515625\n",
      "Training Loss at step 93 : 1221.46533203125\n",
      "Training Loss at step 94 : 1241.9678955078125\n",
      "Training Loss at step 95 : 1213.189208984375\n",
      "Training Loss at step 96 : 1181.56494140625\n",
      "Training Loss at step 97 : 1217.976806640625\n",
      "Training Loss at step 98 : 1221.334228515625\n",
      "Training Loss at step 99 : 1217.1988525390625\n",
      "Training Loss at step 100 : 1217.8990478515625\n",
      "Test Loss 1244.8095703125 \n",
      "\n",
      "Training Loss at step 101 : 1234.6826171875\n",
      "Training Loss at step 102 : 1217.795654296875\n",
      "Training Loss at step 103 : 1214.634521484375\n",
      "Training Loss at step 104 : 1233.9593505859375\n",
      "Training Loss at step 105 : 1205.628173828125\n",
      "Training Loss at step 106 : 1221.3284912109375\n",
      "Training Loss at step 107 : 1215.3916015625\n",
      "Training Loss at step 108 : 1226.954833984375\n",
      "Training Loss at step 109 : 1234.3060302734375\n",
      "Training Loss at step 110 : 1196.5107421875\n",
      "Test Loss 1241.0496826171875 \n",
      "\n",
      "Training Loss at step 111 : 1233.2763671875\n",
      "Training Loss at step 112 : 1236.4368896484375\n",
      "Training Loss at step 113 : 1217.829345703125\n",
      "Training Loss at step 114 : 1222.0396728515625\n",
      "Training Loss at step 115 : 1225.7396240234375\n",
      "Training Loss at step 116 : 1248.40185546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 117 : 1230.3494873046875\n",
      "Training Loss at step 118 : 1211.662353515625\n",
      "Training Loss at step 119 : 1217.0941162109375\n",
      "Training Loss at step 120 : 1198.3387451171875\n",
      "Test Loss 1237.392333984375 \n",
      "\n",
      "Training Loss at step 121 : 1232.5809326171875\n",
      "Training Loss at step 122 : 1229.1885986328125\n",
      "Training Loss at step 123 : 1233.0313720703125\n",
      "Training Loss at step 124 : 1237.5166015625\n",
      "Training Loss at step 125 : 1202.951416015625\n",
      "Training Loss at step 126 : 1195.99072265625\n",
      "Training Loss at step 127 : 1211.8790283203125\n",
      "Training Loss at step 128 : 1211.943115234375\n",
      "Training Loss at step 129 : 1195.049072265625\n",
      "Training Loss at step 130 : 1182.4150390625\n",
      "Test Loss 1233.848876953125 \n",
      "\n",
      "Training Loss at step 131 : 1245.3917236328125\n",
      "Training Loss at step 132 : 1209.510009765625\n",
      "Training Loss at step 133 : 1169.7625732421875\n",
      "Training Loss at step 134 : 1191.270751953125\n",
      "Training Loss at step 135 : 1209.239501953125\n",
      "Training Loss at step 136 : 1201.0711669921875\n",
      "Training Loss at step 137 : 1221.26025390625\n",
      "Training Loss at step 138 : 1196.30810546875\n",
      "Training Loss at step 139 : 1226.2994384765625\n",
      "Training Loss at step 140 : 1207.6524658203125\n",
      "Test Loss 1230.42431640625 \n",
      "\n",
      "Training Loss at step 141 : 1194.7659912109375\n",
      "Training Loss at step 142 : 1201.326416015625\n",
      "Training Loss at step 143 : 1218.249755859375\n",
      "Training Loss at step 144 : 1230.526611328125\n",
      "Training Loss at step 145 : 1197.151611328125\n",
      "Training Loss at step 146 : 1217.713134765625\n",
      "Training Loss at step 147 : 1192.0865478515625\n",
      "Training Loss at step 148 : 1213.21875\n",
      "Training Loss at step 149 : 1199.321533203125\n",
      "Training Loss at step 150 : 1207.9525146484375\n",
      "Test Loss 1226.8372802734375 \n",
      "\n",
      "Training Loss at step 151 : 1208.631591796875\n",
      "Training Loss at step 152 : 1218.693115234375\n",
      "Training Loss at step 153 : 1220.38134765625\n",
      "Training Loss at step 154 : 1221.566162109375\n",
      "Training Loss at step 155 : 1199.095703125\n",
      "Training Loss at step 156 : 1231.5770263671875\n",
      "Training Loss at step 157 : 1220.34765625\n",
      "Training Loss at step 158 : 1185.344970703125\n",
      "Training Loss at step 159 : 1189.2481689453125\n",
      "Training Loss at step 160 : 1225.61572265625\n",
      "Test Loss 1223.6217041015625 \n",
      "\n",
      "Training Loss at step 161 : 1201.506591796875\n",
      "Training Loss at step 162 : 1197.549072265625\n",
      "Training Loss at step 163 : 1225.4915771484375\n",
      "Training Loss at step 164 : 1225.9429931640625\n",
      "Training Loss at step 165 : 1196.057861328125\n",
      "Training Loss at step 166 : 1211.19970703125\n",
      "Training Loss at step 167 : 1215.3858642578125\n",
      "Training Loss at step 168 : 1201.630615234375\n",
      "Training Loss at step 169 : 1212.35205078125\n",
      "Training Loss at step 170 : 1195.286865234375\n",
      "Test Loss 1220.5762939453125 \n",
      "\n",
      "Training Loss at step 171 : 1202.390625\n",
      "Training Loss at step 172 : 1198.8668212890625\n",
      "Training Loss at step 173 : 1178.84765625\n",
      "Training Loss at step 174 : 1212.3712158203125\n",
      "Training Loss at step 175 : 1161.8277587890625\n",
      "Training Loss at step 176 : 1213.6014404296875\n",
      "Training Loss at step 177 : 1207.74365234375\n",
      "Training Loss at step 178 : 1203.1492919921875\n",
      "Training Loss at step 179 : 1215.26611328125\n",
      "Training Loss at step 180 : 1232.9256591796875\n",
      "Test Loss 1217.405029296875 \n",
      "\n",
      "Training Loss at step 181 : 1200.4525146484375\n",
      "Training Loss at step 182 : 1191.4005126953125\n",
      "Training Loss at step 183 : 1205.3525390625\n",
      "Training Loss at step 184 : 1198.1854248046875\n",
      "Training Loss at step 185 : 1210.1884765625\n",
      "Training Loss at step 186 : 1201.298583984375\n",
      "Training Loss at step 187 : 1185.001708984375\n",
      "Training Loss at step 188 : 1160.6845703125\n",
      "Training Loss at step 189 : 1193.774169921875\n",
      "Training Loss at step 190 : 1193.24169921875\n",
      "Test Loss 1214.2298583984375 \n",
      "\n",
      "Training Loss at step 191 : 1201.1339111328125\n",
      "Training Loss at step 192 : 1212.747314453125\n",
      "Training Loss at step 193 : 1185.7535400390625\n",
      "Training Loss at step 194 : 1185.799072265625\n",
      "Training Loss at step 195 : 1160.5177001953125\n",
      "Training Loss at step 196 : 1181.377685546875\n",
      "Training Loss at step 197 : 1185.591552734375\n",
      "Training Loss at step 198 : 1203.07763671875\n",
      "Training Loss at step 199 : 1166.040283203125\n",
      "Training Loss at step 200 : 1187.16748046875\n",
      "Test Loss 1211.2012939453125 \n",
      "\n",
      "Training Loss at step 201 : 1179.798095703125\n",
      "Training Loss at step 202 : 1173.968017578125\n",
      "Training Loss at step 203 : 1183.05126953125\n",
      "Training Loss at step 204 : 1195.684814453125\n",
      "Training Loss at step 205 : 1161.0738525390625\n",
      "Training Loss at step 206 : 1183.2034912109375\n",
      "Training Loss at step 207 : 1170.4234619140625\n",
      "Training Loss at step 208 : 1141.83837890625\n",
      "Training Loss at step 209 : 1168.765625\n",
      "Training Loss at step 210 : 1200.08251953125\n",
      "Test Loss 1208.2601318359375 \n",
      "\n",
      "Training Loss at step 211 : 1183.434814453125\n",
      "Training Loss at step 212 : 1178.6337890625\n",
      "Training Loss at step 213 : 1185.9996337890625\n",
      "Training Loss at step 214 : 1144.1123046875\n",
      "Training Loss at step 215 : 1147.186279296875\n",
      "Training Loss at step 216 : 1190.883056640625\n",
      "Training Loss at step 217 : 1190.005615234375\n",
      "Training Loss at step 218 : 1176.8555908203125\n",
      "Training Loss at step 219 : 1185.8851318359375\n",
      "Training Loss at step 220 : 1181.5498046875\n",
      "Test Loss 1205.18994140625 \n",
      "\n",
      "Training Loss at step 221 : 1170.5264892578125\n",
      "Training Loss at step 222 : 1196.1873779296875\n",
      "Training Loss at step 223 : 1172.3544921875\n",
      "Training Loss at step 224 : 1189.8155517578125\n",
      "Training Loss at step 225 : 1177.389404296875\n",
      "Training Loss at step 226 : 1177.679931640625\n",
      "Training Loss at step 227 : 1188.7557373046875\n",
      "Training Loss at step 228 : 1183.61279296875\n",
      "Training Loss at step 229 : 1182.7293701171875\n",
      "Training Loss at step 230 : 1202.0780029296875\n",
      "Test Loss 1202.3770751953125 \n",
      "\n",
      "Training Loss at step 231 : 1202.3948974609375\n",
      "Training Loss at step 232 : 1221.47900390625\n",
      "Training Loss at step 233 : 1189.0653076171875\n",
      "Training Loss at step 234 : 1155.823486328125\n",
      "Training Loss at step 235 : 1193.4881591796875\n",
      "Training Loss at step 236 : 1185.756591796875\n",
      "Training Loss at step 237 : 1157.4931640625\n",
      "Training Loss at step 238 : 1173.8179931640625\n",
      "Training Loss at step 239 : 1150.61865234375\n",
      "Training Loss at step 240 : 1142.7423095703125\n",
      "Test Loss 1199.565185546875 \n",
      "\n",
      "Training Loss at step 241 : 1124.297607421875\n",
      "Training Loss at step 242 : 1179.9569091796875\n",
      "Training Loss at step 243 : 1189.060791015625\n",
      "Training Loss at step 244 : 1153.057373046875\n",
      "Training Loss at step 245 : 1159.73974609375\n",
      "Training Loss at step 246 : 1215.0364990234375\n",
      "Training Loss at step 247 : 1172.4739990234375\n",
      "Training Loss at step 248 : 1166.296630859375\n",
      "Training Loss at step 249 : 1175.4515380859375\n",
      "Training Loss at step 250 : 1187.7874755859375\n",
      "Test Loss 1196.8936767578125 \n",
      "\n",
      "Training Loss at step 251 : 1178.2862548828125\n",
      "Training Loss at step 252 : 1173.03271484375\n",
      "Training Loss at step 253 : 1167.7237548828125\n",
      "Training Loss at step 254 : 1152.408447265625\n",
      "Training Loss at step 255 : 1201.505615234375\n",
      "Training Loss at step 256 : 1181.1839599609375\n",
      "Training Loss at step 257 : 1175.520751953125\n",
      "Training Loss at step 258 : 1149.126708984375\n",
      "Training Loss at step 259 : 1196.103271484375\n",
      "Training Loss at step 260 : 1156.763427734375\n",
      "Test Loss 1194.174072265625 \n",
      "\n",
      "Training Loss at step 261 : 1180.745361328125\n",
      "Training Loss at step 262 : 1163.0013427734375\n",
      "Training Loss at step 263 : 1185.384765625\n",
      "Training Loss at step 264 : 1166.0872802734375\n",
      "Training Loss at step 265 : 1149.8140869140625\n",
      "Training Loss at step 266 : 1153.724609375\n",
      "Training Loss at step 267 : 1181.9429931640625\n",
      "Training Loss at step 268 : 1160.3594970703125\n",
      "Training Loss at step 269 : 1159.79638671875\n",
      "Training Loss at step 270 : 1182.2637939453125\n",
      "Test Loss 1191.4775390625 \n",
      "\n",
      "Training Loss at step 271 : 1185.5836181640625\n",
      "Training Loss at step 272 : 1152.4736328125\n",
      "Training Loss at step 273 : 1168.007568359375\n",
      "Training Loss at step 274 : 1142.1748046875\n",
      "Training Loss at step 275 : 1158.6044921875\n",
      "Training Loss at step 276 : 1182.687744140625\n",
      "Training Loss at step 277 : 1177.9537353515625\n",
      "Training Loss at step 278 : 1149.365966796875\n",
      "Training Loss at step 279 : 1140.66650390625\n",
      "Training Loss at step 280 : 1144.437744140625\n",
      "Test Loss 1188.86474609375 \n",
      "\n",
      "Starting training epoch 3\n",
      "Training Loss at step 0 : 1172.8607177734375\n",
      "Test Loss 1188.6109619140625 \n",
      "\n",
      "Training Loss at step 1 : 1195.4554443359375\n",
      "Training Loss at step 2 : 1160.062744140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 3 : 1174.1744384765625\n",
      "Training Loss at step 4 : 1162.0469970703125\n",
      "Training Loss at step 5 : 1172.4268798828125\n",
      "Training Loss at step 6 : 1138.0657958984375\n",
      "Training Loss at step 7 : 1166.4063720703125\n",
      "Training Loss at step 8 : 1176.197265625\n",
      "Training Loss at step 9 : 1176.4366455078125\n",
      "Training Loss at step 10 : 1151.728271484375\n",
      "Test Loss 1185.977783203125 \n",
      "\n",
      "Training Loss at step 11 : 1180.3331298828125\n",
      "Training Loss at step 12 : 1160.28759765625\n",
      "Training Loss at step 13 : 1160.55419921875\n",
      "Training Loss at step 14 : 1169.2099609375\n",
      "Training Loss at step 15 : 1124.197021484375\n",
      "Training Loss at step 16 : 1171.75439453125\n",
      "Training Loss at step 17 : 1163.45703125\n",
      "Training Loss at step 18 : 1153.32666015625\n",
      "Training Loss at step 19 : 1152.4873046875\n",
      "Training Loss at step 20 : 1202.395263671875\n",
      "Test Loss 1183.2950439453125 \n",
      "\n",
      "Training Loss at step 21 : 1154.4072265625\n",
      "Training Loss at step 22 : 1144.1990966796875\n",
      "Training Loss at step 23 : 1140.597412109375\n",
      "Training Loss at step 24 : 1148.0548095703125\n",
      "Training Loss at step 25 : 1196.3603515625\n",
      "Training Loss at step 26 : 1143.8011474609375\n",
      "Training Loss at step 27 : 1168.4342041015625\n",
      "Training Loss at step 28 : 1147.9903564453125\n",
      "Training Loss at step 29 : 1187.764404296875\n",
      "Training Loss at step 30 : 1179.117431640625\n",
      "Test Loss 1180.6785888671875 \n",
      "\n",
      "Training Loss at step 31 : 1145.901123046875\n",
      "Training Loss at step 32 : 1171.2613525390625\n",
      "Training Loss at step 33 : 1170.0997314453125\n",
      "Training Loss at step 34 : 1166.49560546875\n",
      "Training Loss at step 35 : 1173.8662109375\n",
      "Training Loss at step 36 : 1155.233642578125\n",
      "Training Loss at step 37 : 1142.6248779296875\n",
      "Training Loss at step 38 : 1155.8309326171875\n",
      "Training Loss at step 39 : 1133.8157958984375\n",
      "Training Loss at step 40 : 1169.1044921875\n",
      "Test Loss 1178.044677734375 \n",
      "\n",
      "Training Loss at step 41 : 1193.95947265625\n",
      "Training Loss at step 42 : 1127.1314697265625\n",
      "Training Loss at step 43 : 1147.0743408203125\n",
      "Training Loss at step 44 : 1199.923828125\n",
      "Training Loss at step 45 : 1166.64453125\n",
      "Training Loss at step 46 : 1133.7611083984375\n",
      "Training Loss at step 47 : 1152.35205078125\n",
      "Training Loss at step 48 : 1124.0943603515625\n",
      "Training Loss at step 49 : 1153.353515625\n",
      "Training Loss at step 50 : 1131.334716796875\n",
      "Test Loss 1175.8428955078125 \n",
      "\n",
      "Training Loss at step 51 : 1145.1510009765625\n",
      "Training Loss at step 52 : 1148.5693359375\n",
      "Training Loss at step 53 : 1150.0238037109375\n",
      "Training Loss at step 54 : 1126.86474609375\n",
      "Training Loss at step 55 : 1162.6107177734375\n",
      "Training Loss at step 56 : 1187.577392578125\n",
      "Training Loss at step 57 : 1138.722412109375\n",
      "Training Loss at step 58 : 1161.260009765625\n",
      "Training Loss at step 59 : 1170.2109375\n",
      "Training Loss at step 60 : 1146.0751953125\n",
      "Test Loss 1173.4349365234375 \n",
      "\n",
      "Training Loss at step 61 : 1186.5885009765625\n",
      "Training Loss at step 62 : 1132.009521484375\n",
      "Training Loss at step 63 : 1147.904296875\n",
      "Training Loss at step 64 : 1149.12451171875\n",
      "Training Loss at step 65 : 1167.8699951171875\n",
      "Training Loss at step 66 : 1146.6297607421875\n",
      "Training Loss at step 67 : 1125.499755859375\n",
      "Training Loss at step 68 : 1149.70361328125\n",
      "Training Loss at step 69 : 1154.6124267578125\n",
      "Training Loss at step 70 : 1146.550048828125\n",
      "Test Loss 1171.0162353515625 \n",
      "\n",
      "Training Loss at step 71 : 1158.908935546875\n",
      "Training Loss at step 72 : 1175.30712890625\n",
      "Training Loss at step 73 : 1127.0528564453125\n",
      "Training Loss at step 74 : 1135.9476318359375\n",
      "Training Loss at step 75 : 1139.2816162109375\n",
      "Training Loss at step 76 : 1149.81005859375\n",
      "Training Loss at step 77 : 1157.459716796875\n",
      "Training Loss at step 78 : 1144.5555419921875\n",
      "Training Loss at step 79 : 1180.122314453125\n",
      "Training Loss at step 80 : 1174.804443359375\n",
      "Test Loss 1168.7347412109375 \n",
      "\n",
      "Training Loss at step 81 : 1171.102294921875\n",
      "Training Loss at step 82 : 1142.4599609375\n",
      "Training Loss at step 83 : 1153.2666015625\n",
      "Training Loss at step 84 : 1142.7431640625\n",
      "Training Loss at step 85 : 1128.47412109375\n",
      "Training Loss at step 86 : 1144.9075927734375\n",
      "Training Loss at step 87 : 1124.7537841796875\n",
      "Training Loss at step 88 : 1153.498046875\n",
      "Training Loss at step 89 : 1128.04248046875\n",
      "Training Loss at step 90 : 1154.7037353515625\n",
      "Test Loss 1166.1419677734375 \n",
      "\n",
      "Training Loss at step 91 : 1163.365966796875\n",
      "Training Loss at step 92 : 1120.814697265625\n",
      "Training Loss at step 93 : 1117.544677734375\n",
      "Training Loss at step 94 : 1141.1837158203125\n",
      "Training Loss at step 95 : 1163.4566650390625\n",
      "Training Loss at step 96 : 1126.401611328125\n",
      "Training Loss at step 97 : 1128.484375\n",
      "Training Loss at step 98 : 1154.4183349609375\n",
      "Training Loss at step 99 : 1120.1641845703125\n",
      "Training Loss at step 100 : 1146.926025390625\n",
      "Test Loss 1163.88134765625 \n",
      "\n",
      "Training Loss at step 101 : 1130.0924072265625\n",
      "Training Loss at step 102 : 1130.6220703125\n",
      "Training Loss at step 103 : 1154.986083984375\n",
      "Training Loss at step 104 : 1153.099365234375\n",
      "Training Loss at step 105 : 1145.8704833984375\n",
      "Training Loss at step 106 : 1138.724609375\n",
      "Training Loss at step 107 : 1139.737548828125\n",
      "Training Loss at step 108 : 1148.5992431640625\n",
      "Training Loss at step 109 : 1129.403076171875\n",
      "Training Loss at step 110 : 1122.0240478515625\n",
      "Test Loss 1161.853759765625 \n",
      "\n",
      "Training Loss at step 111 : 1121.04248046875\n",
      "Training Loss at step 112 : 1109.132080078125\n",
      "Training Loss at step 113 : 1154.997802734375\n",
      "Training Loss at step 114 : 1125.6212158203125\n",
      "Training Loss at step 115 : 1144.40185546875\n",
      "Training Loss at step 116 : 1156.158447265625\n",
      "Training Loss at step 117 : 1143.2427978515625\n",
      "Training Loss at step 118 : 1157.349853515625\n",
      "Training Loss at step 119 : 1123.64208984375\n",
      "Training Loss at step 120 : 1116.7135009765625\n",
      "Test Loss 1159.6142578125 \n",
      "\n",
      "Training Loss at step 121 : 1158.2669677734375\n",
      "Training Loss at step 122 : 1138.1077880859375\n",
      "Training Loss at step 123 : 1116.020263671875\n",
      "Training Loss at step 124 : 1141.565673828125\n",
      "Training Loss at step 125 : 1127.793212890625\n",
      "Training Loss at step 126 : 1127.830810546875\n",
      "Training Loss at step 127 : 1150.9581298828125\n",
      "Training Loss at step 128 : 1132.485595703125\n",
      "Training Loss at step 129 : 1146.40087890625\n",
      "Training Loss at step 130 : 1136.203369140625\n",
      "Test Loss 1157.658935546875 \n",
      "\n",
      "Training Loss at step 131 : 1162.40576171875\n",
      "Training Loss at step 132 : 1122.2535400390625\n",
      "Training Loss at step 133 : 1104.359619140625\n",
      "Training Loss at step 134 : 1163.8529052734375\n",
      "Training Loss at step 135 : 1141.0205078125\n",
      "Training Loss at step 136 : 1165.6612548828125\n",
      "Training Loss at step 137 : 1151.73193359375\n",
      "Training Loss at step 138 : 1119.1781005859375\n",
      "Training Loss at step 139 : 1079.2603759765625\n",
      "Training Loss at step 140 : 1135.1680908203125\n",
      "Test Loss 1155.55419921875 \n",
      "\n",
      "Training Loss at step 141 : 1139.123046875\n",
      "Training Loss at step 142 : 1109.2919921875\n",
      "Training Loss at step 143 : 1148.4913330078125\n",
      "Training Loss at step 144 : 1121.7269287109375\n",
      "Training Loss at step 145 : 1149.55126953125\n",
      "Training Loss at step 146 : 1147.5164794921875\n",
      "Training Loss at step 147 : 1109.556640625\n",
      "Training Loss at step 148 : 1146.2578125\n",
      "Training Loss at step 149 : 1144.126220703125\n",
      "Training Loss at step 150 : 1128.4114990234375\n",
      "Test Loss 1153.693115234375 \n",
      "\n",
      "Training Loss at step 151 : 1130.128662109375\n",
      "Training Loss at step 152 : 1124.504150390625\n",
      "Training Loss at step 153 : 1123.04296875\n",
      "Training Loss at step 154 : 1126.918701171875\n",
      "Training Loss at step 155 : 1179.2615966796875\n",
      "Training Loss at step 156 : 1160.8685302734375\n",
      "Training Loss at step 157 : 1103.4024658203125\n",
      "Training Loss at step 158 : 1141.75439453125\n",
      "Training Loss at step 159 : 1130.6707763671875\n",
      "Training Loss at step 160 : 1139.83837890625\n",
      "Test Loss 1151.6962890625 \n",
      "\n",
      "Training Loss at step 161 : 1135.6514892578125\n",
      "Training Loss at step 162 : 1142.287109375\n",
      "Training Loss at step 163 : 1113.6719970703125\n",
      "Training Loss at step 164 : 1131.5762939453125\n",
      "Training Loss at step 165 : 1179.9228515625\n",
      "Training Loss at step 166 : 1160.5799560546875\n",
      "Training Loss at step 167 : 1123.9234619140625\n",
      "Training Loss at step 168 : 1157.609375\n",
      "Training Loss at step 169 : 1132.3531494140625\n",
      "Training Loss at step 170 : 1160.0855712890625\n",
      "Test Loss 1149.9522705078125 \n",
      "\n",
      "Training Loss at step 171 : 1134.9783935546875\n",
      "Training Loss at step 172 : 1148.7772216796875\n",
      "Training Loss at step 173 : 1148.017578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 174 : 1159.4443359375\n",
      "Training Loss at step 175 : 1132.3023681640625\n",
      "Training Loss at step 176 : 1110.9798583984375\n",
      "Training Loss at step 177 : 1110.9029541015625\n",
      "Training Loss at step 178 : 1119.7078857421875\n",
      "Training Loss at step 179 : 1131.437255859375\n",
      "Training Loss at step 180 : 1121.8316650390625\n",
      "Test Loss 1147.9083251953125 \n",
      "\n",
      "Training Loss at step 181 : 1151.67236328125\n",
      "Training Loss at step 182 : 1082.86865234375\n",
      "Training Loss at step 183 : 1106.83154296875\n",
      "Training Loss at step 184 : 1145.2127685546875\n",
      "Training Loss at step 185 : 1115.81787109375\n",
      "Training Loss at step 186 : 1127.125732421875\n",
      "Training Loss at step 187 : 1140.0703125\n",
      "Training Loss at step 188 : 1123.3162841796875\n",
      "Training Loss at step 189 : 1130.07666015625\n",
      "Training Loss at step 190 : 1113.6214599609375\n",
      "Test Loss 1146.045654296875 \n",
      "\n",
      "Training Loss at step 191 : 1127.7913818359375\n",
      "Training Loss at step 192 : 1112.5863037109375\n",
      "Training Loss at step 193 : 1132.5804443359375\n",
      "Training Loss at step 194 : 1120.4388427734375\n",
      "Training Loss at step 195 : 1133.154296875\n",
      "Training Loss at step 196 : 1120.3787841796875\n",
      "Training Loss at step 197 : 1113.92431640625\n",
      "Training Loss at step 198 : 1119.5364990234375\n",
      "Training Loss at step 199 : 1081.4970703125\n",
      "Training Loss at step 200 : 1115.0863037109375\n",
      "Test Loss 1144.373779296875 \n",
      "\n",
      "Training Loss at step 201 : 1118.8594970703125\n",
      "Training Loss at step 202 : 1130.6920166015625\n",
      "Training Loss at step 203 : 1126.8377685546875\n",
      "Training Loss at step 204 : 1133.8447265625\n",
      "Training Loss at step 205 : 1082.9818115234375\n",
      "Training Loss at step 206 : 1087.758056640625\n",
      "Training Loss at step 207 : 1103.8565673828125\n",
      "Training Loss at step 208 : 1147.1363525390625\n",
      "Training Loss at step 209 : 1135.4815673828125\n",
      "Training Loss at step 210 : 1113.37548828125\n",
      "Test Loss 1142.571044921875 \n",
      "\n",
      "Training Loss at step 211 : 1142.3939208984375\n",
      "Training Loss at step 212 : 1153.307861328125\n",
      "Training Loss at step 213 : 1176.5955810546875\n",
      "Training Loss at step 214 : 1125.0072021484375\n",
      "Training Loss at step 215 : 1121.931640625\n",
      "Training Loss at step 216 : 1111.1904296875\n",
      "Training Loss at step 217 : 1122.82861328125\n",
      "Training Loss at step 218 : 1095.6876220703125\n",
      "Training Loss at step 219 : 1145.64794921875\n",
      "Training Loss at step 220 : 1145.78076171875\n",
      "Test Loss 1140.6993408203125 \n",
      "\n",
      "Training Loss at step 221 : 1118.8389892578125\n",
      "Training Loss at step 222 : 1092.304931640625\n",
      "Training Loss at step 223 : 1102.62890625\n",
      "Training Loss at step 224 : 1105.3751220703125\n",
      "Training Loss at step 225 : 1156.495849609375\n",
      "Training Loss at step 226 : 1122.253662109375\n",
      "Training Loss at step 227 : 1133.7047119140625\n",
      "Training Loss at step 228 : 1093.65478515625\n",
      "Training Loss at step 229 : 1115.3311767578125\n",
      "Training Loss at step 230 : 1104.37353515625\n",
      "Test Loss 1139.081298828125 \n",
      "\n",
      "Training Loss at step 231 : 1120.5538330078125\n",
      "Training Loss at step 232 : 1116.233154296875\n",
      "Training Loss at step 233 : 1118.3284912109375\n",
      "Training Loss at step 234 : 1109.3641357421875\n",
      "Training Loss at step 235 : 1141.8934326171875\n",
      "Training Loss at step 236 : 1122.3135986328125\n",
      "Training Loss at step 237 : 1144.9251708984375\n",
      "Training Loss at step 238 : 1124.739013671875\n",
      "Training Loss at step 239 : 1096.2427978515625\n",
      "Training Loss at step 240 : 1119.574951171875\n",
      "Test Loss 1137.3941650390625 \n",
      "\n",
      "Training Loss at step 241 : 1138.480224609375\n",
      "Training Loss at step 242 : 1139.6373291015625\n",
      "Training Loss at step 243 : 1132.1383056640625\n",
      "Training Loss at step 244 : 1108.8564453125\n",
      "Training Loss at step 245 : 1132.328125\n",
      "Training Loss at step 246 : 1133.703369140625\n",
      "Training Loss at step 247 : 1102.213623046875\n",
      "Training Loss at step 248 : 1137.1678466796875\n",
      "Training Loss at step 249 : 1115.0103759765625\n",
      "Training Loss at step 250 : 1112.966796875\n",
      "Test Loss 1135.8583984375 \n",
      "\n",
      "Training Loss at step 251 : 1085.651611328125\n",
      "Training Loss at step 252 : 1096.0755615234375\n",
      "Training Loss at step 253 : 1130.435546875\n",
      "Training Loss at step 254 : 1144.12109375\n",
      "Training Loss at step 255 : 1109.7662353515625\n",
      "Training Loss at step 256 : 1152.93115234375\n",
      "Training Loss at step 257 : 1119.391357421875\n",
      "Training Loss at step 258 : 1126.502685546875\n",
      "Training Loss at step 259 : 1105.6737060546875\n",
      "Training Loss at step 260 : 1117.74072265625\n",
      "Test Loss 1134.280029296875 \n",
      "\n",
      "Training Loss at step 261 : 1124.2779541015625\n",
      "Training Loss at step 262 : 1104.350830078125\n",
      "Training Loss at step 263 : 1105.0323486328125\n",
      "Training Loss at step 264 : 1105.396484375\n",
      "Training Loss at step 265 : 1099.802001953125\n",
      "Training Loss at step 266 : 1109.3533935546875\n",
      "Training Loss at step 267 : 1096.29443359375\n",
      "Training Loss at step 268 : 1110.46142578125\n",
      "Training Loss at step 269 : 1108.1044921875\n",
      "Training Loss at step 270 : 1116.5762939453125\n",
      "Test Loss 1132.7481689453125 \n",
      "\n",
      "Training Loss at step 271 : 1118.79443359375\n",
      "Training Loss at step 272 : 1113.962890625\n",
      "Training Loss at step 273 : 1130.3668212890625\n",
      "Training Loss at step 274 : 1130.1466064453125\n",
      "Training Loss at step 275 : 1099.456787109375\n",
      "Training Loss at step 276 : 1113.161376953125\n",
      "Training Loss at step 277 : 1119.7257080078125\n",
      "Training Loss at step 278 : 1115.76123046875\n",
      "Training Loss at step 279 : 1104.37158203125\n",
      "Training Loss at step 280 : 1131.7589111328125\n",
      "Test Loss 1131.1033935546875 \n",
      "\n",
      "Starting training epoch 4\n",
      "Training Loss at step 0 : 1130.55517578125\n",
      "Test Loss 1130.9552001953125 \n",
      "\n",
      "Training Loss at step 1 : 1099.960205078125\n",
      "Training Loss at step 2 : 1123.595703125\n",
      "Training Loss at step 3 : 1116.29248046875\n",
      "Training Loss at step 4 : 1110.109375\n",
      "Training Loss at step 5 : 1097.42822265625\n",
      "Training Loss at step 6 : 1091.489501953125\n",
      "Training Loss at step 7 : 1145.201904296875\n",
      "Training Loss at step 8 : 1157.3692626953125\n",
      "Training Loss at step 9 : 1109.172119140625\n",
      "Training Loss at step 10 : 1136.38818359375\n",
      "Test Loss 1129.4384765625 \n",
      "\n",
      "Training Loss at step 11 : 1126.9661865234375\n",
      "Training Loss at step 12 : 1121.763916015625\n",
      "Training Loss at step 13 : 1128.0791015625\n",
      "Training Loss at step 14 : 1096.4156494140625\n",
      "Training Loss at step 15 : 1126.87353515625\n",
      "Training Loss at step 16 : 1088.7308349609375\n",
      "Training Loss at step 17 : 1143.9573974609375\n",
      "Training Loss at step 18 : 1083.015625\n",
      "Training Loss at step 19 : 1083.98486328125\n",
      "Training Loss at step 20 : 1096.77978515625\n",
      "Test Loss 1127.85986328125 \n",
      "\n",
      "Training Loss at step 21 : 1130.3173828125\n",
      "Training Loss at step 22 : 1096.386474609375\n",
      "Training Loss at step 23 : 1082.94287109375\n",
      "Training Loss at step 24 : 1077.1590576171875\n",
      "Training Loss at step 25 : 1124.8134765625\n",
      "Training Loss at step 26 : 1133.9632568359375\n",
      "Training Loss at step 27 : 1125.1488037109375\n",
      "Training Loss at step 28 : 1106.7996826171875\n",
      "Training Loss at step 29 : 1111.395263671875\n",
      "Training Loss at step 30 : 1102.03466796875\n",
      "Test Loss 1126.32568359375 \n",
      "\n",
      "Training Loss at step 31 : 1093.986572265625\n",
      "Training Loss at step 32 : 1136.75830078125\n",
      "Training Loss at step 33 : 1101.3065185546875\n",
      "Training Loss at step 34 : 1141.3297119140625\n",
      "Training Loss at step 35 : 1142.579345703125\n",
      "Training Loss at step 36 : 1067.697265625\n",
      "Training Loss at step 37 : 1060.9027099609375\n",
      "Training Loss at step 38 : 1110.9852294921875\n",
      "Training Loss at step 39 : 1094.381103515625\n",
      "Training Loss at step 40 : 1078.299560546875\n",
      "Test Loss 1124.878662109375 \n",
      "\n",
      "Training Loss at step 41 : 1085.9647216796875\n",
      "Training Loss at step 42 : 1110.216552734375\n",
      "Training Loss at step 43 : 1099.805419921875\n",
      "Training Loss at step 44 : 1105.0670166015625\n",
      "Training Loss at step 45 : 1102.018310546875\n",
      "Training Loss at step 46 : 1092.3546142578125\n",
      "Training Loss at step 47 : 1073.0189208984375\n",
      "Training Loss at step 48 : 1100.2764892578125\n",
      "Training Loss at step 49 : 1094.547607421875\n",
      "Training Loss at step 50 : 1134.945556640625\n",
      "Test Loss 1123.3486328125 \n",
      "\n",
      "Training Loss at step 51 : 1107.579833984375\n",
      "Training Loss at step 52 : 1124.2916259765625\n",
      "Training Loss at step 53 : 1118.0955810546875\n",
      "Training Loss at step 54 : 1093.43408203125\n",
      "Training Loss at step 55 : 1102.1175537109375\n",
      "Training Loss at step 56 : 1078.594482421875\n",
      "Training Loss at step 57 : 1118.05126953125\n",
      "Training Loss at step 58 : 1109.766845703125\n",
      "Training Loss at step 59 : 1107.830810546875\n",
      "Training Loss at step 60 : 1123.2974853515625\n",
      "Test Loss 1121.87548828125 \n",
      "\n",
      "Training Loss at step 61 : 1101.4683837890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 62 : 1075.0015869140625\n",
      "Training Loss at step 63 : 1126.9097900390625\n",
      "Training Loss at step 64 : 1081.0462646484375\n",
      "Training Loss at step 65 : 1067.6912841796875\n",
      "Training Loss at step 66 : 1114.36572265625\n",
      "Training Loss at step 67 : 1097.0784912109375\n",
      "Training Loss at step 68 : 1114.0009765625\n",
      "Training Loss at step 69 : 1118.1849365234375\n",
      "Training Loss at step 70 : 1110.9488525390625\n",
      "Test Loss 1120.8388671875 \n",
      "\n",
      "Training Loss at step 71 : 1114.8939208984375\n",
      "Training Loss at step 72 : 1089.768798828125\n",
      "Training Loss at step 73 : 1074.716796875\n",
      "Training Loss at step 74 : 1104.423828125\n",
      "Training Loss at step 75 : 1110.311279296875\n",
      "Training Loss at step 76 : 1062.760498046875\n",
      "Training Loss at step 77 : 1100.7301025390625\n",
      "Training Loss at step 78 : 1095.9246826171875\n",
      "Training Loss at step 79 : 1092.065673828125\n",
      "Training Loss at step 80 : 1090.5928955078125\n",
      "Test Loss 1119.559814453125 \n",
      "\n",
      "Training Loss at step 81 : 1070.8499755859375\n",
      "Training Loss at step 82 : 1094.9923095703125\n",
      "Training Loss at step 83 : 1107.0457763671875\n",
      "Training Loss at step 84 : 1112.0194091796875\n",
      "Training Loss at step 85 : 1113.3551025390625\n",
      "Training Loss at step 86 : 1083.25634765625\n",
      "Training Loss at step 87 : 1069.8175048828125\n",
      "Training Loss at step 88 : 1108.4010009765625\n",
      "Training Loss at step 89 : 1089.019775390625\n",
      "Training Loss at step 90 : 1098.681396484375\n",
      "Test Loss 1118.5169677734375 \n",
      "\n",
      "Training Loss at step 91 : 1123.71728515625\n",
      "Training Loss at step 92 : 1117.97998046875\n",
      "Training Loss at step 93 : 1103.3560791015625\n",
      "Training Loss at step 94 : 1092.144287109375\n",
      "Training Loss at step 95 : 1116.843017578125\n",
      "Training Loss at step 96 : 1151.3189697265625\n",
      "Training Loss at step 97 : 1082.756591796875\n",
      "Training Loss at step 98 : 1111.55859375\n",
      "Training Loss at step 99 : 1134.472412109375\n",
      "Training Loss at step 100 : 1085.7803955078125\n",
      "Test Loss 1117.2403564453125 \n",
      "\n",
      "Training Loss at step 101 : 1076.464111328125\n",
      "Training Loss at step 102 : 1061.637451171875\n",
      "Training Loss at step 103 : 1106.129638671875\n",
      "Training Loss at step 104 : 1092.2742919921875\n",
      "Training Loss at step 105 : 1087.7008056640625\n",
      "Training Loss at step 106 : 1088.88623046875\n",
      "Training Loss at step 107 : 1099.8646240234375\n",
      "Training Loss at step 108 : 1120.451416015625\n",
      "Training Loss at step 109 : 1114.13330078125\n",
      "Training Loss at step 110 : 1077.64111328125\n",
      "Test Loss 1115.9208984375 \n",
      "\n",
      "Training Loss at step 111 : 1090.0364990234375\n",
      "Training Loss at step 112 : 1108.5794677734375\n",
      "Training Loss at step 113 : 1109.9703369140625\n",
      "Training Loss at step 114 : 1071.999267578125\n",
      "Training Loss at step 115 : 1102.64501953125\n",
      "Training Loss at step 116 : 1125.236572265625\n",
      "Training Loss at step 117 : 1042.7659912109375\n",
      "Training Loss at step 118 : 1103.13232421875\n",
      "Training Loss at step 119 : 1090.2177734375\n",
      "Training Loss at step 120 : 1089.4395751953125\n",
      "Test Loss 1114.513427734375 \n",
      "\n",
      "Training Loss at step 121 : 1120.52685546875\n",
      "Training Loss at step 122 : 1115.8477783203125\n",
      "Training Loss at step 123 : 1095.2958984375\n",
      "Training Loss at step 124 : 1091.831298828125\n",
      "Training Loss at step 125 : 1071.53564453125\n",
      "Training Loss at step 126 : 1066.9423828125\n",
      "Training Loss at step 127 : 1112.6591796875\n",
      "Training Loss at step 128 : 1087.72216796875\n",
      "Training Loss at step 129 : 1092.21533203125\n",
      "Training Loss at step 130 : 1130.78955078125\n",
      "Test Loss 1112.9166259765625 \n",
      "\n",
      "Training Loss at step 131 : 1091.74853515625\n",
      "Training Loss at step 132 : 1081.39794921875\n",
      "Training Loss at step 133 : 1099.367919921875\n",
      "Training Loss at step 134 : 1068.665771484375\n",
      "Training Loss at step 135 : 1090.462158203125\n",
      "Training Loss at step 136 : 1104.9915771484375\n",
      "Training Loss at step 137 : 1095.605712890625\n",
      "Training Loss at step 138 : 1108.1690673828125\n",
      "Training Loss at step 139 : 1098.7891845703125\n",
      "Training Loss at step 140 : 1089.219970703125\n",
      "Test Loss 1111.8157958984375 \n",
      "\n",
      "Training Loss at step 141 : 1085.730712890625\n",
      "Training Loss at step 142 : 1073.7177734375\n",
      "Training Loss at step 143 : 1080.744140625\n",
      "Training Loss at step 144 : 1102.180908203125\n",
      "Training Loss at step 145 : 1080.5355224609375\n",
      "Training Loss at step 146 : 1101.577392578125\n",
      "Training Loss at step 147 : 1127.4195556640625\n",
      "Training Loss at step 148 : 1078.79443359375\n",
      "Training Loss at step 149 : 1069.8477783203125\n",
      "Training Loss at step 150 : 1115.0299072265625\n",
      "Test Loss 1110.61865234375 \n",
      "\n",
      "Training Loss at step 151 : 1099.432373046875\n",
      "Training Loss at step 152 : 1104.403076171875\n",
      "Training Loss at step 153 : 1095.9666748046875\n",
      "Training Loss at step 154 : 1061.6795654296875\n",
      "Training Loss at step 155 : 1105.1929931640625\n",
      "Training Loss at step 156 : 1089.5902099609375\n",
      "Training Loss at step 157 : 1122.552978515625\n",
      "Training Loss at step 158 : 1106.71875\n",
      "Training Loss at step 159 : 1085.2879638671875\n",
      "Training Loss at step 160 : 1099.9793701171875\n",
      "Test Loss 1109.45361328125 \n",
      "\n",
      "Training Loss at step 161 : 1087.429443359375\n",
      "Training Loss at step 162 : 1091.662353515625\n",
      "Training Loss at step 163 : 1089.794677734375\n",
      "Training Loss at step 164 : 1129.213623046875\n",
      "Training Loss at step 165 : 1095.875244140625\n",
      "Training Loss at step 166 : 1084.81640625\n",
      "Training Loss at step 167 : 1109.5181884765625\n",
      "Training Loss at step 168 : 1071.72119140625\n",
      "Training Loss at step 169 : 1084.6561279296875\n",
      "Training Loss at step 170 : 1096.62841796875\n",
      "Test Loss 1108.1363525390625 \n",
      "\n",
      "Training Loss at step 171 : 1077.4560546875\n",
      "Training Loss at step 172 : 1111.4580078125\n",
      "Training Loss at step 173 : 1090.26416015625\n",
      "Training Loss at step 174 : 1097.6673583984375\n",
      "Training Loss at step 175 : 1110.7108154296875\n",
      "Training Loss at step 176 : 1089.878173828125\n",
      "Training Loss at step 177 : 1087.933837890625\n",
      "Training Loss at step 178 : 1066.08642578125\n",
      "Training Loss at step 179 : 1064.291259765625\n",
      "Training Loss at step 180 : 1092.0\n",
      "Test Loss 1106.9794921875 \n",
      "\n",
      "Training Loss at step 181 : 1086.74853515625\n",
      "Training Loss at step 182 : 1086.4732666015625\n",
      "Training Loss at step 183 : 1086.7054443359375\n",
      "Training Loss at step 184 : 1126.2508544921875\n",
      "Training Loss at step 185 : 1088.38134765625\n",
      "Training Loss at step 186 : 1089.736328125\n",
      "Training Loss at step 187 : 1121.71630859375\n",
      "Training Loss at step 188 : 1090.431884765625\n",
      "Training Loss at step 189 : 1063.457763671875\n",
      "Training Loss at step 190 : 1071.5289306640625\n",
      "Test Loss 1105.897216796875 \n",
      "\n",
      "Training Loss at step 191 : 1058.3338623046875\n",
      "Training Loss at step 192 : 1095.7506103515625\n",
      "Training Loss at step 193 : 1085.499267578125\n",
      "Training Loss at step 194 : 1053.45703125\n",
      "Training Loss at step 195 : 1097.044677734375\n",
      "Training Loss at step 196 : 1112.3765869140625\n",
      "Training Loss at step 197 : 1090.0416259765625\n",
      "Training Loss at step 198 : 1098.0767822265625\n",
      "Training Loss at step 199 : 1121.71630859375\n",
      "Training Loss at step 200 : 1094.216064453125\n",
      "Test Loss 1104.883056640625 \n",
      "\n",
      "Training Loss at step 201 : 1091.84765625\n",
      "Training Loss at step 202 : 1070.38427734375\n",
      "Training Loss at step 203 : 1088.9853515625\n",
      "Training Loss at step 204 : 1064.1217041015625\n",
      "Training Loss at step 205 : 1114.5364990234375\n",
      "Training Loss at step 206 : 1034.087890625\n",
      "Training Loss at step 207 : 1085.5994873046875\n",
      "Training Loss at step 208 : 1113.2445068359375\n",
      "Training Loss at step 209 : 1099.796630859375\n",
      "Training Loss at step 210 : 1114.2369384765625\n",
      "Test Loss 1103.6234130859375 \n",
      "\n",
      "Training Loss at step 211 : 1113.0028076171875\n",
      "Training Loss at step 212 : 1083.6290283203125\n",
      "Training Loss at step 213 : 1055.9881591796875\n",
      "Training Loss at step 214 : 1084.0849609375\n",
      "Training Loss at step 215 : 1069.142822265625\n",
      "Training Loss at step 216 : 1097.3597412109375\n",
      "Training Loss at step 217 : 1080.4686279296875\n",
      "Training Loss at step 218 : 1068.4119873046875\n",
      "Training Loss at step 219 : 1060.8062744140625\n",
      "Training Loss at step 220 : 1071.110595703125\n",
      "Test Loss 1102.7379150390625 \n",
      "\n",
      "Training Loss at step 221 : 1080.2713623046875\n",
      "Training Loss at step 222 : 1089.91796875\n",
      "Training Loss at step 223 : 1063.834228515625\n",
      "Training Loss at step 224 : 1095.8309326171875\n",
      "Training Loss at step 225 : 1113.0986328125\n",
      "Training Loss at step 226 : 1109.4085693359375\n",
      "Training Loss at step 227 : 1097.117431640625\n",
      "Training Loss at step 228 : 1091.567138671875\n",
      "Training Loss at step 229 : 1125.148193359375\n",
      "Training Loss at step 230 : 1073.02685546875\n",
      "Test Loss 1101.8018798828125 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 231 : 1075.86279296875\n",
      "Training Loss at step 232 : 1083.8201904296875\n",
      "Training Loss at step 233 : 1118.0755615234375\n",
      "Training Loss at step 234 : 1093.5516357421875\n",
      "Training Loss at step 235 : 1095.54541015625\n",
      "Training Loss at step 236 : 1063.7518310546875\n",
      "Training Loss at step 237 : 1078.0076904296875\n",
      "Training Loss at step 238 : 1076.435546875\n",
      "Training Loss at step 239 : 1052.4671630859375\n",
      "Training Loss at step 240 : 1076.060791015625\n",
      "Test Loss 1100.9549560546875 \n",
      "\n",
      "Training Loss at step 241 : 1091.9854736328125\n",
      "Training Loss at step 242 : 1075.293212890625\n",
      "Training Loss at step 243 : 1056.3487548828125\n",
      "Training Loss at step 244 : 1106.1517333984375\n",
      "Training Loss at step 245 : 1072.837890625\n",
      "Training Loss at step 246 : 1093.38232421875\n",
      "Training Loss at step 247 : 1058.9580078125\n",
      "Training Loss at step 248 : 1056.9599609375\n",
      "Training Loss at step 249 : 1091.85205078125\n",
      "Training Loss at step 250 : 1075.8927001953125\n",
      "Test Loss 1099.9345703125 \n",
      "\n",
      "Training Loss at step 251 : 1074.8543701171875\n",
      "Training Loss at step 252 : 1054.4599609375\n",
      "Training Loss at step 253 : 1073.84912109375\n",
      "Training Loss at step 254 : 1063.04931640625\n",
      "Training Loss at step 255 : 1083.0283203125\n",
      "Training Loss at step 256 : 1070.862548828125\n",
      "Training Loss at step 257 : 1079.4146728515625\n",
      "Training Loss at step 258 : 1122.2265625\n",
      "Training Loss at step 259 : 1087.54833984375\n",
      "Training Loss at step 260 : 1075.5955810546875\n",
      "Test Loss 1099.03466796875 \n",
      "\n",
      "Training Loss at step 261 : 1063.6553955078125\n",
      "Training Loss at step 262 : 1058.54833984375\n",
      "Training Loss at step 263 : 1105.020263671875\n",
      "Training Loss at step 264 : 1043.436767578125\n",
      "Training Loss at step 265 : 1062.974853515625\n",
      "Training Loss at step 266 : 1088.43115234375\n",
      "Training Loss at step 267 : 1077.49951171875\n",
      "Training Loss at step 268 : 1087.7879638671875\n",
      "Training Loss at step 269 : 1099.0303955078125\n",
      "Training Loss at step 270 : 1084.33154296875\n",
      "Test Loss 1098.0584716796875 \n",
      "\n",
      "Training Loss at step 271 : 1075.3477783203125\n",
      "Training Loss at step 272 : 1057.10205078125\n",
      "Training Loss at step 273 : 1050.8912353515625\n",
      "Training Loss at step 274 : 1065.2147216796875\n",
      "Training Loss at step 275 : 1076.8466796875\n",
      "Training Loss at step 276 : 1073.766845703125\n",
      "Training Loss at step 277 : 1078.9840087890625\n",
      "Training Loss at step 278 : 1070.7371826171875\n",
      "Training Loss at step 279 : 1059.317626953125\n",
      "Training Loss at step 280 : 1089.851318359375\n",
      "Test Loss 1097.2652587890625 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i,key in enumerate(all_spans):\n",
    "    for span in all_spans[key]:\n",
    "        data.append((span, i))\n",
    "\n",
    "data = [ (d[0],[1 if i == d[1] else 0 for i in range(len(all_spans.keys()))]) for d in data]\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 5e-4\n",
    "\n",
    "model = SentenceClassifier(list(all_spans.keys()))\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "test_x = [d[0] for d in data[:2000]]\n",
    "test_y = torch.tensor([d[1] for d in data[:2000]], dtype=torch.float)\n",
    "\n",
    "train_data = data[2000:]\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    print(\"Starting training epoch\", epoch)\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    train_x = [d[0] for d in train_data]\n",
    "    train_y = [d[1] for d in train_data]\n",
    "\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    for i in range(len(train_data)//2000):\n",
    "        batch_x = train_x[i*2000:(i+1)*2000]\n",
    "        batch_y = torch.tensor(train_y[i*2000:(i+1)*2000], dtype=torch.float)\n",
    "        x_batches.append(batch_x)\n",
    "        y_batches.append(batch_y)\n",
    "\n",
    "    for step in range(len(x_batches)):\n",
    "\n",
    "        y_train_pred = model(x_batches[step])\n",
    "        train_loss = loss_fn(y_train_pred, y_batches[step])\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        print(\"Training Loss at step\",step,\":\",train_loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            y_test_pred = model(test_x)\n",
    "            test_loss = loss_fn(y_test_pred, test_y)\n",
    "            test_losses.append(test_loss)\n",
    "            print(\"Test Loss\",test_loss.item(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df632b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train_losses[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee27868b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_domain = list(range(len(train_losses)))\n",
    "test_domain = ([i*10 for i in range(len(test_losses))])\n",
    "\n",
    "plt.plot(train_domain, train_losses, label=\"train\")\n",
    "plt.plot(test_domain, test_losses, label=\"test\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8432ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c462155",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./distilberta-mfc-no-context.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d4ddb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reloaded = SentenceClassifier(list(all_spans.keys()))\n",
    "model_reloaded.load_state_dict(torch.load(\"./distilberta-mfc-no-context.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "432e7de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8b177a4833d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_spans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_reloaded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_str\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_str\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;34m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mtrans_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'token_type_ids'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtrans_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_type_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "test_str = \"shipping lines cannot support demand\"\n",
    "keys = list(all_spans.keys())\n",
    "print(keys[int(torch.argmax(model_reloaded([test_str])))])\n",
    "print(keys[int(torch.argmax(model([test_str])))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "616f431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "544f2c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(all_spans.keys())\n",
    "\n",
    "pred = int(torch.argmax(test))\n",
    "\n",
    "labels[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279f069c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'NN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-da6abd6b0c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mrobertaClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'NN'"
     ]
    }
   ],
   "source": [
    "from torch import NN\n",
    "\n",
    "class robertaClassifier(nn.Module):\n",
    "    def __init__():\n",
    "        pretrained = RobertaModel.from_pretrained('roberta-base')\n",
    "        padder = F.pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3eb29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
