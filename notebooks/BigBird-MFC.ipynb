{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3016 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 321 articles and 8147 spans to fit bert input size\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "all_spans = {}\n",
    "\n",
    "with open(\"../../mfc_v4.0/spans_with_context.json\", \"r\") as f:\n",
    "    all_spans = json.load(f)\n",
    "    \n",
    "articles = all_spans[\"articles\"]\n",
    "spans = all_spans[\"spans\"]\n",
    "\n",
    "a_deleted = 0\n",
    "for article in list(articles.keys()):\n",
    "    if len(tokenizer(articles[article])['input_ids']) >= 512:\n",
    "        del articles[article]\n",
    "        a_deleted += 1\n",
    "        \n",
    "s_deleted = 0\n",
    "filtered_spans = []\n",
    "for span in spans:\n",
    "    if span[-1] not in articles:\n",
    "        s_deleted += 1\n",
    "        continue\n",
    "    filtered_spans.append(span)\n",
    "    \n",
    "spans = filtered_spans\n",
    "labels = set(e[0] for e in spans)\n",
    "\n",
    "print(\"Deleted {} articles and {} spans to fit bert input size\".format(a_deleted, s_deleted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as NN\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "\n",
    "class FullContextSpanClassifier(NN.Module):\n",
    "    def __init__(self, labels, reporting=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.transformer = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        for params in self.transformer.parameters():\n",
    "            params.requires_grad = False\n",
    "        self.transformer.eval()\n",
    "        self.fc = NN.Linear(768, len(labels))\n",
    "        self.logits = NN.Softmax()\n",
    "        self.labels = labels\n",
    "        self.reporting=reporting\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tokens = x[0]\n",
    "        indices = x[1]\n",
    "        dims = list(indices.shape)\n",
    "        indices = torch.flatten(indices)\n",
    "        \n",
    "        self.report(\"Data unpacked. running bigbird...\")\n",
    "        \n",
    "        x = self.transformer(**tokens).last_hidden_state\n",
    "        \n",
    "        self.report(\"bigbird run. applying mask and summing...\")\n",
    "        \n",
    "        x = torch.reshape(x, (dims[0]*dims[1], 768))\n",
    "        self.report(\"mask shape:\", indices.shape, \"data shape:\", x.shape)\n",
    "        \n",
    "        x = (x.t()*indices).t()\n",
    "        self.report(\"after masking, data is of shape\", x.shape)\n",
    "        x = torch.reshape(x, (dims[0], dims[1], 768))\n",
    "        \n",
    "        x = torch.sum(x, dim=1)\n",
    "        self.report(\"after summing, data is of shape\", x.shape)\n",
    "        \n",
    "        x = normalize(x, dim=1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.logits(x)\n",
    "        \n",
    "        self.report(\"classifier run.\")\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def report(self,*args):\n",
    "        if self.reporting:\n",
    "            print(\"(FullContextSpanClassifier): \", \" \".join([str(x) for x in args]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_annotation_mask(offset_mapping, batch_bounds):\n",
    "    token_spans = []\n",
    "    for i, inp in enumerate(offset_mapping):\n",
    "\n",
    "        start_idx = -1\n",
    "        end_idx = -1\n",
    "\n",
    "        for j, span in enumerate(inp):\n",
    "            tok_start = span[0]\n",
    "            tok_end = span[1]\n",
    "            annotation_start = batch_bounds[i][0]\n",
    "            annotation_end = batch_bounds[i][1]\n",
    "            if tok_end > annotation_start and start_idx == -1:\n",
    "                start_idx = j\n",
    "            if tok_end > annotation_end:\n",
    "                end_idx = j\n",
    "                break\n",
    "        token_spans.append([1 if i >= start_idx and i < end_idx else 0 for i in range(len(inp))])\n",
    "    return token_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/sam/.local/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "constructing batches...\n",
      "batches constructed. starting training steps...\n",
      "Training Loss at step 0 : 14.959721565246582\n",
      "Test Loss 14.867717742919922 \n",
      "\n",
      "done with step 0 of 311\n",
      "current time: 0.011228454113006592 est epoch finish:  3.4920492291450502\n",
      "Training Loss at step 1 : 14.894336700439453\n",
      "Training Loss at step 2 : 14.858057975769043\n",
      "Training Loss at step 3 : 14.871723175048828\n",
      "Training Loss at step 4 : 14.857890129089355\n",
      "Training Loss at step 5 : 14.851085662841797\n",
      "Training Loss at step 6 : 14.791114807128906\n",
      "Training Loss at step 7 : 14.797819137573242\n",
      "Training Loss at step 8 : 14.750685691833496\n",
      "Training Loss at step 9 : 14.730365753173828\n",
      "Training Loss at step 10 : 14.79826545715332\n",
      "Test Loss 14.77617073059082 \n",
      "\n",
      "done with step 10 of 311\n",
      "current time: 0.07496475378672282 est epoch finish:  2.1194580388791633\n",
      "Training Loss at step 11 : 14.66836166381836\n",
      "Training Loss at step 12 : 14.718295097351074\n",
      "Training Loss at step 13 : 14.719673156738281\n",
      "Training Loss at step 14 : 14.782073020935059\n",
      "Training Loss at step 15 : 14.735532760620117\n",
      "Training Loss at step 16 : 14.666997909545898\n",
      "Training Loss at step 17 : 14.70473575592041\n",
      "Training Loss at step 18 : 14.721081733703613\n",
      "Training Loss at step 19 : 14.819938659667969\n",
      "Training Loss at step 20 : 14.702545166015625\n",
      "Test Loss 14.726005554199219 \n",
      "\n",
      "done with step 20 of 311\n",
      "current time: 0.13811822334925333 est epoch finish:  2.0454651172198948\n",
      "Training Loss at step 21 : 14.640329360961914\n",
      "Training Loss at step 22 : 14.526067733764648\n",
      "Training Loss at step 23 : 14.676734924316406\n",
      "Training Loss at step 24 : 14.657090187072754\n",
      "Training Loss at step 25 : 14.575481414794922\n",
      "Training Loss at step 26 : 14.6260347366333\n",
      "Training Loss at step 27 : 14.681526184082031\n",
      "Training Loss at step 28 : 14.638670921325684\n",
      "Training Loss at step 29 : 14.589092254638672\n",
      "Training Loss at step 30 : 14.504438400268555\n",
      "Test Loss 14.692578315734863 \n",
      "\n",
      "done with step 30 of 311\n",
      "current time: 0.2021547834078471 est epoch finish:  2.028068956123885\n",
      "Training Loss at step 31 : 14.519195556640625\n",
      "Training Loss at step 32 : 14.586626052856445\n",
      "Training Loss at step 33 : 14.54588508605957\n",
      "Training Loss at step 34 : 14.670275688171387\n",
      "Training Loss at step 35 : 14.469854354858398\n",
      "Training Loss at step 36 : 14.590648651123047\n",
      "Training Loss at step 37 : 14.62772274017334\n",
      "Training Loss at step 38 : 14.526590347290039\n",
      "Training Loss at step 39 : 14.486689567565918\n",
      "Training Loss at step 40 : 14.53089714050293\n",
      "Test Loss 14.650032997131348 \n",
      "\n",
      "done with step 40 of 311\n",
      "current time: 0.2645954926808675 est epoch finish:  2.0070536152134095\n",
      "Training Loss at step 41 : 14.564129829406738\n",
      "Training Loss at step 42 : 14.342215538024902\n",
      "Training Loss at step 43 : 14.426498413085938\n",
      "Training Loss at step 44 : 14.58516788482666\n",
      "Training Loss at step 45 : 14.434224128723145\n",
      "Training Loss at step 46 : 14.53390121459961\n",
      "Training Loss at step 47 : 14.579174041748047\n",
      "Training Loss at step 48 : 14.458316802978516\n",
      "Training Loss at step 49 : 14.445606231689453\n",
      "Training Loss at step 50 : 14.515145301818848\n",
      "Test Loss 14.632133483886719 \n",
      "\n",
      "done with step 50 of 311\n",
      "current time: 0.3284486492474874 est epoch finish:  2.0028927434503645\n",
      "Training Loss at step 51 : 14.437420845031738\n",
      "Training Loss at step 52 : 14.502147674560547\n",
      "Training Loss at step 53 : 14.419245719909668\n",
      "Training Loss at step 54 : 14.409858703613281\n",
      "Training Loss at step 55 : 14.527795791625977\n",
      "Training Loss at step 56 : 14.452646255493164\n",
      "Training Loss at step 57 : 14.4701566696167\n",
      "Training Loss at step 58 : 14.676634788513184\n",
      "Training Loss at step 59 : 14.600128173828125\n",
      "Training Loss at step 60 : 14.575093269348145\n",
      "Test Loss 14.605663299560547 \n",
      "\n",
      "done with step 60 of 311\n",
      "current time: 0.39400264819463093 est epoch finish:  2.008767599811971\n",
      "Training Loss at step 61 : 14.466986656188965\n",
      "Training Loss at step 62 : 14.292312622070312\n",
      "Training Loss at step 63 : 14.329612731933594\n",
      "Training Loss at step 64 : 14.353239059448242\n",
      "Training Loss at step 65 : 14.40070629119873\n",
      "Training Loss at step 66 : 14.355830192565918\n",
      "Training Loss at step 67 : 14.41333293914795\n",
      "Training Loss at step 68 : 14.301450729370117\n",
      "Training Loss at step 69 : 14.08911418914795\n",
      "Training Loss at step 70 : 14.144020080566406\n",
      "Test Loss 14.58308219909668 \n",
      "\n",
      "done with step 70 of 311\n",
      "current time: 0.46028249263763427 est epoch finish:  2.0161669747930175\n",
      "Training Loss at step 71 : 14.797281265258789\n",
      "Training Loss at step 72 : 14.266841888427734\n",
      "Training Loss at step 73 : 14.207818984985352\n",
      "Training Loss at step 74 : 14.344566345214844\n",
      "Training Loss at step 75 : 14.428274154663086\n",
      "Training Loss at step 76 : 14.56521224975586\n",
      "Training Loss at step 77 : 14.219880104064941\n",
      "Training Loss at step 78 : 14.095708847045898\n",
      "Training Loss at step 79 : 14.319390296936035\n",
      "Training Loss at step 80 : 14.369339942932129\n",
      "Test Loss 14.560327529907227 \n",
      "\n",
      "done with step 80 of 311\n",
      "current time: 0.5243018706639607 est epoch finish:  2.0130602688455776\n",
      "Training Loss at step 81 : 14.419920921325684\n",
      "Training Loss at step 82 : 14.441702842712402\n",
      "Training Loss at step 83 : 14.288752555847168\n",
      "Training Loss at step 84 : 14.002073287963867\n",
      "Training Loss at step 85 : 14.172502517700195\n",
      "Training Loss at step 86 : 14.375255584716797\n",
      "Training Loss at step 87 : 14.369346618652344\n",
      "Training Loss at step 88 : 14.35531234741211\n",
      "Training Loss at step 89 : 14.14932632446289\n",
      "Training Loss at step 90 : 14.25786018371582\n",
      "Test Loss 14.555051803588867 \n",
      "\n",
      "done with step 90 of 311\n",
      "current time: 0.5898043831189473 est epoch finish:  2.0157050895603588\n",
      "Training Loss at step 91 : 14.176795959472656\n",
      "Training Loss at step 92 : 14.255077362060547\n",
      "Training Loss at step 93 : 14.112340927124023\n",
      "Training Loss at step 94 : 14.18418025970459\n",
      "Training Loss at step 95 : 13.98786735534668\n",
      "Training Loss at step 96 : 13.957626342773438\n",
      "Training Loss at step 97 : 14.039300918579102\n",
      "Training Loss at step 98 : 14.094903945922852\n",
      "Training Loss at step 99 : 14.12855339050293\n",
      "Training Loss at step 100 : 13.987893104553223\n",
      "Test Loss 14.538894653320312 \n",
      "\n",
      "done with step 100 of 311\n",
      "current time: 0.654053270816803 est epoch finish:  2.0139660121190666\n",
      "Training Loss at step 101 : 14.356401443481445\n",
      "Training Loss at step 102 : 14.509035110473633\n",
      "Training Loss at step 103 : 14.021677017211914\n",
      "Training Loss at step 104 : 14.099874496459961\n",
      "Training Loss at step 105 : 14.185898780822754\n",
      "Training Loss at step 106 : 14.336325645446777\n",
      "Training Loss at step 107 : 14.026288032531738\n",
      "Training Loss at step 108 : 14.129608154296875\n",
      "Training Loss at step 109 : 14.302682876586914\n",
      "Training Loss at step 110 : 14.168964385986328\n",
      "Test Loss 14.520773887634277 \n",
      "\n",
      "done with step 110 of 311\n",
      "current time: 0.719774619738261 est epoch finish:  2.0166658264738664\n",
      "Training Loss at step 111 : 13.900726318359375\n",
      "Training Loss at step 112 : 14.109943389892578\n",
      "Training Loss at step 113 : 14.423736572265625\n",
      "Training Loss at step 114 : 14.053512573242188\n",
      "Training Loss at step 115 : 13.99001407623291\n",
      "Training Loss at step 116 : 14.048162460327148\n",
      "Training Loss at step 117 : 14.380462646484375\n",
      "Training Loss at step 118 : 14.019929885864258\n",
      "Training Loss at step 119 : 14.171670913696289\n",
      "Training Loss at step 120 : 14.088581085205078\n",
      "Test Loss 14.500014305114746 \n",
      "\n",
      "done with step 120 of 311\n",
      "current time: 0.7854752143224081 est epoch finish:  2.018866046729495\n",
      "Training Loss at step 121 : 14.188457489013672\n",
      "Training Loss at step 122 : 14.323100090026855\n",
      "Training Loss at step 123 : 13.963747024536133\n",
      "Training Loss at step 124 : 14.208879470825195\n",
      "Training Loss at step 125 : 14.346389770507812\n",
      "Training Loss at step 126 : 14.347846031188965\n",
      "Training Loss at step 127 : 14.400190353393555\n",
      "Training Loss at step 128 : 14.050210952758789\n",
      "Training Loss at step 129 : 14.02358627319336\n",
      "Training Loss at step 130 : 13.87131404876709\n",
      "Test Loss 14.49460220336914 \n",
      "\n",
      "done with step 130 of 311\n",
      "current time: 0.8501970767974854 est epoch finish:  2.0184068006413582\n",
      "Training Loss at step 131 : 14.401603698730469\n",
      "Training Loss at step 132 : 14.004568099975586\n",
      "Training Loss at step 133 : 14.40561294555664\n",
      "Training Loss at step 134 : 14.289970397949219\n",
      "Training Loss at step 135 : 14.355634689331055\n",
      "Training Loss at step 136 : 14.111517906188965\n",
      "Training Loss at step 137 : 14.13900375366211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 138 : 14.141304969787598\n",
      "Training Loss at step 139 : 13.964757919311523\n",
      "Training Loss at step 140 : 13.989887237548828\n",
      "Test Loss 14.484991073608398 \n",
      "\n",
      "done with step 140 of 311\n",
      "current time: 0.9137767990430196 est epoch finish:  2.015493507109072\n",
      "Training Loss at step 141 : 14.272119522094727\n",
      "Training Loss at step 142 : 14.026771545410156\n",
      "Training Loss at step 143 : 14.262874603271484\n",
      "Training Loss at step 144 : 13.904277801513672\n",
      "Training Loss at step 145 : 13.968494415283203\n",
      "Training Loss at step 146 : 14.079180717468262\n",
      "Training Loss at step 147 : 14.188714981079102\n",
      "Training Loss at step 148 : 14.207623481750488\n",
      "Training Loss at step 149 : 14.084989547729492\n",
      "Training Loss at step 150 : 13.898887634277344\n",
      "Test Loss 14.469326972961426 \n",
      "\n",
      "done with step 150 of 311\n",
      "current time: 0.9755862792332967 est epoch finish:  2.0093200850434125\n",
      "Training Loss at step 151 : 14.138829231262207\n",
      "Training Loss at step 152 : 13.954305648803711\n",
      "Training Loss at step 153 : 14.029413223266602\n",
      "Training Loss at step 154 : 13.99413776397705\n",
      "Training Loss at step 155 : 13.687232971191406\n",
      "Training Loss at step 156 : 13.96645450592041\n",
      "Training Loss at step 157 : 14.260787010192871\n",
      "Training Loss at step 158 : 13.951891899108887\n",
      "Training Loss at step 159 : 14.088058471679688\n",
      "Training Loss at step 160 : 13.942368507385254\n",
      "Test Loss 14.44771957397461 \n",
      "\n",
      "done with step 160 of 311\n",
      "current time: 1.0404163082440694 est epoch finish:  2.0097482724466187\n",
      "Training Loss at step 161 : 13.704963684082031\n",
      "Training Loss at step 162 : 14.042366027832031\n",
      "Training Loss at step 163 : 13.825660705566406\n",
      "Training Loss at step 164 : 13.8961181640625\n",
      "Training Loss at step 165 : 14.39146614074707\n",
      "Training Loss at step 166 : 13.70366096496582\n",
      "Training Loss at step 167 : 13.737771034240723\n",
      "Training Loss at step 168 : 13.83504867553711\n",
      "Training Loss at step 169 : 14.117337226867676\n",
      "Training Loss at step 170 : 13.894553184509277\n",
      "Test Loss 14.44963264465332 \n",
      "\n",
      "done with step 170 of 311\n",
      "current time: 1.1038365244865418 est epoch finish:  2.007562334007687\n",
      "Training Loss at step 171 : 13.72494125366211\n",
      "Training Loss at step 172 : 13.917877197265625\n",
      "Training Loss at step 173 : 13.695648193359375\n",
      "Training Loss at step 174 : 13.89393424987793\n",
      "Training Loss at step 175 : 14.035669326782227\n",
      "Training Loss at step 176 : 13.993341445922852\n",
      "Training Loss at step 177 : 13.753375053405762\n",
      "Training Loss at step 178 : 14.076522827148438\n",
      "Training Loss at step 179 : 14.419418334960938\n",
      "Training Loss at step 180 : 13.951254844665527\n",
      "Test Loss 14.439092636108398 \n",
      "\n",
      "done with step 180 of 311\n",
      "current time: 1.1678898294766744 est epoch finish:  2.0067057291008052\n",
      "Training Loss at step 181 : 13.752630233764648\n",
      "Training Loss at step 182 : 14.310226440429688\n",
      "Training Loss at step 183 : 14.00960636138916\n",
      "Training Loss at step 184 : 14.041191101074219\n",
      "Training Loss at step 185 : 14.190780639648438\n",
      "Training Loss at step 186 : 13.46662712097168\n",
      "Training Loss at step 187 : 14.377053260803223\n",
      "Training Loss at step 188 : 14.127107620239258\n",
      "Training Loss at step 189 : 14.188405990600586\n",
      "Training Loss at step 190 : 13.940441131591797\n",
      "Test Loss 14.438897132873535 \n",
      "\n",
      "done with step 190 of 311\n",
      "current time: 1.2310486952463786 est epoch finish:  2.0044824304797055\n",
      "Training Loss at step 191 : 14.011390686035156\n",
      "Training Loss at step 192 : 14.0049409866333\n",
      "Training Loss at step 193 : 13.645040512084961\n",
      "Training Loss at step 194 : 13.91108226776123\n",
      "Training Loss at step 195 : 13.745424270629883\n",
      "Training Loss at step 196 : 14.176627159118652\n",
      "Training Loss at step 197 : 13.479598045349121\n",
      "Training Loss at step 198 : 13.9813232421875\n",
      "Training Loss at step 199 : 14.16145133972168\n",
      "Training Loss at step 200 : 14.117486953735352\n",
      "Test Loss 14.411205291748047 \n",
      "\n",
      "done with step 200 of 311\n",
      "current time: 1.2961629112561543 est epoch finish:  2.0055057980132536\n",
      "Training Loss at step 201 : 13.834880828857422\n",
      "Training Loss at step 202 : 13.779468536376953\n",
      "Training Loss at step 203 : 13.438849449157715\n",
      "Training Loss at step 204 : 14.202686309814453\n",
      "Training Loss at step 205 : 13.790281295776367\n",
      "Training Loss at step 206 : 13.633124351501465\n",
      "Training Loss at step 207 : 14.019214630126953\n",
      "Training Loss at step 208 : 14.20882797241211\n",
      "Training Loss at step 209 : 14.062604904174805\n",
      "Training Loss at step 210 : 14.009439468383789\n",
      "Test Loss 14.426589965820312 \n",
      "\n",
      "done with step 210 of 311\n",
      "current time: 1.3597421129544576 est epoch finish:  2.0041696546390346\n",
      "Training Loss at step 211 : 14.047981262207031\n",
      "Training Loss at step 212 : 13.642879486083984\n",
      "Training Loss at step 213 : 14.285975456237793\n",
      "Training Loss at step 214 : 13.269744873046875\n",
      "Training Loss at step 215 : 13.582893371582031\n",
      "Training Loss at step 216 : 13.726692199707031\n",
      "Training Loss at step 217 : 14.289350509643555\n",
      "Training Loss at step 218 : 13.24688720703125\n",
      "Training Loss at step 219 : 13.994918823242188\n",
      "Training Loss at step 220 : 13.913463592529297\n",
      "Test Loss 14.411447525024414 \n",
      "\n",
      "done with step 220 of 311\n",
      "current time: 1.4239156166712443 est epoch finish:  2.003790754682158\n",
      "Training Loss at step 221 : 14.105620384216309\n",
      "Training Loss at step 222 : 13.536079406738281\n",
      "Training Loss at step 223 : 13.782431602478027\n",
      "Training Loss at step 224 : 13.737552642822266\n",
      "Training Loss at step 225 : 13.541467666625977\n",
      "Training Loss at step 226 : 13.871767044067383\n",
      "Training Loss at step 227 : 13.522050857543945\n",
      "Training Loss at step 228 : 13.655017852783203\n",
      "Training Loss at step 229 : 13.464973449707031\n",
      "Training Loss at step 230 : 14.324613571166992\n",
      "Test Loss 14.431180000305176 \n",
      "\n",
      "done with step 230 of 311\n",
      "current time: 1.4882903138796488 est epoch finish:  2.0037155308076655\n",
      "Training Loss at step 231 : 13.74195671081543\n",
      "Training Loss at step 232 : 14.25015640258789\n",
      "Training Loss at step 233 : 14.121622085571289\n",
      "Training Loss at step 234 : 13.710030555725098\n",
      "Training Loss at step 235 : 13.483259201049805\n",
      "Training Loss at step 236 : 13.943746566772461\n",
      "Training Loss at step 237 : 13.8501558303833\n",
      "Training Loss at step 238 : 13.561103820800781\n",
      "Training Loss at step 239 : 13.576066017150879\n",
      "Training Loss at step 240 : 14.086276054382324\n",
      "Test Loss 14.42379093170166 \n",
      "\n",
      "done with step 240 of 311\n",
      "current time: 1.5534059882164002 est epoch finish:  2.004602748279255\n",
      "Training Loss at step 241 : 13.988229751586914\n",
      "Training Loss at step 242 : 13.953486442565918\n",
      "Training Loss at step 243 : 13.780866622924805\n",
      "Training Loss at step 244 : 14.172706604003906\n",
      "Training Loss at step 245 : 13.583564758300781\n",
      "Training Loss at step 246 : 14.135910987854004\n",
      "Training Loss at step 247 : 13.821758270263672\n",
      "Training Loss at step 248 : 13.652664184570312\n",
      "Training Loss at step 249 : 14.362305641174316\n",
      "Training Loss at step 250 : 13.884180068969727\n",
      "Test Loss 14.421159744262695 \n",
      "\n",
      "done with step 250 of 311\n",
      "current time: 1.618864321708679 est epoch finish:  2.005843840842228\n",
      "Training Loss at step 251 : 13.758759498596191\n",
      "Training Loss at step 252 : 13.85367202758789\n",
      "Training Loss at step 253 : 13.594797134399414\n",
      "Training Loss at step 254 : 14.029624938964844\n",
      "Training Loss at step 255 : 13.31026840209961\n",
      "Training Loss at step 256 : 13.820423126220703\n",
      "Training Loss at step 257 : 13.301692962646484\n",
      "Training Loss at step 258 : 13.744586944580078\n",
      "Training Loss at step 259 : 13.656843185424805\n",
      "Training Loss at step 260 : 13.378129959106445\n",
      "Test Loss 14.434831619262695 \n",
      "\n",
      "done with step 260 of 311\n",
      "current time: 1.684266996383667 est epoch finish:  2.0069235091008446\n",
      "Training Loss at step 261 : 13.605558395385742\n",
      "Training Loss at step 262 : 13.551290512084961\n",
      "Training Loss at step 263 : 13.434206008911133\n",
      "Training Loss at step 264 : 13.875997543334961\n",
      "Training Loss at step 265 : 14.063862800598145\n",
      "Training Loss at step 266 : 13.528127670288086\n",
      "Training Loss at step 267 : 14.005199432373047\n",
      "Training Loss at step 268 : 13.379056930541992\n",
      "Training Loss at step 269 : 14.052396774291992\n",
      "Training Loss at step 270 : 13.569891929626465\n",
      "Test Loss 14.412954330444336 \n",
      "\n",
      "done with step 270 of 311\n",
      "current time: 1.7517306447029113 est epoch finish:  2.0102886734413485\n",
      "Training Loss at step 271 : 14.156376838684082\n",
      "Training Loss at step 272 : 14.540592193603516\n",
      "Training Loss at step 273 : 14.026281356811523\n",
      "Training Loss at step 274 : 13.466514587402344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 275 : 13.759088516235352\n",
      "Training Loss at step 276 : 13.976300239562988\n",
      "Training Loss at step 277 : 14.11009693145752\n",
      "Training Loss at step 278 : 13.846817016601562\n",
      "Training Loss at step 279 : 13.529170989990234\n",
      "Training Loss at step 280 : 14.044897079467773\n",
      "Test Loss 14.435081481933594 \n",
      "\n",
      "done with step 280 of 311\n",
      "current time: 1.819424311319987 est epoch finish:  2.0136688997171386\n",
      "Training Loss at step 281 : 13.88992977142334\n",
      "Training Loss at step 282 : 13.429948806762695\n",
      "Training Loss at step 283 : 13.854198455810547\n",
      "Training Loss at step 284 : 12.915481567382812\n",
      "Training Loss at step 285 : 13.7476806640625\n",
      "Training Loss at step 286 : 14.205766677856445\n",
      "Training Loss at step 287 : 13.832663536071777\n",
      "Training Loss at step 288 : 13.262690544128418\n",
      "Training Loss at step 289 : 13.662324905395508\n",
      "Training Loss at step 290 : 13.929732322692871\n",
      "Test Loss 14.420700073242188 \n",
      "\n",
      "done with step 290 of 311\n",
      "current time: 1.8861392339070637 est epoch finish:  2.015770796374903\n",
      "Training Loss at step 291 : 14.071735382080078\n",
      "Training Loss at step 292 : 13.565109252929688\n",
      "Training Loss at step 293 : 13.928653717041016\n",
      "Training Loss at step 294 : 14.013606071472168\n",
      "Training Loss at step 295 : 13.705116271972656\n",
      "Training Loss at step 296 : 13.562875747680664\n",
      "Training Loss at step 297 : 14.019002914428711\n",
      "Training Loss at step 298 : 13.59841537475586\n",
      "Training Loss at step 299 : 14.019274711608887\n",
      "Training Loss at step 300 : 13.533583641052246\n",
      "Test Loss 14.414566040039062 \n",
      "\n",
      "done with step 300 of 311\n",
      "current time: 1.9532416820526124 est epoch finish:  2.0181334322869184\n",
      "Training Loss at step 301 : 13.496321678161621\n",
      "Training Loss at step 302 : 13.13154411315918\n",
      "Training Loss at step 303 : 13.589690208435059\n",
      "Training Loss at step 304 : 13.635906219482422\n",
      "Training Loss at step 305 : 13.077180862426758\n",
      "Training Loss at step 306 : 14.44054126739502\n",
      "Training Loss at step 307 : 13.67647933959961\n",
      "Training Loss at step 308 : 14.1489839553833\n",
      "Training Loss at step 309 : 13.087672233581543\n",
      "Training Loss at step 310 : 14.090631484985352\n",
      "Test Loss 14.41457748413086 \n",
      "\n",
      "done with step 310 of 311\n",
      "current time: 2.020219608147939 est epoch finish:  2.0202196081479387\n",
      "Starting training epoch 1\n",
      "constructing batches...\n",
      "batches constructed. starting training steps...\n",
      "Training Loss at step 0 : 14.015278816223145\n",
      "Test Loss 14.410292625427246 \n",
      "\n",
      "done with step 0 of 311\n",
      "current time: 0.0108177383740743 est epoch finish:  3.3643166343371074\n",
      "Training Loss at step 1 : 13.563302993774414\n",
      "Training Loss at step 2 : 13.544885635375977\n",
      "Training Loss at step 3 : 13.140053749084473\n",
      "Training Loss at step 4 : 14.58255386352539\n",
      "Training Loss at step 5 : 13.182435035705566\n",
      "Training Loss at step 6 : 13.329141616821289\n",
      "Training Loss at step 7 : 13.078009605407715\n",
      "Training Loss at step 8 : 13.995499610900879\n",
      "Training Loss at step 9 : 13.674520492553711\n",
      "Training Loss at step 10 : 13.91430377960205\n",
      "Test Loss 14.410250663757324 \n",
      "\n",
      "done with step 10 of 311\n",
      "current time: 0.07688806454340617 est epoch finish:  2.173835279363574\n",
      "Training Loss at step 11 : 13.771764755249023\n",
      "Training Loss at step 12 : 13.892004013061523\n",
      "Training Loss at step 13 : 13.893630027770996\n",
      "Training Loss at step 14 : 12.951181411743164\n",
      "Training Loss at step 15 : 13.390128135681152\n",
      "Training Loss at step 16 : 13.263792037963867\n",
      "Training Loss at step 17 : 14.007205963134766\n",
      "Training Loss at step 18 : 13.265199661254883\n",
      "Training Loss at step 19 : 14.349359512329102\n",
      "Training Loss at step 20 : 13.394264221191406\n",
      "Test Loss 14.414249420166016 \n",
      "\n",
      "done with step 20 of 311\n",
      "current time: 0.14255385398864745 est epoch finish:  2.1111546947842554\n",
      "Training Loss at step 21 : 13.646552085876465\n",
      "Training Loss at step 22 : 13.591594696044922\n",
      "Training Loss at step 23 : 13.795137405395508\n",
      "Training Loss at step 24 : 14.427184104919434\n",
      "Training Loss at step 25 : 13.779062271118164\n",
      "Training Loss at step 26 : 14.141189575195312\n",
      "Training Loss at step 27 : 13.505277633666992\n",
      "Training Loss at step 28 : 13.500935554504395\n",
      "Training Loss at step 29 : 13.569515228271484\n",
      "Training Loss at step 30 : 14.299003601074219\n",
      "Test Loss 14.427848815917969 \n",
      "\n",
      "done with step 30 of 311\n",
      "current time: 0.2074251929918925 est epoch finish:  2.080943065176728\n",
      "Training Loss at step 31 : 13.06010627746582\n",
      "Training Loss at step 32 : 13.534400939941406\n",
      "Training Loss at step 33 : 14.796689987182617\n",
      "Training Loss at step 34 : 13.790998458862305\n",
      "Training Loss at step 35 : 13.055917739868164\n",
      "Training Loss at step 36 : 13.492692947387695\n",
      "Training Loss at step 37 : 13.419870376586914\n",
      "Training Loss at step 38 : 12.34328842163086\n",
      "Training Loss at step 39 : 13.448892593383789\n",
      "Training Loss at step 40 : 13.38985824584961\n",
      "Test Loss 14.416008949279785 \n",
      "\n",
      "done with step 40 of 311\n",
      "current time: 0.2736406445503235 est epoch finish:  2.075664401345137\n",
      "Training Loss at step 41 : 13.350841522216797\n",
      "Training Loss at step 42 : 13.354324340820312\n",
      "Training Loss at step 43 : 13.065197944641113\n",
      "Training Loss at step 44 : 13.865005493164062\n",
      "Training Loss at step 45 : 13.869674682617188\n",
      "Training Loss at step 46 : 13.462362289428711\n",
      "Training Loss at step 47 : 13.885744094848633\n",
      "Training Loss at step 48 : 14.005620956420898\n",
      "Training Loss at step 49 : 14.331982612609863\n",
      "Training Loss at step 50 : 13.22917366027832\n",
      "Test Loss 14.420920372009277 \n",
      "\n",
      "done with step 50 of 311\n",
      "current time: 0.34065791765848796 est epoch finish:  2.0773453410154854\n",
      "Training Loss at step 51 : 14.252519607543945\n",
      "Training Loss at step 52 : 13.695355415344238\n",
      "Training Loss at step 53 : 13.285906791687012\n",
      "Training Loss at step 54 : 13.46639633178711\n",
      "Training Loss at step 55 : 13.947786331176758\n",
      "Training Loss at step 56 : 13.520644187927246\n",
      "Training Loss at step 57 : 12.929727554321289\n",
      "Training Loss at step 58 : 13.600994110107422\n",
      "Training Loss at step 59 : 13.547690391540527\n",
      "Training Loss at step 60 : 14.186896324157715\n",
      "Test Loss 14.412734985351562 \n",
      "\n",
      "done with step 60 of 311\n",
      "current time: 0.4087437589963277 est epoch finish:  2.0839230991452116\n",
      "Training Loss at step 61 : 13.522603988647461\n",
      "Training Loss at step 62 : 13.273869514465332\n",
      "Training Loss at step 63 : 13.99136734008789\n",
      "Training Loss at step 64 : 13.221549987792969\n",
      "Training Loss at step 65 : 13.680288314819336\n",
      "Training Loss at step 66 : 13.923616409301758\n",
      "Training Loss at step 67 : 13.19082260131836\n",
      "Training Loss at step 68 : 13.362069129943848\n",
      "Training Loss at step 69 : 13.24866008758545\n",
      "Training Loss at step 70 : 13.26435661315918\n",
      "Test Loss 14.409931182861328 \n",
      "\n",
      "done with step 70 of 311\n",
      "current time: 0.4740032951037089 est epoch finish:  2.0762679546092038\n",
      "Training Loss at step 71 : 14.057943344116211\n",
      "Training Loss at step 72 : 14.42625904083252\n",
      "Training Loss at step 73 : 13.31218147277832\n",
      "Training Loss at step 74 : 13.37185287475586\n",
      "Training Loss at step 75 : 13.197671890258789\n",
      "Training Loss at step 76 : 13.130340576171875\n",
      "Training Loss at step 77 : 13.629610061645508\n",
      "Training Loss at step 78 : 13.648228645324707\n",
      "Training Loss at step 79 : 13.921255111694336\n",
      "Training Loss at step 80 : 12.87657356262207\n",
      "Test Loss 14.408286094665527 \n",
      "\n",
      "done with step 80 of 311\n",
      "current time: 0.5402804811795553 est epoch finish:  2.074410242553601\n",
      "Training Loss at step 81 : 13.2622709274292\n",
      "Training Loss at step 82 : 13.842811584472656\n",
      "Training Loss at step 83 : 14.517790794372559\n",
      "Training Loss at step 84 : 13.564595222473145\n",
      "Training Loss at step 85 : 13.500984191894531\n",
      "Training Loss at step 86 : 13.376440048217773\n",
      "Training Loss at step 87 : 13.223320007324219\n",
      "Training Loss at step 88 : 13.566299438476562\n",
      "Training Loss at step 89 : 12.872406005859375\n",
      "Training Loss at step 90 : 14.129839897155762\n",
      "Test Loss 14.416915893554688 \n",
      "\n",
      "done with step 90 of 311\n",
      "current time: 0.606015125910441 est epoch finish:  2.0711066391005186\n",
      "Training Loss at step 91 : 13.221502304077148\n",
      "Training Loss at step 92 : 14.241849899291992\n",
      "Training Loss at step 93 : 13.355506896972656\n",
      "Training Loss at step 94 : 13.531856536865234\n",
      "Training Loss at step 95 : 13.960098266601562\n",
      "Training Loss at step 96 : 14.212772369384766\n",
      "Training Loss at step 97 : 13.086347579956055\n",
      "Training Loss at step 98 : 13.01279067993164\n",
      "Training Loss at step 99 : 13.820242881774902\n",
      "Training Loss at step 100 : 13.95442008972168\n",
      "Test Loss 14.432524681091309 \n",
      "\n",
      "done with step 100 of 311\n",
      "current time: 0.6734841267267863 est epoch finish:  2.0737976575448567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 101 : 13.081175804138184\n",
      "Training Loss at step 102 : 13.608415603637695\n",
      "Training Loss at step 103 : 13.568195343017578\n",
      "Training Loss at step 104 : 13.751971244812012\n",
      "Training Loss at step 105 : 13.440061569213867\n",
      "Training Loss at step 106 : 13.408931732177734\n",
      "Training Loss at step 107 : 13.024662017822266\n",
      "Training Loss at step 108 : 12.553852081298828\n",
      "Training Loss at step 109 : 12.523609161376953\n",
      "Training Loss at step 110 : 13.171409606933594\n",
      "Test Loss 14.427536010742188 \n",
      "\n",
      "done with step 110 of 311\n",
      "current time: 0.7408387581507365 est epoch finish:  2.075683367431343\n",
      "Training Loss at step 111 : 13.76736068725586\n",
      "Training Loss at step 112 : 14.42209243774414\n",
      "Training Loss at step 113 : 12.77018928527832\n",
      "Training Loss at step 114 : 13.640714645385742\n",
      "Training Loss at step 115 : 13.607872009277344\n",
      "Training Loss at step 116 : 13.017206192016602\n",
      "Training Loss at step 117 : 13.498541831970215\n",
      "Training Loss at step 118 : 13.130196571350098\n",
      "Training Loss at step 119 : 13.821210861206055\n",
      "Training Loss at step 120 : 12.697227478027344\n",
      "Test Loss 14.431112289428711 \n",
      "\n",
      "done with step 120 of 311\n",
      "current time: 0.8087193369865417 est epoch finish:  2.078609204981938\n",
      "Training Loss at step 121 : 13.146600723266602\n",
      "Training Loss at step 122 : 13.991372108459473\n",
      "Training Loss at step 123 : 13.415855407714844\n",
      "Training Loss at step 124 : 13.655948638916016\n",
      "Training Loss at step 125 : 12.832559585571289\n",
      "Training Loss at step 126 : 14.426314353942871\n",
      "Training Loss at step 127 : 13.174216270446777\n",
      "Training Loss at step 128 : 13.20197868347168\n",
      "Training Loss at step 129 : 13.820249557495117\n",
      "Training Loss at step 130 : 13.599529266357422\n",
      "Test Loss 14.426908493041992 \n",
      "\n",
      "done with step 130 of 311\n",
      "current time: 0.8741631468137105 est epoch finish:  2.075303348542473\n",
      "Training Loss at step 131 : 12.774837493896484\n",
      "Training Loss at step 132 : 13.617557525634766\n",
      "Training Loss at step 133 : 13.457504272460938\n",
      "Training Loss at step 134 : 14.074831008911133\n",
      "Training Loss at step 135 : 13.217245101928711\n",
      "Training Loss at step 136 : 13.379938125610352\n",
      "Training Loss at step 137 : 13.38443660736084\n",
      "Training Loss at step 138 : 12.478145599365234\n",
      "Training Loss at step 139 : 13.20521068572998\n",
      "Training Loss at step 140 : 13.89070987701416\n",
      "Test Loss 14.4185791015625 \n",
      "\n",
      "done with step 140 of 311\n",
      "current time: 0.938639231522878 est epoch finish:  2.070331922011454\n",
      "Training Loss at step 141 : 12.271961212158203\n",
      "Training Loss at step 142 : 13.729164123535156\n",
      "Training Loss at step 143 : 13.160343170166016\n",
      "Training Loss at step 144 : 13.872793197631836\n",
      "Training Loss at step 145 : 13.14665412902832\n",
      "Training Loss at step 146 : 13.653375625610352\n",
      "Training Loss at step 147 : 13.817898750305176\n",
      "Training Loss at step 148 : 13.490583419799805\n",
      "Training Loss at step 149 : 12.971906661987305\n",
      "Training Loss at step 150 : 13.421309471130371\n",
      "Test Loss 14.427143096923828 \n",
      "\n",
      "done with step 150 of 311\n",
      "current time: 1.0042787591616313 est epoch finish:  2.06841519271038\n",
      "Training Loss at step 151 : 13.5191650390625\n",
      "Training Loss at step 152 : 12.65165901184082\n",
      "Training Loss at step 153 : 12.53211784362793\n",
      "Training Loss at step 154 : 14.041403770446777\n",
      "Training Loss at step 155 : 14.676979064941406\n",
      "Training Loss at step 156 : 13.508337020874023\n",
      "Training Loss at step 157 : 12.871116638183594\n",
      "Training Loss at step 158 : 13.276544570922852\n",
      "Training Loss at step 159 : 13.120174407958984\n",
      "Training Loss at step 160 : 12.862035751342773\n",
      "Test Loss 14.430119514465332 \n",
      "\n",
      "done with step 160 of 311\n",
      "current time: 1.071862228711446 est epoch finish:  2.0704916343432282\n",
      "Training Loss at step 161 : 13.749622344970703\n",
      "Training Loss at step 162 : 13.800705909729004\n",
      "Training Loss at step 163 : 13.326764106750488\n",
      "Training Loss at step 164 : 13.517077445983887\n",
      "Training Loss at step 165 : 13.71562671661377\n",
      "Training Loss at step 166 : 12.85087776184082\n",
      "Training Loss at step 167 : 13.854597091674805\n",
      "Training Loss at step 168 : 13.573686599731445\n",
      "Training Loss at step 169 : 12.573713302612305\n",
      "Training Loss at step 170 : 13.78050422668457\n",
      "Test Loss 14.441139221191406 \n",
      "\n",
      "done with step 170 of 311\n",
      "current time: 1.137487777074178 est epoch finish:  2.068764319708008\n",
      "Training Loss at step 171 : 13.715737342834473\n",
      "Training Loss at step 172 : 13.549942016601562\n",
      "Training Loss at step 173 : 12.937732696533203\n",
      "Training Loss at step 174 : 13.136052131652832\n",
      "Training Loss at step 175 : 13.589362144470215\n",
      "Training Loss at step 176 : 13.150297164916992\n",
      "Training Loss at step 177 : 13.471104621887207\n",
      "Training Loss at step 178 : 12.51291275024414\n",
      "Training Loss at step 179 : 13.334762573242188\n",
      "Training Loss at step 180 : 13.840824127197266\n",
      "Test Loss 14.425725936889648 \n",
      "\n",
      "done with step 180 of 311\n",
      "current time: 1.2028565843900045 est epoch finish:  2.066786727874538\n",
      "Training Loss at step 181 : 13.286246299743652\n",
      "Training Loss at step 182 : 13.899965286254883\n",
      "Training Loss at step 183 : 13.067048072814941\n",
      "Training Loss at step 184 : 13.957420349121094\n",
      "Training Loss at step 185 : 13.425034523010254\n",
      "Training Loss at step 186 : 12.830979347229004\n",
      "Training Loss at step 187 : 13.433767318725586\n",
      "Training Loss at step 188 : 13.509689331054688\n",
      "Training Loss at step 189 : 13.681644439697266\n",
      "Training Loss at step 190 : 12.978565216064453\n",
      "Test Loss 14.425468444824219 \n",
      "\n",
      "done with step 190 of 311\n",
      "current time: 1.2714613517125448 est epoch finish:  2.0702852376052427\n",
      "Training Loss at step 191 : 12.602510452270508\n",
      "Training Loss at step 192 : 13.991337776184082\n",
      "Training Loss at step 193 : 12.595178604125977\n",
      "Training Loss at step 194 : 13.519773483276367\n",
      "Training Loss at step 195 : 13.612092971801758\n",
      "Training Loss at step 196 : 13.479072570800781\n",
      "Training Loss at step 197 : 12.888775825500488\n",
      "Training Loss at step 198 : 13.434659957885742\n",
      "Training Loss at step 199 : 13.032304763793945\n",
      "Training Loss at step 200 : 14.198149681091309\n",
      "Test Loss 14.417243957519531 \n",
      "\n",
      "done with step 200 of 311\n",
      "current time: 1.3416833877563477 est epoch finish:  2.0759379780707667\n",
      "Training Loss at step 201 : 12.246931076049805\n",
      "Training Loss at step 202 : 13.198114395141602\n",
      "Training Loss at step 203 : 13.293127059936523\n",
      "Training Loss at step 204 : 13.195287704467773\n",
      "Training Loss at step 205 : 13.359567642211914\n",
      "Training Loss at step 206 : 12.2573881149292\n",
      "Training Loss at step 207 : 13.602916717529297\n",
      "Training Loss at step 208 : 13.39542007446289\n",
      "Training Loss at step 209 : 14.834945678710938\n",
      "Training Loss at step 210 : 12.361104965209961\n",
      "Test Loss 14.410548210144043 \n",
      "\n",
      "done with step 210 of 311\n",
      "current time: 1.410511573155721 est epoch finish:  2.0790004703859206\n",
      "Training Loss at step 211 : 14.19565486907959\n",
      "Training Loss at step 212 : 13.078067779541016\n",
      "Training Loss at step 213 : 12.295525550842285\n",
      "Training Loss at step 214 : 13.83830738067627\n",
      "Training Loss at step 215 : 12.979438781738281\n",
      "Training Loss at step 216 : 13.711824417114258\n",
      "Training Loss at step 217 : 13.18726921081543\n",
      "Training Loss at step 218 : 13.58248233795166\n",
      "Training Loss at step 219 : 14.204411506652832\n",
      "Training Loss at step 220 : 12.991872787475586\n",
      "Test Loss 14.427067756652832 \n",
      "\n",
      "done with step 220 of 311\n",
      "current time: 1.4770319660504658 est epoch finish:  2.078538196568755\n",
      "Training Loss at step 221 : 13.338156700134277\n",
      "Training Loss at step 222 : 13.77224349975586\n",
      "Training Loss at step 223 : 13.69668197631836\n",
      "Training Loss at step 224 : 12.436919212341309\n",
      "Training Loss at step 225 : 12.801422119140625\n",
      "Training Loss at step 226 : 12.867538452148438\n",
      "Training Loss at step 227 : 12.445884704589844\n",
      "Training Loss at step 228 : 12.590764045715332\n",
      "Training Loss at step 229 : 12.924568176269531\n",
      "Training Loss at step 230 : 14.674217224121094\n",
      "Test Loss 14.44676685333252 \n",
      "\n",
      "done with step 230 of 311\n",
      "current time: 1.5431833863258362 est epoch finish:  2.0776191911140045\n",
      "Training Loss at step 231 : 12.3958158493042\n",
      "Training Loss at step 232 : 13.405131340026855\n",
      "Training Loss at step 233 : 13.478832244873047\n",
      "Training Loss at step 234 : 13.677472114562988\n",
      "Training Loss at step 235 : 13.367792129516602\n",
      "Training Loss at step 236 : 13.760848999023438\n",
      "Training Loss at step 237 : 13.175703048706055\n",
      "Training Loss at step 238 : 12.986696243286133\n",
      "Training Loss at step 239 : 13.380138397216797\n",
      "Training Loss at step 240 : 13.36510944366455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 14.445720672607422 \n",
      "\n",
      "done with step 240 of 311\n",
      "current time: 1.6101922591527302 est epoch finish:  2.077882956831947\n",
      "Training Loss at step 241 : 13.958657264709473\n",
      "Training Loss at step 242 : 11.826604843139648\n",
      "Training Loss at step 243 : 13.099632263183594\n",
      "Training Loss at step 244 : 13.531580924987793\n",
      "Training Loss at step 245 : 12.88785171508789\n",
      "Training Loss at step 246 : 13.23388957977295\n",
      "Training Loss at step 247 : 13.044831275939941\n",
      "Training Loss at step 248 : 12.97305965423584\n",
      "Training Loss at step 249 : 13.947416305541992\n",
      "Training Loss at step 250 : 13.702718734741211\n",
      "Test Loss 14.447389602661133 \n",
      "\n",
      "done with step 250 of 311\n",
      "current time: 1.6765708724657695 est epoch finish:  2.0773447862026067\n",
      "Training Loss at step 251 : 12.810556411743164\n",
      "Training Loss at step 252 : 12.394550323486328\n",
      "Training Loss at step 253 : 12.547500610351562\n",
      "Training Loss at step 254 : 13.24925708770752\n",
      "Training Loss at step 255 : 12.890790939331055\n",
      "Training Loss at step 256 : 14.373592376708984\n",
      "Training Loss at step 257 : 11.431245803833008\n",
      "Training Loss at step 258 : 13.692482948303223\n",
      "Training Loss at step 259 : 13.231669425964355\n",
      "Training Loss at step 260 : 12.878604888916016\n",
      "Test Loss 14.444620132446289 \n",
      "\n",
      "done with step 260 of 311\n",
      "current time: 1.7411585172017416 est epoch finish:  2.074713788696328\n",
      "Training Loss at step 261 : 13.366981506347656\n",
      "Training Loss at step 262 : 12.620267868041992\n",
      "Training Loss at step 263 : 12.556072235107422\n",
      "Training Loss at step 264 : 12.87436580657959\n",
      "Training Loss at step 265 : 13.249320983886719\n",
      "Training Loss at step 266 : 12.81101131439209\n",
      "Training Loss at step 267 : 12.840507507324219\n",
      "Training Loss at step 268 : 13.567044258117676\n",
      "Training Loss at step 269 : 12.166947364807129\n",
      "Training Loss at step 270 : 13.410294532775879\n",
      "Test Loss 14.470209121704102 \n",
      "\n",
      "done with step 270 of 311\n",
      "current time: 1.8076276620229086 est epoch finish:  2.0744361730225997\n",
      "Training Loss at step 271 : 13.451164245605469\n",
      "Training Loss at step 272 : 12.692081451416016\n",
      "Training Loss at step 273 : 12.608343124389648\n",
      "Training Loss at step 274 : 13.641151428222656\n",
      "Training Loss at step 275 : 12.641891479492188\n",
      "Training Loss at step 276 : 13.838401794433594\n",
      "Training Loss at step 277 : 13.470648765563965\n",
      "Training Loss at step 278 : 12.294041633605957\n",
      "Training Loss at step 279 : 12.667892456054688\n",
      "Training Loss at step 280 : 13.304759979248047\n",
      "Test Loss 14.454816818237305 \n",
      "\n",
      "done with step 280 of 311\n",
      "current time: 1.872982092698415 est epoch finish:  2.072944593698246\n",
      "Training Loss at step 281 : 13.9769868850708\n",
      "Training Loss at step 282 : 13.00651741027832\n",
      "Training Loss at step 283 : 12.565686225891113\n",
      "Training Loss at step 284 : 12.554506301879883\n",
      "Training Loss at step 285 : 12.858658790588379\n",
      "Training Loss at step 286 : 11.456839561462402\n",
      "Training Loss at step 287 : 13.338193893432617\n",
      "Training Loss at step 288 : 13.16691780090332\n",
      "Training Loss at step 289 : 13.863438606262207\n",
      "Training Loss at step 290 : 12.370784759521484\n",
      "Test Loss 14.460967063903809 \n",
      "\n",
      "done with step 290 of 311\n",
      "current time: 1.9375353137652078 est epoch finish:  2.0706992528556003\n",
      "Training Loss at step 291 : 14.422784805297852\n",
      "Training Loss at step 292 : 12.823287963867188\n",
      "Training Loss at step 293 : 13.599143981933594\n",
      "Training Loss at step 294 : 13.603069305419922\n",
      "Training Loss at step 295 : 13.16223430633545\n",
      "Training Loss at step 296 : 12.930977821350098\n",
      "Training Loss at step 297 : 13.70925521850586\n",
      "Training Loss at step 298 : 13.315984725952148\n",
      "Training Loss at step 299 : 13.723345756530762\n",
      "Training Loss at step 300 : 12.936349868774414\n",
      "Test Loss 14.463016510009766 \n",
      "\n",
      "done with step 300 of 311\n",
      "current time: 2.0038167317708333 est epoch finish:  2.070388716215047\n",
      "Training Loss at step 301 : 12.749614715576172\n",
      "Training Loss at step 302 : 12.678269386291504\n",
      "Training Loss at step 303 : 12.738975524902344\n",
      "Training Loss at step 304 : 13.48171615600586\n",
      "Training Loss at step 305 : 14.178635597229004\n",
      "Training Loss at step 306 : 13.286563873291016\n",
      "Training Loss at step 307 : 13.096965789794922\n",
      "Training Loss at step 308 : 12.940984725952148\n",
      "Training Loss at step 309 : 12.944368362426758\n",
      "Training Loss at step 310 : 13.355672836303711\n",
      "Test Loss 14.462813377380371 \n",
      "\n",
      "done with step 310 of 311\n",
      "current time: 2.0687145590782166 est epoch finish:  2.068714559078216\n",
      "Starting training epoch 2\n",
      "constructing batches...\n",
      "batches constructed. starting training steps...\n",
      "Training Loss at step 0 : 14.099592208862305\n",
      "Test Loss 14.448454856872559 \n",
      "\n",
      "done with step 0 of 311\n",
      "current time: 0.010744094848632812 est epoch finish:  3.3414134979248047\n",
      "Training Loss at step 1 : 12.56119441986084\n",
      "Training Loss at step 2 : 13.844728469848633\n",
      "Training Loss at step 3 : 13.812071800231934\n",
      "Training Loss at step 4 : 13.749927520751953\n",
      "Training Loss at step 5 : 12.7537260055542\n",
      "Training Loss at step 6 : 12.912875175476074\n",
      "Training Loss at step 7 : 13.054727554321289\n",
      "Training Loss at step 8 : 11.910514831542969\n",
      "Training Loss at step 9 : 12.184822082519531\n",
      "Training Loss at step 10 : 12.017854690551758\n",
      "Test Loss 14.501476287841797 \n",
      "\n",
      "done with step 10 of 311\n",
      "current time: 0.07696139415105184 est epoch finish:  2.1759085073615565\n",
      "Training Loss at step 11 : 12.096748352050781\n",
      "Training Loss at step 12 : 13.484818458557129\n",
      "Training Loss at step 13 : 12.463592529296875\n",
      "Training Loss at step 14 : 13.607336044311523\n",
      "Training Loss at step 15 : 12.285163879394531\n",
      "Training Loss at step 16 : 14.714118003845215\n",
      "Training Loss at step 17 : 13.640963554382324\n",
      "Training Loss at step 18 : 12.804665565490723\n",
      "Training Loss at step 19 : 12.351907730102539\n",
      "Training Loss at step 20 : 12.755243301391602\n",
      "Test Loss 14.49012279510498 \n",
      "\n",
      "done with step 20 of 311\n",
      "current time: 0.14488256772359212 est epoch finish:  2.145641836287483\n",
      "Training Loss at step 21 : 12.288483619689941\n",
      "Training Loss at step 22 : 11.417182922363281\n",
      "Training Loss at step 23 : 13.736749649047852\n",
      "Training Loss at step 24 : 12.938976287841797\n",
      "Training Loss at step 25 : 13.292248725891113\n",
      "Training Loss at step 26 : 14.465353012084961\n",
      "Training Loss at step 27 : 12.355225563049316\n",
      "Training Loss at step 28 : 13.105331420898438\n",
      "Training Loss at step 29 : 13.335490226745605\n",
      "Training Loss at step 30 : 13.629392623901367\n",
      "Test Loss 14.507087707519531 \n",
      "\n",
      "done with step 30 of 311\n",
      "current time: 0.21053268512090048 est epoch finish:  2.112118228148389\n",
      "Training Loss at step 31 : 12.462764739990234\n",
      "Training Loss at step 32 : 13.29936408996582\n",
      "Training Loss at step 33 : 12.89091968536377\n",
      "Training Loss at step 34 : 13.408056259155273\n",
      "Training Loss at step 35 : 14.226612091064453\n",
      "Training Loss at step 36 : 14.018107414245605\n",
      "Training Loss at step 37 : 13.648855209350586\n",
      "Training Loss at step 38 : 13.568570137023926\n",
      "Training Loss at step 39 : 14.174093246459961\n",
      "Training Loss at step 40 : 12.506643295288086\n",
      "Test Loss 14.468452453613281 \n",
      "\n",
      "done with step 40 of 311\n",
      "current time: 0.2758013884226481 est epoch finish:  2.09205443413277\n",
      "Training Loss at step 41 : 13.900848388671875\n",
      "Training Loss at step 42 : 13.316583633422852\n",
      "Training Loss at step 43 : 12.08132553100586\n",
      "Training Loss at step 44 : 12.562833786010742\n",
      "Training Loss at step 45 : 12.884620666503906\n",
      "Training Loss at step 46 : 12.047501564025879\n",
      "Training Loss at step 47 : 13.020547866821289\n",
      "Training Loss at step 48 : 14.249411582946777\n",
      "Training Loss at step 49 : 12.999187469482422\n",
      "Training Loss at step 50 : 13.081762313842773\n",
      "Test Loss 14.478474617004395 \n",
      "\n",
      "done with step 50 of 311\n",
      "current time: 0.3417027552922567 est epoch finish:  2.083716801880232\n",
      "Training Loss at step 51 : 13.666961669921875\n",
      "Training Loss at step 52 : 12.056639671325684\n",
      "Training Loss at step 53 : 12.831537246704102\n",
      "Training Loss at step 54 : 13.073078155517578\n",
      "Training Loss at step 55 : 12.824220657348633\n",
      "Training Loss at step 56 : 13.44743537902832\n",
      "Training Loss at step 57 : 13.726009368896484\n",
      "Training Loss at step 58 : 12.962661743164062\n",
      "Training Loss at step 59 : 13.041725158691406\n",
      "Training Loss at step 60 : 13.190958023071289\n",
      "Test Loss 14.535128593444824 \n",
      "\n",
      "done with step 60 of 311\n",
      "current time: 0.40677303075790405 est epoch finish:  2.073875615831281\n",
      "Training Loss at step 61 : 11.984989166259766\n",
      "Training Loss at step 62 : 13.43794059753418\n",
      "Training Loss at step 63 : 12.436687469482422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 64 : 12.83047103881836\n",
      "Training Loss at step 65 : 13.076187133789062\n",
      "Training Loss at step 66 : 13.783448219299316\n",
      "Training Loss at step 67 : 12.406609535217285\n",
      "Training Loss at step 68 : 14.17767333984375\n",
      "Training Loss at step 69 : 13.489090919494629\n",
      "Training Loss at step 70 : 12.72929859161377\n",
      "Test Loss 14.506133079528809 \n",
      "\n",
      "done with step 70 of 311\n",
      "current time: 0.4708092411359151 est epoch finish:  2.062277098496755\n",
      "Training Loss at step 71 : 13.181140899658203\n",
      "Training Loss at step 72 : 13.677448272705078\n",
      "Training Loss at step 73 : 13.361368179321289\n",
      "Training Loss at step 74 : 12.37997055053711\n",
      "Training Loss at step 75 : 13.447749137878418\n",
      "Training Loss at step 76 : 13.247318267822266\n",
      "Training Loss at step 77 : 11.632349967956543\n",
      "Training Loss at step 78 : 12.633420944213867\n",
      "Training Loss at step 79 : 13.67714786529541\n",
      "Training Loss at step 80 : 13.982742309570312\n",
      "Test Loss 14.514778137207031 \n",
      "\n",
      "done with step 80 of 311\n",
      "current time: 0.5366899212201436 est epoch finish:  2.06062426542549\n",
      "Training Loss at step 81 : 12.586811065673828\n",
      "Training Loss at step 82 : 12.732417106628418\n",
      "Training Loss at step 83 : 13.105405807495117\n",
      "Training Loss at step 84 : 12.723291397094727\n",
      "Training Loss at step 85 : 13.939140319824219\n",
      "Training Loss at step 86 : 13.10584831237793\n",
      "Training Loss at step 87 : 13.185924530029297\n",
      "Training Loss at step 88 : 12.812591552734375\n",
      "Training Loss at step 89 : 12.296611785888672\n",
      "Training Loss at step 90 : 13.20544147491455\n",
      "Test Loss 14.50090217590332 \n",
      "\n",
      "done with step 90 of 311\n",
      "current time: 0.6023193359375 est epoch finish:  2.0584759722699175\n",
      "Training Loss at step 91 : 12.860441207885742\n",
      "Training Loss at step 92 : 13.46688461303711\n",
      "Training Loss at step 93 : 13.237611770629883\n",
      "Training Loss at step 94 : 13.259073257446289\n",
      "Training Loss at step 95 : 12.628641128540039\n",
      "Training Loss at step 96 : 12.567105293273926\n",
      "Training Loss at step 97 : 12.810711860656738\n",
      "Training Loss at step 98 : 14.156646728515625\n",
      "Training Loss at step 99 : 11.52094841003418\n",
      "Training Loss at step 100 : 13.179651260375977\n",
      "Test Loss 14.473978996276855 \n",
      "\n",
      "done with step 100 of 311\n",
      "current time: 0.6671603918075562 est epoch finish:  2.054325562892574\n",
      "Training Loss at step 101 : 12.700582504272461\n",
      "Training Loss at step 102 : 12.90671443939209\n",
      "Training Loss at step 103 : 12.289321899414062\n",
      "Training Loss at step 104 : 12.57435417175293\n",
      "Training Loss at step 105 : 13.693901062011719\n",
      "Training Loss at step 106 : 12.636282920837402\n",
      "Training Loss at step 107 : 13.890780448913574\n",
      "Training Loss at step 108 : 13.285039901733398\n",
      "Training Loss at step 109 : 13.350475311279297\n",
      "Training Loss at step 110 : 12.443260192871094\n",
      "Test Loss 14.462539672851562 \n",
      "\n",
      "done with step 110 of 311\n",
      "current time: 0.7357233723004659 est epoch finish:  2.0613510701391435\n",
      "Training Loss at step 111 : 13.614617347717285\n",
      "Training Loss at step 112 : 13.257137298583984\n",
      "Training Loss at step 113 : 13.67288589477539\n",
      "Training Loss at step 114 : 13.08997631072998\n",
      "Training Loss at step 115 : 14.677434921264648\n",
      "Training Loss at step 116 : 12.19610595703125\n",
      "Training Loss at step 117 : 13.800615310668945\n",
      "Training Loss at step 118 : 12.320093154907227\n",
      "Training Loss at step 119 : 12.856416702270508\n",
      "Training Loss at step 120 : 12.313163757324219\n",
      "Test Loss 14.530631065368652 \n",
      "\n",
      "done with step 120 of 311\n",
      "current time: 0.8023742119471232 est epoch finish:  2.062300660459135\n",
      "Training Loss at step 121 : 11.882343292236328\n",
      "Training Loss at step 122 : 12.391036033630371\n",
      "Training Loss at step 123 : 12.169708251953125\n",
      "Training Loss at step 124 : 13.744012832641602\n",
      "Training Loss at step 125 : 12.733787536621094\n",
      "Training Loss at step 126 : 13.424360275268555\n",
      "Training Loss at step 127 : 13.540827751159668\n",
      "Training Loss at step 128 : 12.484743118286133\n",
      "Training Loss at step 129 : 12.581019401550293\n",
      "Training Loss at step 130 : 12.966109275817871\n",
      "Test Loss 14.48859977722168 \n",
      "\n",
      "done with step 130 of 311\n",
      "current time: 0.8709635655085246 est epoch finish:  2.067707395978253\n",
      "Training Loss at step 131 : 12.624427795410156\n",
      "Training Loss at step 132 : 13.835198402404785\n",
      "Training Loss at step 133 : 13.171671867370605\n",
      "Training Loss at step 134 : 12.507450103759766\n",
      "Training Loss at step 135 : 13.008193969726562\n",
      "Training Loss at step 136 : 12.586736679077148\n",
      "Training Loss at step 137 : 13.522979736328125\n",
      "Training Loss at step 138 : 14.461019515991211\n",
      "Training Loss at step 139 : 13.328792572021484\n",
      "Training Loss at step 140 : 12.219991683959961\n",
      "Test Loss 14.51123046875 \n",
      "\n",
      "done with step 140 of 311\n",
      "current time: 0.937994392712911 est epoch finish:  2.068909617969612\n",
      "Training Loss at step 141 : 11.981274604797363\n",
      "Training Loss at step 142 : 12.91859245300293\n",
      "Training Loss at step 143 : 12.887590408325195\n",
      "Training Loss at step 144 : 13.805038452148438\n",
      "Training Loss at step 145 : 12.36451530456543\n",
      "Training Loss at step 146 : 12.669567108154297\n",
      "Training Loss at step 147 : 11.61696720123291\n",
      "Training Loss at step 148 : 14.226367950439453\n",
      "Training Loss at step 149 : 13.872163772583008\n",
      "Training Loss at step 150 : 13.634624481201172\n",
      "Test Loss 14.498085021972656 \n",
      "\n",
      "done with step 150 of 311\n",
      "current time: 1.0053951660792033 est epoch finish:  2.07071454735518\n",
      "Training Loss at step 151 : 12.727359771728516\n",
      "Training Loss at step 152 : 14.052532196044922\n",
      "Training Loss at step 153 : 13.547744750976562\n",
      "Training Loss at step 154 : 11.352777481079102\n",
      "Training Loss at step 155 : 12.816168785095215\n",
      "Training Loss at step 156 : 13.355598449707031\n",
      "Training Loss at step 157 : 13.487857818603516\n",
      "Training Loss at step 158 : 11.738317489624023\n",
      "Training Loss at step 159 : 11.882043838500977\n",
      "Training Loss at step 160 : 13.091405868530273\n",
      "Test Loss 14.537696838378906 \n",
      "\n",
      "done with step 160 of 311\n",
      "current time: 1.0720369815826416 est epoch finish:  2.0708292004484568\n",
      "Training Loss at step 161 : 13.779816627502441\n",
      "Training Loss at step 162 : 13.66939640045166\n",
      "Training Loss at step 163 : 13.185741424560547\n",
      "Training Loss at step 164 : 13.321432113647461\n",
      "Training Loss at step 165 : 14.065409660339355\n",
      "Training Loss at step 166 : 13.161581039428711\n",
      "Training Loss at step 167 : 12.510480880737305\n",
      "Training Loss at step 168 : 11.201933860778809\n",
      "Training Loss at step 169 : 13.056012153625488\n",
      "Training Loss at step 170 : 12.14862060546875\n",
      "Test Loss 14.52981185913086 \n",
      "\n",
      "done with step 170 of 311\n",
      "current time: 1.138478930791219 est epoch finish:  2.0705669443044976\n",
      "Training Loss at step 171 : 14.303614616394043\n",
      "Training Loss at step 172 : 12.975431442260742\n",
      "Training Loss at step 173 : 13.400197982788086\n",
      "Training Loss at step 174 : 13.506592750549316\n",
      "Training Loss at step 175 : 12.572413444519043\n",
      "Training Loss at step 176 : 11.779280662536621\n",
      "Training Loss at step 177 : 12.622308731079102\n",
      "Training Loss at step 178 : 11.947515487670898\n",
      "Training Loss at step 179 : 12.673101425170898\n",
      "Training Loss at step 180 : 13.482280731201172\n",
      "Test Loss 14.490275382995605 \n",
      "\n",
      "done with step 180 of 311\n",
      "current time: 1.2050437887509664 est epoch finish:  2.0705448524947543\n",
      "Training Loss at step 181 : 13.802261352539062\n",
      "Training Loss at step 182 : 13.02379035949707\n",
      "Training Loss at step 183 : 13.201828002929688\n",
      "Training Loss at step 184 : 13.404727935791016\n",
      "Training Loss at step 185 : 12.257247924804688\n",
      "Training Loss at step 186 : 12.490206718444824\n",
      "Training Loss at step 187 : 11.95824909210205\n",
      "Training Loss at step 188 : 12.266542434692383\n",
      "Training Loss at step 189 : 12.313135147094727\n",
      "Training Loss at step 190 : 12.151607513427734\n",
      "Test Loss 14.503108024597168 \n",
      "\n",
      "done with step 190 of 311\n",
      "current time: 1.270335054397583 est epoch finish:  2.0684513189405673\n",
      "Training Loss at step 191 : 13.173612594604492\n",
      "Training Loss at step 192 : 11.870231628417969\n",
      "Training Loss at step 193 : 12.34511947631836\n",
      "Training Loss at step 194 : 14.736750602722168\n",
      "Training Loss at step 195 : 13.909195899963379\n",
      "Training Loss at step 196 : 12.816627502441406\n",
      "Training Loss at step 197 : 13.854863166809082\n",
      "Training Loss at step 198 : 13.892524719238281\n",
      "Training Loss at step 199 : 13.291635513305664\n",
      "Training Loss at step 200 : 12.748891830444336\n",
      "Test Loss 14.474098205566406 \n",
      "\n",
      "done with step 200 of 311\n",
      "current time: 1.3372846802075704 est epoch finish:  2.0691320176345993\n",
      "Training Loss at step 201 : 12.87759780883789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 202 : 14.572595596313477\n",
      "Training Loss at step 203 : 13.59028434753418\n",
      "Training Loss at step 204 : 13.331632614135742\n",
      "Training Loss at step 205 : 12.138372421264648\n",
      "Training Loss at step 206 : 14.118921279907227\n",
      "Training Loss at step 207 : 11.309013366699219\n",
      "Training Loss at step 208 : 12.205883026123047\n",
      "Training Loss at step 209 : 13.460623741149902\n",
      "Training Loss at step 210 : 12.777891159057617\n",
      "Test Loss 14.509445190429688 \n",
      "\n",
      "done with step 210 of 311\n",
      "current time: 1.402266263961792 est epoch finish:  2.066847431716196\n",
      "Training Loss at step 211 : 12.646352767944336\n",
      "Training Loss at step 212 : 12.905194282531738\n",
      "Training Loss at step 213 : 13.198047637939453\n",
      "Training Loss at step 214 : 11.913836479187012\n",
      "Training Loss at step 215 : 12.618432998657227\n",
      "Training Loss at step 216 : 12.674047470092773\n",
      "Training Loss at step 217 : 12.224212646484375\n",
      "Training Loss at step 218 : 13.84684944152832\n",
      "Training Loss at step 219 : 13.12644100189209\n",
      "Training Loss at step 220 : 12.264508247375488\n",
      "Test Loss 14.521675109863281 \n",
      "\n",
      "done with step 220 of 311\n",
      "current time: 1.4683602531750997 est epoch finish:  2.0663350169115655\n",
      "Training Loss at step 221 : 13.479974746704102\n",
      "Training Loss at step 222 : 11.90705680847168\n",
      "Training Loss at step 223 : 14.39582633972168\n",
      "Training Loss at step 224 : 13.489511489868164\n",
      "Training Loss at step 225 : 13.512920379638672\n",
      "Training Loss at step 226 : 13.80794620513916\n",
      "Training Loss at step 227 : 12.369821548461914\n",
      "Training Loss at step 228 : 13.392498016357422\n",
      "Training Loss at step 229 : 12.738870620727539\n",
      "Training Loss at step 230 : 12.111078262329102\n",
      "Test Loss 14.488901138305664 \n",
      "\n",
      "done with step 230 of 311\n",
      "current time: 1.5361856897672017 est epoch finish:  2.068198049859739\n",
      "Training Loss at step 231 : 13.916296005249023\n",
      "Training Loss at step 232 : 13.419817924499512\n",
      "Training Loss at step 233 : 11.913400650024414\n",
      "Training Loss at step 234 : 13.286584854125977\n",
      "Training Loss at step 235 : 12.648178100585938\n",
      "Training Loss at step 236 : 12.624110221862793\n",
      "Training Loss at step 237 : 13.060420036315918\n",
      "Training Loss at step 238 : 12.94625186920166\n",
      "Training Loss at step 239 : 13.110897064208984\n",
      "Training Loss at step 240 : 13.96334457397461\n",
      "Test Loss 14.474959373474121 \n",
      "\n",
      "done with step 240 of 311\n",
      "current time: 1.6026459813117981 est epoch finish:  2.0681448140579635\n",
      "Training Loss at step 241 : 13.281755447387695\n",
      "Training Loss at step 242 : 14.008957862854004\n",
      "Training Loss at step 243 : 12.927800178527832\n",
      "Training Loss at step 244 : 13.766493797302246\n",
      "Training Loss at step 245 : 14.480180740356445\n",
      "Training Loss at step 246 : 11.901522636413574\n",
      "Training Loss at step 247 : 12.380435943603516\n",
      "Training Loss at step 248 : 12.913741111755371\n",
      "Training Loss at step 249 : 12.633146286010742\n",
      "Training Loss at step 250 : 12.494699478149414\n",
      "Test Loss 14.44428539276123 \n",
      "\n",
      "done with step 250 of 311\n",
      "current time: 1.6688365697860719 est epoch finish:  2.0677616462289574\n",
      "Training Loss at step 251 : 11.936412811279297\n",
      "Training Loss at step 252 : 13.44715690612793\n",
      "Training Loss at step 253 : 12.768682479858398\n",
      "Training Loss at step 254 : 12.92943000793457\n",
      "Training Loss at step 255 : 12.487029075622559\n",
      "Training Loss at step 256 : 13.55700969696045\n",
      "Training Loss at step 257 : 13.041068077087402\n",
      "Training Loss at step 258 : 12.192049980163574\n",
      "Training Loss at step 259 : 13.476927757263184\n",
      "Training Loss at step 260 : 12.522134780883789\n",
      "Test Loss 14.4591064453125 \n",
      "\n",
      "done with step 260 of 311\n",
      "current time: 1.734968860944112 est epoch finish:  2.067338374534938\n",
      "Training Loss at step 261 : 13.756163597106934\n",
      "Training Loss at step 262 : 13.217326164245605\n",
      "Training Loss at step 263 : 13.913869857788086\n",
      "Training Loss at step 264 : 13.22208023071289\n",
      "Training Loss at step 265 : 14.05209732055664\n",
      "Training Loss at step 266 : 12.661600112915039\n",
      "Training Loss at step 267 : 12.854228973388672\n",
      "Training Loss at step 268 : 13.600784301757812\n",
      "Training Loss at step 269 : 13.200082778930664\n",
      "Training Loss at step 270 : 13.542985916137695\n",
      "Test Loss 14.449586868286133 \n",
      "\n",
      "done with step 270 of 311\n",
      "current time: 1.8009889960289 est epoch finish:  2.066817630129107\n",
      "Training Loss at step 271 : 12.767816543579102\n",
      "Training Loss at step 272 : 13.13192367553711\n",
      "Training Loss at step 273 : 13.265680313110352\n",
      "Training Loss at step 274 : 13.853404998779297\n",
      "Training Loss at step 275 : 13.313973426818848\n",
      "Training Loss at step 276 : 11.387468338012695\n",
      "Training Loss at step 277 : 12.336883544921875\n",
      "Training Loss at step 278 : 11.3633451461792\n",
      "Training Loss at step 279 : 12.443099975585938\n",
      "Training Loss at step 280 : 13.293813705444336\n",
      "Test Loss 14.471368789672852 \n",
      "\n",
      "done with step 280 of 311\n",
      "current time: 1.868179694811503 est epoch finish:  2.067629484293158\n",
      "Training Loss at step 281 : 13.10180377960205\n",
      "Training Loss at step 282 : 12.84945011138916\n",
      "Training Loss at step 283 : 13.022485733032227\n",
      "Training Loss at step 284 : 13.45666790008545\n",
      "Training Loss at step 285 : 12.978865623474121\n",
      "Training Loss at step 286 : 13.207448959350586\n",
      "Training Loss at step 287 : 13.046819686889648\n",
      "Training Loss at step 288 : 12.862642288208008\n",
      "Training Loss at step 289 : 14.26035213470459\n",
      "Training Loss at step 290 : 12.656596183776855\n",
      "Test Loss 14.446033477783203 \n",
      "\n",
      "done with step 290 of 311\n",
      "current time: 1.933470865090688 est epoch finish:  2.066355460629567\n",
      "Training Loss at step 291 : 12.473855972290039\n",
      "Training Loss at step 292 : 13.959218978881836\n",
      "Training Loss at step 293 : 12.499844551086426\n",
      "Training Loss at step 294 : 12.86534309387207\n",
      "Training Loss at step 295 : 13.549562454223633\n",
      "Training Loss at step 296 : 11.976883888244629\n",
      "Training Loss at step 297 : 11.944384574890137\n",
      "Training Loss at step 298 : 12.775602340698242\n",
      "Training Loss at step 299 : 12.883106231689453\n",
      "Training Loss at step 300 : 12.632739067077637\n",
      "Test Loss 14.428088188171387 \n",
      "\n",
      "done with step 300 of 311\n",
      "current time: 2.001241377989451 est epoch finish:  2.067727802507373\n",
      "Training Loss at step 301 : 13.567325592041016\n",
      "Training Loss at step 302 : 12.140922546386719\n",
      "Training Loss at step 303 : 12.586501121520996\n",
      "Training Loss at step 304 : 12.601249694824219\n",
      "Training Loss at step 305 : 13.029051780700684\n",
      "Training Loss at step 306 : 12.466408729553223\n",
      "Training Loss at step 307 : 11.29704475402832\n",
      "Training Loss at step 308 : 14.273801803588867\n",
      "Training Loss at step 309 : 12.828651428222656\n",
      "Training Loss at step 310 : 13.219985961914062\n",
      "Test Loss 14.478643417358398 \n",
      "\n",
      "done with step 310 of 311\n",
      "current time: 2.0668126424153646 est epoch finish:  2.066812642415365\n",
      "Starting training epoch 3\n",
      "constructing batches...\n",
      "batches constructed. starting training steps...\n",
      "Training Loss at step 0 : 13.08003044128418\n",
      "Test Loss 14.465005874633789 \n",
      "\n",
      "done with step 0 of 311\n",
      "current time: 0.011401009559631348 est epoch finish:  3.5457139730453493\n",
      "Training Loss at step 1 : 12.50188159942627\n",
      "Training Loss at step 2 : 12.591407775878906\n",
      "Training Loss at step 3 : 12.572275161743164\n",
      "Training Loss at step 4 : 12.691780090332031\n",
      "Training Loss at step 5 : 13.237951278686523\n",
      "Training Loss at step 6 : 13.267971992492676\n",
      "Training Loss at step 7 : 14.209981918334961\n",
      "Training Loss at step 8 : 13.63776683807373\n",
      "Training Loss at step 9 : 12.050628662109375\n",
      "Training Loss at step 10 : 12.020578384399414\n",
      "Test Loss 14.436464309692383 \n",
      "\n",
      "done with step 10 of 311\n",
      "current time: 0.0781139612197876 est epoch finish:  2.2084947217594495\n",
      "Training Loss at step 11 : 12.844022750854492\n",
      "Training Loss at step 12 : 12.939431190490723\n",
      "Training Loss at step 13 : 12.686355590820312\n",
      "Training Loss at step 14 : 13.46636962890625\n",
      "Training Loss at step 15 : 11.635019302368164\n",
      "Training Loss at step 16 : 13.068607330322266\n",
      "Training Loss at step 17 : 13.089162826538086\n",
      "Training Loss at step 18 : 12.577274322509766\n",
      "Training Loss at step 19 : 12.271953582763672\n",
      "Training Loss at step 20 : 12.316362380981445\n",
      "Test Loss 14.460153579711914 \n",
      "\n",
      "done with step 20 of 311\n",
      "current time: 0.14526466925938925 est epoch finish:  2.1513005780795265\n",
      "Training Loss at step 21 : 11.282543182373047\n",
      "Training Loss at step 22 : 11.69395637512207\n",
      "Training Loss at step 23 : 12.564146041870117\n",
      "Training Loss at step 24 : 12.161272048950195\n",
      "Training Loss at step 25 : 13.370068550109863\n",
      "Training Loss at step 26 : 13.381902694702148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 27 : 13.610698699951172\n",
      "Training Loss at step 28 : 12.461273193359375\n",
      "Training Loss at step 29 : 12.013086318969727\n",
      "Training Loss at step 30 : 13.557920455932617\n",
      "Test Loss 14.502076148986816 \n",
      "\n",
      "done with step 30 of 311\n",
      "current time: 0.21086597045262653 est epoch finish:  2.1154618326053822\n",
      "Training Loss at step 31 : 13.977310180664062\n",
      "Training Loss at step 32 : 12.708261489868164\n",
      "Training Loss at step 33 : 14.162622451782227\n",
      "Training Loss at step 34 : 12.614825248718262\n",
      "Training Loss at step 35 : 12.785511016845703\n",
      "Training Loss at step 36 : 12.876684188842773\n",
      "Training Loss at step 37 : 11.471185684204102\n",
      "Training Loss at step 38 : 12.360395431518555\n",
      "Training Loss at step 39 : 12.785368919372559\n",
      "Training Loss at step 40 : 12.071310997009277\n",
      "Test Loss 14.499242782592773 \n",
      "\n",
      "done with step 40 of 311\n",
      "current time: 0.2786492705345154 est epoch finish:  2.113656661859373\n",
      "Training Loss at step 41 : 12.071365356445312\n",
      "Training Loss at step 42 : 13.062594413757324\n",
      "Training Loss at step 43 : 12.99091625213623\n",
      "Training Loss at step 44 : 12.78885269165039\n",
      "Training Loss at step 45 : 12.010512351989746\n",
      "Training Loss at step 46 : 12.538265228271484\n",
      "Training Loss at step 47 : 12.742561340332031\n",
      "Training Loss at step 48 : 13.281759262084961\n",
      "Training Loss at step 49 : 11.487817764282227\n",
      "Training Loss at step 50 : 12.917163848876953\n",
      "Test Loss 14.45895767211914 \n",
      "\n",
      "done with step 50 of 311\n",
      "current time: 0.3437239209810893 est epoch finish:  2.0960419495121325\n",
      "Training Loss at step 51 : 12.870861053466797\n",
      "Training Loss at step 52 : 13.586318969726562\n",
      "Training Loss at step 53 : 13.07544994354248\n",
      "Training Loss at step 54 : 11.956153869628906\n",
      "Training Loss at step 55 : 11.521177291870117\n",
      "Training Loss at step 56 : 13.389945983886719\n",
      "Training Loss at step 57 : 12.654960632324219\n",
      "Training Loss at step 58 : 13.496201515197754\n",
      "Training Loss at step 59 : 12.75194263458252\n",
      "Training Loss at step 60 : 13.155641555786133\n",
      "Test Loss 14.526578903198242 \n",
      "\n",
      "done with step 60 of 311\n",
      "current time: 0.41007999976476034 est epoch finish:  2.0907357365055814\n",
      "Training Loss at step 61 : 13.2745361328125\n",
      "Training Loss at step 62 : 12.154934883117676\n",
      "Training Loss at step 63 : 12.306098937988281\n",
      "Training Loss at step 64 : 12.56430435180664\n",
      "Training Loss at step 65 : 13.867856979370117\n",
      "Training Loss at step 66 : 13.6128568649292\n",
      "Training Loss at step 67 : 11.724655151367188\n",
      "Training Loss at step 68 : 13.029375076293945\n",
      "Training Loss at step 69 : 12.829327583312988\n",
      "Training Loss at step 70 : 12.670137405395508\n",
      "Test Loss 14.461533546447754 \n",
      "\n",
      "done with step 70 of 311\n",
      "current time: 0.4761588056882223 est epoch finish:  2.0857096981554526\n",
      "Training Loss at step 71 : 12.412610054016113\n",
      "Training Loss at step 72 : 13.175222396850586\n",
      "Training Loss at step 73 : 13.532469749450684\n",
      "Training Loss at step 74 : 12.410104751586914\n",
      "Training Loss at step 75 : 13.288333892822266\n",
      "Training Loss at step 76 : 12.873468399047852\n",
      "Training Loss at step 77 : 13.381022453308105\n",
      "Training Loss at step 78 : 12.496482849121094\n",
      "Training Loss at step 79 : 14.268926620483398\n",
      "Training Loss at step 80 : 13.455866813659668\n",
      "Test Loss 14.47269058227539 \n",
      "\n",
      "done with step 80 of 311\n",
      "current time: 0.5414851466814677 est epoch finish:  2.0790355631844006\n",
      "Training Loss at step 81 : 13.30364990234375\n",
      "Training Loss at step 82 : 12.497013092041016\n",
      "Training Loss at step 83 : 12.120950698852539\n",
      "Training Loss at step 84 : 12.215288162231445\n",
      "Training Loss at step 85 : 12.256811141967773\n",
      "Training Loss at step 86 : 13.418594360351562\n",
      "Training Loss at step 87 : 12.265668869018555\n",
      "Training Loss at step 88 : 11.877197265625\n",
      "Training Loss at step 89 : 11.873037338256836\n",
      "Training Loss at step 90 : 10.921154022216797\n",
      "Test Loss 14.464252471923828 \n",
      "\n",
      "done with step 90 of 311\n",
      "current time: 0.608131738503774 est epoch finish:  2.0783403370843265\n",
      "Training Loss at step 91 : 12.860271453857422\n",
      "Training Loss at step 92 : 12.084745407104492\n",
      "Training Loss at step 93 : 12.009441375732422\n",
      "Training Loss at step 94 : 13.788570404052734\n",
      "Training Loss at step 95 : 12.441652297973633\n",
      "Training Loss at step 96 : 12.687919616699219\n",
      "Training Loss at step 97 : 11.614795684814453\n",
      "Training Loss at step 98 : 12.013776779174805\n",
      "Training Loss at step 99 : 12.653742790222168\n",
      "Training Loss at step 100 : 11.972899436950684\n",
      "Test Loss 14.453540802001953 \n",
      "\n",
      "done with step 100 of 311\n",
      "current time: 0.6733132123947143 est epoch finish:  2.073271376779764\n",
      "Training Loss at step 101 : 12.9768648147583\n",
      "Training Loss at step 102 : 13.889416694641113\n",
      "Training Loss at step 103 : 10.616894721984863\n",
      "Training Loss at step 104 : 12.684965133666992\n",
      "Training Loss at step 105 : 12.308591842651367\n",
      "Training Loss at step 106 : 12.046960830688477\n",
      "Training Loss at step 107 : 11.085956573486328\n",
      "Training Loss at step 108 : 13.794239044189453\n",
      "Training Loss at step 109 : 11.932050704956055\n",
      "Training Loss at step 110 : 13.786943435668945\n",
      "Test Loss 14.468645095825195 \n",
      "\n",
      "done with step 110 of 311\n",
      "current time: 0.7378750920295716 est epoch finish:  2.067379762353124\n",
      "Training Loss at step 111 : 11.98777961730957\n",
      "Training Loss at step 112 : 12.680574417114258\n",
      "Training Loss at step 113 : 12.858294486999512\n",
      "Training Loss at step 114 : 13.497530937194824\n",
      "Training Loss at step 115 : 13.497047424316406\n",
      "Training Loss at step 116 : 12.68216323852539\n",
      "Training Loss at step 117 : 12.937707901000977\n",
      "Training Loss at step 118 : 13.202741622924805\n",
      "Training Loss at step 119 : 14.279228210449219\n",
      "Training Loss at step 120 : 12.526102066040039\n",
      "Test Loss 14.448583602905273 \n",
      "\n",
      "done with step 120 of 311\n",
      "current time: 0.8032740036646525 est epoch finish:  2.064613348262041\n",
      "Training Loss at step 121 : 11.66859245300293\n",
      "Training Loss at step 122 : 13.062503814697266\n",
      "Training Loss at step 123 : 11.802247047424316\n",
      "Training Loss at step 124 : 10.569841384887695\n",
      "Training Loss at step 125 : 13.765405654907227\n",
      "Training Loss at step 126 : 13.844746589660645\n",
      "Training Loss at step 127 : 13.939006805419922\n",
      "Training Loss at step 128 : 12.894966125488281\n",
      "Training Loss at step 129 : 11.076010704040527\n",
      "Training Loss at step 130 : 12.958252906799316\n",
      "Test Loss 14.447711944580078 \n",
      "\n",
      "done with step 130 of 311\n",
      "current time: 0.8711830894152324 est epoch finish:  2.0682285557873072\n",
      "Training Loss at step 131 : 12.732370376586914\n",
      "Training Loss at step 132 : 14.238515853881836\n",
      "Training Loss at step 133 : 12.658427238464355\n",
      "Training Loss at step 134 : 14.31893539428711\n",
      "Training Loss at step 135 : 14.962932586669922\n",
      "Training Loss at step 136 : 13.133419036865234\n",
      "Training Loss at step 137 : 13.228656768798828\n",
      "Training Loss at step 138 : 12.329814910888672\n",
      "Training Loss at step 139 : 13.289042472839355\n",
      "Training Loss at step 140 : 12.576847076416016\n",
      "Test Loss 14.424978256225586 \n",
      "\n",
      "done with step 140 of 311\n",
      "current time: 0.9368126670519511 est epoch finish:  2.0663031166890553\n",
      "Training Loss at step 141 : 13.439656257629395\n",
      "Training Loss at step 142 : 14.004651069641113\n",
      "Training Loss at step 143 : 12.587620735168457\n",
      "Training Loss at step 144 : 13.035968780517578\n",
      "Training Loss at step 145 : 11.997642517089844\n",
      "Training Loss at step 146 : 13.20313835144043\n",
      "Training Loss at step 147 : 11.950996398925781\n",
      "Training Loss at step 148 : 13.399690628051758\n",
      "Training Loss at step 149 : 11.424243927001953\n",
      "Training Loss at step 150 : 11.318460464477539\n",
      "Test Loss 14.479524612426758 \n",
      "\n",
      "done with step 150 of 311\n",
      "current time: 1.0032630840937296 est epoch finish:  2.0663233056499997\n",
      "Training Loss at step 151 : 12.83885383605957\n",
      "Training Loss at step 152 : 11.398515701293945\n",
      "Training Loss at step 153 : 14.05478286743164\n",
      "Training Loss at step 154 : 12.127813339233398\n",
      "Training Loss at step 155 : 11.587332725524902\n",
      "Training Loss at step 156 : 12.483756065368652\n",
      "Training Loss at step 157 : 13.042646408081055\n",
      "Training Loss at step 158 : 12.74151611328125\n",
      "Training Loss at step 159 : 12.490591049194336\n",
      "Training Loss at step 160 : 13.641300201416016\n",
      "Test Loss 14.489273071289062 \n",
      "\n",
      "done with step 160 of 311\n",
      "current time: 1.0702117045720418 est epoch finish:  2.0673033547944413\n",
      "Training Loss at step 161 : 11.899895668029785\n",
      "Training Loss at step 162 : 13.911108016967773\n",
      "Training Loss at step 163 : 13.313553810119629\n",
      "Training Loss at step 164 : 11.747770309448242\n",
      "Training Loss at step 165 : 13.617124557495117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 166 : 12.16242504119873\n",
      "Training Loss at step 167 : 14.092852592468262\n",
      "Training Loss at step 168 : 13.58592414855957\n",
      "Training Loss at step 169 : 12.82476806640625\n",
      "Training Loss at step 170 : 11.981552124023438\n",
      "Test Loss 14.422200202941895 \n",
      "\n",
      "done with step 170 of 311\n",
      "current time: 1.1334516684214273 est epoch finish:  2.061423794614409\n",
      "Training Loss at step 171 : 12.989645004272461\n",
      "Training Loss at step 172 : 14.505607604980469\n",
      "Training Loss at step 173 : 12.13363265991211\n",
      "Training Loss at step 174 : 14.361852645874023\n",
      "Training Loss at step 175 : 10.738110542297363\n",
      "Training Loss at step 176 : 12.778749465942383\n",
      "Training Loss at step 177 : 13.304718971252441\n",
      "Training Loss at step 178 : 12.961150169372559\n",
      "Training Loss at step 179 : 14.002437591552734\n",
      "Training Loss at step 180 : 13.125179290771484\n",
      "Test Loss 14.40896987915039 \n",
      "\n",
      "done with step 180 of 311\n",
      "current time: 1.1996949752171835 est epoch finish:  2.061354349682564\n",
      "Training Loss at step 181 : 12.590780258178711\n",
      "Training Loss at step 182 : 14.12669563293457\n",
      "Training Loss at step 183 : 15.474027633666992\n",
      "Training Loss at step 184 : 11.4964599609375\n",
      "Training Loss at step 185 : 12.092818260192871\n",
      "Training Loss at step 186 : 12.173954963684082\n",
      "Training Loss at step 187 : 13.603862762451172\n",
      "Training Loss at step 188 : 13.146240234375\n",
      "Training Loss at step 189 : 13.420600891113281\n",
      "Training Loss at step 190 : 14.055816650390625\n",
      "Test Loss 14.430557250976562 \n",
      "\n",
      "done with step 190 of 311\n",
      "current time: 1.2645644068717956 est epoch finish:  2.0590551337022434\n",
      "Training Loss at step 191 : 13.497072219848633\n",
      "Training Loss at step 192 : 12.011333465576172\n",
      "Training Loss at step 193 : 13.860762596130371\n",
      "Training Loss at step 194 : 13.172711372375488\n",
      "Training Loss at step 195 : 12.303898811340332\n",
      "Training Loss at step 196 : 13.763280868530273\n",
      "Training Loss at step 197 : 11.983732223510742\n",
      "Training Loss at step 198 : 13.008623123168945\n",
      "Training Loss at step 199 : 12.909029006958008\n",
      "Training Loss at step 200 : 12.356460571289062\n",
      "Test Loss 14.383979797363281 \n",
      "\n",
      "done with step 200 of 311\n",
      "current time: 1.3303165435791016 est epoch finish:  2.058350472900998\n",
      "Training Loss at step 201 : 12.06207275390625\n",
      "Training Loss at step 202 : 12.52910041809082\n",
      "Training Loss at step 203 : 14.003273010253906\n",
      "Training Loss at step 204 : 12.287144660949707\n",
      "Training Loss at step 205 : 10.918322563171387\n",
      "Training Loss at step 206 : 13.346517562866211\n",
      "Training Loss at step 207 : 11.038739204406738\n",
      "Training Loss at step 208 : 11.88656997680664\n",
      "Training Loss at step 209 : 12.10104751586914\n",
      "Training Loss at step 210 : 11.469520568847656\n",
      "Test Loss 14.420355796813965 \n",
      "\n",
      "done with step 210 of 311\n",
      "current time: 1.3950202345848084 est epoch finish:  2.0561672651937224\n",
      "Training Loss at step 211 : 12.934450149536133\n",
      "Training Loss at step 212 : 12.917600631713867\n",
      "Training Loss at step 213 : 10.970573425292969\n",
      "Training Loss at step 214 : 12.447872161865234\n",
      "Training Loss at step 215 : 10.414779663085938\n",
      "Training Loss at step 216 : 13.332828521728516\n",
      "Training Loss at step 217 : 10.857990264892578\n",
      "Training Loss at step 218 : 13.816680908203125\n",
      "Training Loss at step 219 : 12.933256149291992\n",
      "Training Loss at step 220 : 12.988689422607422\n",
      "Test Loss 14.405158042907715 \n",
      "\n",
      "done with step 220 of 311\n",
      "current time: 1.4589356660842896 est epoch finish:  2.0530723626797016\n",
      "Training Loss at step 221 : 13.054598808288574\n",
      "Training Loss at step 222 : 12.890959739685059\n",
      "Training Loss at step 223 : 14.073914527893066\n",
      "Training Loss at step 224 : 12.940437316894531\n",
      "Training Loss at step 225 : 13.713794708251953\n",
      "Training Loss at step 226 : 11.68210220336914\n",
      "Training Loss at step 227 : 11.130441665649414\n",
      "Training Loss at step 228 : 12.589189529418945\n",
      "Training Loss at step 229 : 14.19549560546875\n",
      "Training Loss at step 230 : 13.248258590698242\n",
      "Test Loss 14.42866325378418 \n",
      "\n",
      "done with step 230 of 311\n",
      "current time: 1.5228604833285013 est epoch finish:  2.050258053312398\n",
      "Training Loss at step 231 : 13.284847259521484\n",
      "Training Loss at step 232 : 13.571351051330566\n",
      "Training Loss at step 233 : 12.375916481018066\n",
      "Training Loss at step 234 : 11.931207656860352\n",
      "Training Loss at step 235 : 11.082904815673828\n",
      "Training Loss at step 236 : 11.509576797485352\n",
      "Training Loss at step 237 : 11.529706954956055\n",
      "Training Loss at step 238 : 12.728567123413086\n",
      "Training Loss at step 239 : 13.334632873535156\n",
      "Training Loss at step 240 : 12.934600830078125\n",
      "Test Loss 14.394815444946289 \n",
      "\n",
      "done with step 240 of 311\n",
      "current time: 1.5877092599868774 est epoch finish:  2.0488696259581696\n",
      "Training Loss at step 241 : 10.567524909973145\n",
      "Training Loss at step 242 : 13.744375228881836\n",
      "Training Loss at step 243 : 13.177703857421875\n",
      "Training Loss at step 244 : 13.12232780456543\n",
      "Training Loss at step 245 : 13.309597969055176\n",
      "Training Loss at step 246 : 13.103475570678711\n",
      "Training Loss at step 247 : 12.68966007232666\n",
      "Training Loss at step 248 : 14.061980247497559\n",
      "Training Loss at step 249 : 14.11307144165039\n",
      "Training Loss at step 250 : 12.189764976501465\n",
      "Test Loss 14.449209213256836 \n",
      "\n",
      "done with step 250 of 311\n",
      "current time: 1.6532192627588909 est epoch finish:  2.0484111183984663\n",
      "Training Loss at step 251 : 11.376760482788086\n",
      "Training Loss at step 252 : 12.11559009552002\n",
      "Training Loss at step 253 : 11.49222183227539\n",
      "Training Loss at step 254 : 12.541402816772461\n",
      "Training Loss at step 255 : 13.35396957397461\n",
      "Training Loss at step 256 : 13.112688064575195\n",
      "Training Loss at step 257 : 12.12819766998291\n",
      "Training Loss at step 258 : 13.420255661010742\n",
      "Training Loss at step 259 : 12.688393592834473\n",
      "Training Loss at step 260 : 12.858013153076172\n",
      "Test Loss 14.39676284790039 \n",
      "\n",
      "done with step 260 of 311\n",
      "current time: 1.7175684094429016 est epoch finish:  2.0466045032059097\n",
      "Training Loss at step 261 : 13.157386779785156\n",
      "Training Loss at step 262 : 12.151859283447266\n",
      "Training Loss at step 263 : 11.951714515686035\n",
      "Training Loss at step 264 : 12.9569730758667\n",
      "Training Loss at step 265 : 14.100312232971191\n",
      "Training Loss at step 266 : 12.976975440979004\n",
      "Training Loss at step 267 : 12.308444023132324\n",
      "Training Loss at step 268 : 12.327239990234375\n",
      "Training Loss at step 269 : 12.485258102416992\n",
      "Training Loss at step 270 : 12.864317893981934\n",
      "Test Loss 14.398781776428223 \n",
      "\n",
      "done with step 270 of 311\n",
      "current time: 1.7822535316149393 est epoch finish:  2.0453167835138237\n",
      "Training Loss at step 271 : 12.44624137878418\n",
      "Training Loss at step 272 : 13.238525390625\n",
      "Training Loss at step 273 : 13.343220710754395\n",
      "Training Loss at step 274 : 11.96383285522461\n",
      "Training Loss at step 275 : 12.79014778137207\n",
      "Training Loss at step 276 : 12.796369552612305\n",
      "Training Loss at step 277 : 12.858867645263672\n",
      "Training Loss at step 278 : 13.078250885009766\n",
      "Training Loss at step 279 : 13.941413879394531\n",
      "Training Loss at step 280 : 13.66360855102539\n",
      "Test Loss 14.374889373779297 \n",
      "\n",
      "done with step 280 of 311\n",
      "current time: 1.8488521258036295 est epoch finish:  2.0462384737541948\n",
      "Training Loss at step 281 : 13.351078033447266\n",
      "Training Loss at step 282 : 12.093637466430664\n",
      "Training Loss at step 283 : 12.468221664428711\n",
      "Training Loss at step 284 : 12.097784042358398\n",
      "Training Loss at step 285 : 13.674062728881836\n",
      "Training Loss at step 286 : 13.835245132446289\n",
      "Training Loss at step 287 : 12.385807037353516\n",
      "Training Loss at step 288 : 11.473154067993164\n",
      "Training Loss at step 289 : 14.489351272583008\n",
      "Training Loss at step 290 : 14.363029479980469\n",
      "Test Loss 14.369760513305664 \n",
      "\n",
      "done with step 290 of 311\n",
      "current time: 1.9142910877863566 est epoch finish:  2.0458574855723604\n",
      "Training Loss at step 291 : 12.911054611206055\n",
      "Training Loss at step 292 : 13.493559837341309\n",
      "Training Loss at step 293 : 11.709098815917969\n",
      "Training Loss at step 294 : 11.232004165649414\n",
      "Training Loss at step 295 : 13.136825561523438\n",
      "Training Loss at step 296 : 12.065570831298828\n",
      "Training Loss at step 297 : 14.565362930297852\n",
      "Training Loss at step 298 : 12.776924133300781\n",
      "Training Loss at step 299 : 13.529187202453613\n",
      "Training Loss at step 300 : 11.782890319824219\n",
      "Test Loss 14.393714904785156 \n",
      "\n",
      "done with step 300 of 311\n",
      "current time: 1.9799258788426717 est epoch finish:  2.0457041472427604\n",
      "Training Loss at step 301 : 12.361774444580078\n",
      "Training Loss at step 302 : 10.432332992553711\n",
      "Training Loss at step 303 : 12.28404426574707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 304 : 11.517258644104004\n",
      "Training Loss at step 305 : 10.691617965698242\n",
      "Training Loss at step 306 : 13.69145679473877\n",
      "Training Loss at step 307 : 12.121540069580078\n",
      "Training Loss at step 308 : 12.093564987182617\n",
      "Training Loss at step 309 : 12.956635475158691\n",
      "Training Loss at step 310 : 12.824211120605469\n",
      "Test Loss 14.327960968017578 \n",
      "\n",
      "done with step 310 of 311\n",
      "current time: 2.0454909642537435 est epoch finish:  2.0454909642537435\n",
      "Starting training epoch 4\n",
      "constructing batches...\n",
      "batches constructed. starting training steps...\n",
      "Training Loss at step 0 : 13.52159309387207\n",
      "Test Loss 14.407410621643066 \n",
      "\n",
      "done with step 0 of 311\n",
      "current time: 0.011130237579345703 est epoch finish:  3.4615038871765136\n",
      "Training Loss at step 1 : 13.75076675415039\n",
      "Training Loss at step 2 : 12.319293975830078\n",
      "Training Loss at step 3 : 12.660737991333008\n",
      "Training Loss at step 4 : 12.147886276245117\n",
      "Training Loss at step 5 : 13.71452808380127\n",
      "Training Loss at step 6 : 12.600147247314453\n",
      "Training Loss at step 7 : 12.996814727783203\n",
      "Training Loss at step 8 : 13.410226821899414\n",
      "Training Loss at step 9 : 13.893567085266113\n",
      "Training Loss at step 10 : 12.85791015625\n",
      "Test Loss 14.369403839111328 \n",
      "\n",
      "done with step 10 of 311\n",
      "current time: 0.0773304263750712 est epoch finish:  2.1863420547861043\n",
      "Training Loss at step 11 : 13.027790069580078\n",
      "Training Loss at step 12 : 12.667360305786133\n",
      "Training Loss at step 13 : 12.22793960571289\n",
      "Training Loss at step 14 : 13.791459083557129\n",
      "Training Loss at step 15 : 13.84582233428955\n",
      "Training Loss at step 16 : 13.32105827331543\n",
      "Training Loss at step 17 : 11.589689254760742\n",
      "Training Loss at step 18 : 13.002367973327637\n",
      "Training Loss at step 19 : 13.182355880737305\n",
      "Training Loss at step 20 : 11.808274269104004\n",
      "Test Loss 14.416481971740723 \n",
      "\n",
      "done with step 20 of 311\n",
      "current time: 0.14370519320170086 est epoch finish:  2.1282054802728076\n",
      "Training Loss at step 21 : 12.075674057006836\n",
      "Training Loss at step 22 : 12.228272438049316\n",
      "Training Loss at step 23 : 12.026103973388672\n",
      "Training Loss at step 24 : 11.29947280883789\n",
      "Training Loss at step 25 : 14.110673904418945\n",
      "Training Loss at step 26 : 13.289414405822754\n",
      "Training Loss at step 27 : 13.660171508789062\n",
      "Training Loss at step 28 : 14.2598876953125\n",
      "Training Loss at step 29 : 13.150096893310547\n",
      "Training Loss at step 30 : 12.879823684692383\n",
      "Test Loss 14.389347076416016 \n",
      "\n",
      "done with step 30 of 311\n",
      "current time: 0.20823139746983846 est epoch finish:  2.0890311165522504\n",
      "Training Loss at step 31 : 12.19289493560791\n",
      "Training Loss at step 32 : 12.743003845214844\n",
      "Training Loss at step 33 : 12.743271827697754\n",
      "Training Loss at step 34 : 14.080413818359375\n",
      "Training Loss at step 35 : 13.018827438354492\n",
      "Training Loss at step 36 : 13.432682037353516\n",
      "Training Loss at step 37 : 13.860410690307617\n",
      "Training Loss at step 38 : 13.218612670898438\n",
      "Training Loss at step 39 : 12.746034622192383\n",
      "Training Loss at step 40 : 11.907979965209961\n",
      "Test Loss 14.327692031860352 \n",
      "\n",
      "done with step 40 of 311\n",
      "current time: 0.27558535734812417 est epoch finish:  2.090415759396747\n",
      "Training Loss at step 41 : 13.475296020507812\n",
      "Training Loss at step 42 : 13.713035583496094\n",
      "Training Loss at step 43 : 12.404424667358398\n",
      "Training Loss at step 44 : 12.057992935180664\n",
      "Training Loss at step 45 : 11.633418083190918\n",
      "Training Loss at step 46 : 13.240194320678711\n",
      "Training Loss at step 47 : 11.89706802368164\n",
      "Training Loss at step 48 : 12.633870124816895\n",
      "Training Loss at step 49 : 12.74526596069336\n",
      "Training Loss at step 50 : 11.526262283325195\n",
      "Test Loss 14.382057189941406 \n",
      "\n",
      "done with step 50 of 311\n",
      "current time: 0.3396533171335856 est epoch finish:  2.071219247618532\n",
      "Training Loss at step 51 : 12.151826858520508\n",
      "Training Loss at step 52 : 12.617149353027344\n",
      "Training Loss at step 53 : 12.202974319458008\n",
      "Training Loss at step 54 : 12.204296112060547\n",
      "Training Loss at step 55 : 12.77362060546875\n",
      "Training Loss at step 56 : 11.78727912902832\n",
      "Training Loss at step 57 : 11.595131874084473\n",
      "Training Loss at step 58 : 12.172517776489258\n",
      "Training Loss at step 59 : 11.286471366882324\n",
      "Training Loss at step 60 : 12.710638046264648\n",
      "Test Loss 14.333353042602539 \n",
      "\n",
      "done with step 60 of 311\n",
      "current time: 0.40444477399190265 est epoch finish:  2.062005323139045\n",
      "Training Loss at step 61 : 12.244635581970215\n",
      "Training Loss at step 62 : 13.093883514404297\n",
      "Training Loss at step 63 : 13.146239280700684\n",
      "Training Loss at step 64 : 12.467594146728516\n",
      "Training Loss at step 65 : 12.594650268554688\n",
      "Training Loss at step 66 : 13.961809158325195\n",
      "Training Loss at step 67 : 11.826265335083008\n",
      "Training Loss at step 68 : 12.895486831665039\n",
      "Training Loss at step 69 : 11.98775863647461\n",
      "Training Loss at step 70 : 12.236525535583496\n",
      "Test Loss 14.41589069366455 \n",
      "\n",
      "done with step 70 of 311\n",
      "current time: 0.4706831415494283 est epoch finish:  2.061724746786932\n",
      "Training Loss at step 71 : 12.126174926757812\n",
      "Training Loss at step 72 : 12.201374053955078\n",
      "Training Loss at step 73 : 12.340034484863281\n",
      "Training Loss at step 74 : 12.579766273498535\n",
      "Training Loss at step 75 : 13.645980834960938\n",
      "Training Loss at step 76 : 10.951642036437988\n",
      "Training Loss at step 77 : 11.585372924804688\n",
      "Training Loss at step 78 : 12.79848861694336\n",
      "Training Loss at step 79 : 12.318204879760742\n",
      "Training Loss at step 80 : 13.254457473754883\n",
      "Test Loss 14.419233322143555 \n",
      "\n",
      "done with step 80 of 311\n",
      "current time: 0.534006679058075 est epoch finish:  2.050321940581004\n",
      "Training Loss at step 81 : 11.548707962036133\n",
      "Training Loss at step 82 : 13.380622863769531\n",
      "Training Loss at step 83 : 12.568058013916016\n",
      "Training Loss at step 84 : 13.600378036499023\n",
      "Training Loss at step 85 : 13.218695640563965\n",
      "Training Loss at step 86 : 13.56174373626709\n",
      "Training Loss at step 87 : 11.507770538330078\n",
      "Training Loss at step 88 : 13.103038787841797\n",
      "Training Loss at step 89 : 12.30812931060791\n",
      "Training Loss at step 90 : 14.208639144897461\n",
      "Test Loss 14.414782524108887 \n",
      "\n",
      "done with step 90 of 311\n",
      "current time: 0.5981515288352967 est epoch finish:  2.0442321479975525\n",
      "Training Loss at step 91 : 11.977069854736328\n",
      "Training Loss at step 92 : 12.485688209533691\n",
      "Training Loss at step 93 : 12.73779296875\n",
      "Training Loss at step 94 : 11.782665252685547\n",
      "Training Loss at step 95 : 13.329813003540039\n",
      "Training Loss at step 96 : 12.246391296386719\n",
      "Training Loss at step 97 : 13.342345237731934\n",
      "Training Loss at step 98 : 13.26276969909668\n",
      "Training Loss at step 99 : 12.386661529541016\n",
      "Training Loss at step 100 : 11.328192710876465\n",
      "Test Loss 14.354106903076172 \n",
      "\n",
      "done with step 100 of 311\n",
      "current time: 0.662999176979065 est epoch finish:  2.0415123172325664\n",
      "Training Loss at step 101 : 12.454900741577148\n",
      "Training Loss at step 102 : 11.055370330810547\n",
      "Training Loss at step 103 : 11.91330337524414\n",
      "Training Loss at step 104 : 12.626880645751953\n",
      "Training Loss at step 105 : 12.644571304321289\n",
      "Training Loss at step 106 : 12.568743705749512\n",
      "Training Loss at step 107 : 11.92660903930664\n",
      "Training Loss at step 108 : 11.889313697814941\n",
      "Training Loss at step 109 : 11.500760078430176\n",
      "Training Loss at step 110 : 13.161497116088867\n",
      "Test Loss 14.407183647155762 \n",
      "\n",
      "done with step 110 of 311\n",
      "current time: 0.7272465268770854 est epoch finish:  2.0376006293583204\n",
      "Training Loss at step 111 : 13.2203369140625\n",
      "Training Loss at step 112 : 12.751611709594727\n",
      "Training Loss at step 113 : 10.886992454528809\n",
      "Training Loss at step 114 : 12.218758583068848\n",
      "Training Loss at step 115 : 12.232145309448242\n",
      "Training Loss at step 116 : 11.190153121948242\n",
      "Training Loss at step 117 : 13.17885971069336\n",
      "Training Loss at step 118 : 13.036012649536133\n",
      "Training Loss at step 119 : 13.92581844329834\n",
      "Training Loss at step 120 : 12.615409851074219\n",
      "Test Loss 14.373486518859863 \n",
      "\n",
      "done with step 120 of 311\n",
      "current time: 0.7921425580978394 est epoch finish:  2.0360027732927937\n",
      "Training Loss at step 121 : 12.526247024536133\n",
      "Training Loss at step 122 : 13.681798934936523\n",
      "Training Loss at step 123 : 12.38718032836914\n",
      "Training Loss at step 124 : 13.393667221069336\n",
      "Training Loss at step 125 : 12.491687774658203\n",
      "Training Loss at step 126 : 14.111764907836914\n",
      "Training Loss at step 127 : 12.519190788269043\n",
      "Training Loss at step 128 : 10.727397918701172\n",
      "Training Loss at step 129 : 11.446996688842773\n",
      "Training Loss at step 130 : 12.071773529052734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss 14.331729888916016 \n",
      "\n",
      "done with step 130 of 311\n",
      "current time: 0.8612980763117473 est epoch finish:  2.044761081930942\n",
      "Training Loss at step 131 : 10.322671890258789\n",
      "Training Loss at step 132 : 13.11650276184082\n",
      "Training Loss at step 133 : 12.573434829711914\n",
      "Training Loss at step 134 : 11.749130249023438\n",
      "Training Loss at step 135 : 12.863698959350586\n",
      "Training Loss at step 136 : 11.460198402404785\n",
      "Training Loss at step 137 : 11.24962043762207\n",
      "Training Loss at step 138 : 12.231934547424316\n",
      "Training Loss at step 139 : 13.144277572631836\n",
      "Training Loss at step 140 : 13.696168899536133\n",
      "Test Loss 14.340024948120117 \n",
      "\n",
      "done with step 140 of 311\n",
      "current time: 0.9278130610783895 est epoch finish:  2.0464529219530436\n",
      "Training Loss at step 141 : 12.825197219848633\n",
      "Training Loss at step 142 : 11.013589859008789\n",
      "Training Loss at step 143 : 12.330129623413086\n",
      "Training Loss at step 144 : 13.558076858520508\n",
      "Training Loss at step 145 : 12.461968421936035\n",
      "Training Loss at step 146 : 10.48080062866211\n",
      "Training Loss at step 147 : 13.425790786743164\n",
      "Training Loss at step 148 : 12.626931190490723\n",
      "Training Loss at step 149 : 13.570012092590332\n",
      "Training Loss at step 150 : 12.966047286987305\n",
      "Test Loss 14.364401817321777 \n",
      "\n",
      "done with step 150 of 311\n",
      "current time: 0.9956055045127868 est epoch finish:  2.0505517344601105\n",
      "Training Loss at step 151 : 11.833404541015625\n",
      "Training Loss at step 152 : 12.452970504760742\n",
      "Training Loss at step 153 : 14.67585277557373\n",
      "Training Loss at step 154 : 10.529853820800781\n",
      "Training Loss at step 155 : 12.389019966125488\n",
      "Training Loss at step 156 : 12.944726943969727\n",
      "Training Loss at step 157 : 12.419169425964355\n",
      "Training Loss at step 158 : 11.301664352416992\n",
      "Training Loss at step 159 : 12.187848091125488\n",
      "Training Loss at step 160 : 12.18631362915039\n",
      "Test Loss 14.399019241333008 \n",
      "\n",
      "done with step 160 of 311\n",
      "current time: 1.0644339084625245 est epoch finish:  2.0561425188313356\n",
      "Training Loss at step 161 : 12.119413375854492\n",
      "Training Loss at step 162 : 13.476095199584961\n",
      "Training Loss at step 163 : 11.993803024291992\n",
      "Training Loss at step 164 : 11.121870994567871\n",
      "Training Loss at step 165 : 12.237503051757812\n",
      "Training Loss at step 166 : 12.762928009033203\n",
      "Training Loss at step 167 : 12.207746505737305\n",
      "Training Loss at step 168 : 13.77831745147705\n",
      "Training Loss at step 169 : 11.687261581420898\n",
      "Training Loss at step 170 : 12.249006271362305\n",
      "Test Loss 14.369266510009766 \n",
      "\n",
      "done with step 170 of 311\n",
      "current time: 1.1322215358416239 est epoch finish:  2.0591865359458774\n",
      "Training Loss at step 171 : 11.065080642700195\n",
      "Training Loss at step 172 : 11.439553260803223\n",
      "Training Loss at step 173 : 12.604265213012695\n",
      "Training Loss at step 174 : 14.290288925170898\n",
      "Training Loss at step 175 : 12.067854881286621\n",
      "Training Loss at step 176 : 12.025924682617188\n",
      "Training Loss at step 177 : 11.182294845581055\n",
      "Training Loss at step 178 : 14.188308715820312\n",
      "Training Loss at step 179 : 14.039959907531738\n",
      "Training Loss at step 180 : 12.851061820983887\n",
      "Test Loss 14.293034553527832 \n",
      "\n",
      "done with step 180 of 311\n",
      "current time: 1.198700777689616 est epoch finish:  2.059646087632434\n",
      "Training Loss at step 181 : 12.654778480529785\n",
      "Training Loss at step 182 : 13.117818832397461\n",
      "Training Loss at step 183 : 11.524532318115234\n",
      "Training Loss at step 184 : 12.453004837036133\n",
      "Training Loss at step 185 : 14.21839714050293\n",
      "Training Loss at step 186 : 13.507243156433105\n",
      "Training Loss at step 187 : 12.077788352966309\n",
      "Training Loss at step 188 : 12.933977127075195\n",
      "Training Loss at step 189 : 13.08749008178711\n",
      "Training Loss at step 190 : 13.015390396118164\n",
      "Test Loss 14.368637084960938 \n",
      "\n",
      "done with step 190 of 311\n",
      "current time: 1.2643960913022358 est epoch finish:  2.0587810701308658\n",
      "Training Loss at step 191 : 12.8514404296875\n",
      "Training Loss at step 192 : 12.278034210205078\n",
      "Training Loss at step 193 : 11.215442657470703\n",
      "Training Loss at step 194 : 13.537914276123047\n",
      "Training Loss at step 195 : 11.761917114257812\n",
      "Training Loss at step 196 : 12.781763076782227\n",
      "Training Loss at step 197 : 11.198660850524902\n",
      "Training Loss at step 198 : 12.506885528564453\n",
      "Training Loss at step 199 : 13.515995025634766\n",
      "Training Loss at step 200 : 12.067801475524902\n",
      "Test Loss 14.36642837524414 \n",
      "\n",
      "done with step 200 of 311\n",
      "current time: 1.3305006941159567 est epoch finish:  2.058635402338619\n",
      "Training Loss at step 201 : 13.061067581176758\n",
      "Training Loss at step 202 : 12.44190502166748\n",
      "Training Loss at step 203 : 10.743539810180664\n",
      "Training Loss at step 204 : 13.30854606628418\n",
      "Training Loss at step 205 : 12.86574649810791\n",
      "Training Loss at step 206 : 12.99846363067627\n",
      "Training Loss at step 207 : 11.115676879882812\n",
      "Training Loss at step 208 : 14.460880279541016\n",
      "Training Loss at step 209 : 11.370710372924805\n",
      "Training Loss at step 210 : 13.249826431274414\n",
      "Test Loss 14.315603256225586 \n",
      "\n",
      "done with step 210 of 311\n",
      "current time: 1.3954319914182027 est epoch finish:  2.056774167445787\n",
      "Training Loss at step 211 : 12.224069595336914\n",
      "Training Loss at step 212 : 12.648825645446777\n",
      "Training Loss at step 213 : 13.502985000610352\n",
      "Training Loss at step 214 : 11.67066478729248\n",
      "Training Loss at step 215 : 12.567058563232422\n",
      "Training Loss at step 216 : 12.741089820861816\n",
      "Training Loss at step 217 : 13.027046203613281\n",
      "Training Loss at step 218 : 12.465127944946289\n",
      "Training Loss at step 219 : 11.896150588989258\n",
      "Training Loss at step 220 : 10.868950843811035\n",
      "Test Loss 14.309432983398438 \n",
      "\n",
      "done with step 220 of 311\n",
      "current time: 1.4602732102076212 est epoch finish:  2.0549546080297296\n",
      "Training Loss at step 221 : 11.376791000366211\n",
      "Training Loss at step 222 : 14.302091598510742\n",
      "Training Loss at step 223 : 12.603378295898438\n",
      "Training Loss at step 224 : 13.973259925842285\n",
      "Training Loss at step 225 : 10.728254318237305\n",
      "Training Loss at step 226 : 14.309530258178711\n",
      "Training Loss at step 227 : 12.146591186523438\n",
      "Training Loss at step 228 : 11.947443008422852\n",
      "Training Loss at step 229 : 13.629714012145996\n",
      "Training Loss at step 230 : 13.113676071166992\n",
      "Test Loss 14.278100967407227 \n",
      "\n",
      "done with step 230 of 311\n",
      "current time: 1.5256690581639607 est epoch finish:  2.05403929475754\n",
      "Training Loss at step 231 : 12.378828048706055\n",
      "Training Loss at step 232 : 11.326351165771484\n",
      "Training Loss at step 233 : 11.089770317077637\n",
      "Training Loss at step 234 : 13.449012756347656\n",
      "Training Loss at step 235 : 12.698577880859375\n",
      "Training Loss at step 236 : 14.159290313720703\n",
      "Training Loss at step 237 : 13.168180465698242\n",
      "Training Loss at step 238 : 11.53244686126709\n",
      "Training Loss at step 239 : 11.742513656616211\n",
      "Training Loss at step 240 : 12.398889541625977\n",
      "Test Loss 14.280007362365723 \n",
      "\n",
      "done with step 240 of 311\n",
      "current time: 1.5926881551742553 est epoch finish:  2.0552946732746613\n",
      "Training Loss at step 241 : 13.114093780517578\n",
      "Training Loss at step 242 : 13.71769905090332\n",
      "Training Loss at step 243 : 12.418149948120117\n",
      "Training Loss at step 244 : 11.729767799377441\n",
      "Training Loss at step 245 : 12.837867736816406\n",
      "Training Loss at step 246 : 12.38068962097168\n",
      "Training Loss at step 247 : 11.485176086425781\n",
      "Training Loss at step 248 : 11.83044147491455\n",
      "Training Loss at step 249 : 11.779932022094727\n",
      "Training Loss at step 250 : 12.566455841064453\n",
      "Test Loss 14.285505294799805 \n",
      "\n",
      "done with step 250 of 311\n",
      "current time: 1.6615339159965514 est epoch finish:  2.0587133381471214\n",
      "Training Loss at step 251 : 12.916138648986816\n",
      "Training Loss at step 252 : 11.350301742553711\n",
      "Training Loss at step 253 : 12.472535133361816\n",
      "Training Loss at step 254 : 10.656074523925781\n",
      "Training Loss at step 255 : 11.65414810180664\n",
      "Training Loss at step 256 : 12.973268508911133\n",
      "Training Loss at step 257 : 12.989629745483398\n",
      "Training Loss at step 258 : 12.666013717651367\n",
      "Training Loss at step 259 : 12.318557739257812\n",
      "Training Loss at step 260 : 12.552159309387207\n",
      "Test Loss 14.340839385986328 \n",
      "\n",
      "done with step 260 of 311\n",
      "current time: 1.727234955628713 est epoch finish:  2.058122878162949\n",
      "Training Loss at step 261 : 11.133874893188477\n",
      "Training Loss at step 262 : 10.80101490020752\n",
      "Training Loss at step 263 : 11.154844284057617\n",
      "Training Loss at step 264 : 12.230306625366211\n",
      "Training Loss at step 265 : 12.869345664978027\n",
      "Training Loss at step 266 : 11.77934741973877\n",
      "Training Loss at step 267 : 12.608105659484863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss at step 268 : 11.842268943786621\n",
      "Training Loss at step 269 : 14.439458847045898\n",
      "Training Loss at step 270 : 12.698442459106445\n",
      "Test Loss 14.309755325317383 \n",
      "\n",
      "done with step 270 of 311\n",
      "current time: 1.792059353987376 est epoch finish:  2.05656995974197\n",
      "Training Loss at step 271 : 12.564315795898438\n",
      "Training Loss at step 272 : 11.256902694702148\n",
      "Training Loss at step 273 : 12.269036293029785\n",
      "Training Loss at step 274 : 12.185663223266602\n",
      "Training Loss at step 275 : 12.357847213745117\n",
      "Training Loss at step 276 : 13.10462760925293\n",
      "Training Loss at step 277 : 13.103679656982422\n",
      "Training Loss at step 278 : 11.546937942504883\n",
      "Training Loss at step 279 : 11.8087158203125\n",
      "Training Loss at step 280 : 12.84669303894043\n",
      "Test Loss 14.295166015625 \n",
      "\n",
      "done with step 280 of 311\n",
      "current time: 1.8559096654256184 est epoch finish:  2.054049487357179\n",
      "Training Loss at step 281 : 13.00103759765625\n",
      "Training Loss at step 282 : 11.135763168334961\n",
      "Training Loss at step 283 : 13.331686019897461\n",
      "Training Loss at step 284 : 12.53740119934082\n",
      "Training Loss at step 285 : 13.534400939941406\n",
      "Training Loss at step 286 : 11.151238441467285\n",
      "Training Loss at step 287 : 12.24795150756836\n",
      "Training Loss at step 288 : 13.971830368041992\n",
      "Training Loss at step 289 : 12.192054748535156\n",
      "Training Loss at step 290 : 12.70916748046875\n",
      "Test Loss 14.319759368896484 \n",
      "\n",
      "done with step 290 of 311\n",
      "current time: 1.9210158864657083 est epoch finish:  2.0530444697279564\n",
      "Training Loss at step 291 : 12.107521057128906\n",
      "Training Loss at step 292 : 11.062712669372559\n",
      "Training Loss at step 293 : 12.244587898254395\n",
      "Training Loss at step 294 : 12.280468940734863\n",
      "Training Loss at step 295 : 12.31342601776123\n",
      "Training Loss at step 296 : 12.362568855285645\n",
      "Training Loss at step 297 : 13.705662727355957\n",
      "Training Loss at step 298 : 12.607215881347656\n",
      "Training Loss at step 299 : 13.96438217163086\n",
      "Training Loss at step 300 : 12.093453407287598\n",
      "Test Loss 14.286148071289062 \n",
      "\n",
      "done with step 300 of 311\n",
      "current time: 1.9879095355669658 est epoch finish:  2.0539530417319813\n",
      "Training Loss at step 301 : 11.736061096191406\n",
      "Training Loss at step 302 : 12.145851135253906\n",
      "Training Loss at step 303 : 11.686692237854004\n",
      "Training Loss at step 304 : 12.241488456726074\n",
      "Training Loss at step 305 : 12.908500671386719\n",
      "Training Loss at step 306 : 11.428470611572266\n",
      "Training Loss at step 307 : 12.583335876464844\n",
      "Training Loss at step 308 : 11.97349739074707\n",
      "Training Loss at step 309 : 10.709559440612793\n",
      "Training Loss at step 310 : 12.084939002990723\n",
      "Test Loss 14.369218826293945 \n",
      "\n",
      "done with step 310 of 311\n",
      "current time: 2.051869002978007 est epoch finish:  2.051869002978007\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "subset_size = 5000\n",
    "\n",
    "data = spans[:subset_size]\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "keys = [float(i+1) for i in range(15)]\n",
    "\n",
    "data = [ [[1 if i == d[0] else 0 for i in keys],d[1],d[2],d[3]] for d in data]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "random.shuffle(data)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 5e-4\n",
    "\n",
    "model = FullContextSpanClassifier(list(labels)).to(device)\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "test_x = [d[1:] for d in data[:batch_size]]\n",
    "test_x_tokens = tokenizer([articles[x[-1]] for x in test_x], padding=True, return_offsets_mapping=True, return_tensors='pt')\n",
    "test_annotation_mask = calc_annotation_mask(test_x_tokens[\"offset_mapping\"], test_x)\n",
    "test_slice_tensor = torch.tensor(test_annotation_mask, dtype=torch.float)\n",
    "del test_x_tokens[\"offset_mapping\"]\n",
    "test_model_input = [test_x_tokens.to(device), test_slice_tensor.to(device)]\n",
    "\n",
    "test_y = torch.tensor([d[0] for d in data[:batch_size]], dtype=torch.float).to(device)\n",
    "\n",
    "train_data = data[batch_size:]\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    print(\"Starting training epoch\", epoch)\n",
    "    \n",
    "    random.shuffle(train_data)\n",
    "\n",
    "    train_x = [d[1:] for d in train_data]\n",
    "    train_y = [d[0] for d in train_data]\n",
    "\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    print(\"constructing batches...\")\n",
    "    \n",
    "    for i in range(len(train_data)//batch_size):\n",
    "        batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y = torch.tensor(train_y[i*batch_size:(i+1)*batch_size], dtype=torch.float).to(device)\n",
    "        x_batches.append(batch_x)\n",
    "        y_batches.append(batch_y)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(\"batches constructed. starting training steps...\")\n",
    "\n",
    "    for step in range(len(x_batches)):\n",
    "        \n",
    "        #print(\"tokenizing batch {} of {}...\".format(step, len(x_batches)))\n",
    "        \n",
    "        batch_tokens = tokenizer([articles[x[-1]] for x in x_batches[step]], padding=True, truncation=True, return_offsets_mapping=True, return_tensors='pt')\n",
    "        \n",
    "        #print(\"tokenized. calculating annotation mask...\")\n",
    "        \n",
    "        annotation_mask = calc_annotation_mask(batch_tokens[\"offset_mapping\"], x_batches[step])\n",
    "        slice_tensor = torch.tensor(annotation_mask, dtype=torch.float)\n",
    "        del batch_tokens[\"offset_mapping\"]\n",
    "        model_input = [batch_tokens.to(device), slice_tensor.to(device)]\n",
    "        \n",
    "        #print(\"mask calculated. running model...\")\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             embeddings = model(model_input).detach().cpu().numpy().astype('float32')\n",
    "#         all_embeddings.append(embeddings)\n",
    "        y_train_pred = model(model_input)\n",
    "        train_loss = loss_fn(y_train_pred, y_batches[step])\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        print(\"Training Loss at step\",step,\":\",train_loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            y_test_pred = model(test_model_input)\n",
    "            test_loss = loss_fn(y_test_pred, test_y).item()\n",
    "            test_losses.append(test_loss)\n",
    "            print(\"Test Loss\",test_loss,\"\\n\")\n",
    "            \n",
    "            diff = time.time() - t1\n",
    "            proj_end = (diff/(step+1)) * len(x_batches)\n",
    "            print(\"done with step {} of {}\".format(step, len(x_batches)))\n",
    "            print(\"current time:\", diff/60, \"est epoch finish: \", proj_end/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_domain = list(range(len(train_losses)))\n",
    "test_domain = ([i*10 for i in range(len(test_losses))])\n",
    "\n",
    "plt.plot(train_domain, train_losses, label=\"train\")\n",
    "plt.plot(test_domain, test_losses, label=\"test\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./distilberta-mfc-with-context.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reloaded = FullContextSpanClassifier(labels)\n",
    "model_reloaded.load_state_dict(torch.load(\"./distilberta-mfc-with-context.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 2: 0,\n",
       " 4: 0,\n",
       " 8: 0,\n",
       " 16: 0,\n",
       " 32: 0,\n",
       " 64: 20,\n",
       " 128: 2092,\n",
       " 256: 29581,\n",
       " 512: 240,\n",
       " 1024: 75,\n",
       " 2048: 6,\n",
       " 4096: 0,\n",
       " 8192: 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
